from fastapi import APIRouter, Request
from fastapi.responses import HTMLResponse
from fastapi.templating import Jinja2Templates
from pydantic import BaseModel
from langchain_core.messages import HumanMessage, AIMessage
import os
import time

from app.agent.graph import deep_researcher
from app.tools.cache import research_cache


router = APIRouter()
templates = Jinja2Templates(directory="app/templates")


class ChatRequest(BaseModel):
    message: str
    domain: str = "ì½”ë”©"  # í•­ìƒ ì½”ë”©ìœ¼ë¡œ ê³ ì •
    history: list = []  # ì´ì „ ëŒ€í™” ì´ë ¥


@router.get("/", response_class=HTMLResponse)
async def index(request: Request):
    return templates.TemplateResponse(
        "index.html",
        {"request": request}
    )


@router.post("/api/chat")
async def chat(req: ChatRequest):
    """íŒ€ ìƒí™© ê¸°ë°˜ ì½”ë”© AI ë„ì… ì˜ì‚¬ê²°ì • ì—ì´ì „íŠ¸ API"""
    
    try:
        # ë„ë©”ì¸ì€ í•­ìƒ ì½”ë”©ìœ¼ë¡œ ê³ ì •
        domain = "ì½”ë”©"
        
        # ì‹œì‘ ì‹œê°„ ê¸°ë¡
        start_time = time.time()
        
        # 1. ìºì‹œ í™•ì¸
        cached_result = research_cache.get(req.message, domain)
        if cached_result:
            elapsed_time = time.time() - start_time
            print(f"âœ… ìºì‹œì—ì„œ ì‘ë‹µ ë°˜í™˜ (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
            return {"reply": cached_result["reply"]}
        
        # 2. LangGraph ì‹¤í–‰
        print(f"ğŸ” Deep Research ì‹œì‘: {req.message} (ë„ë©”ì¸: {domain})")
        
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        openai_key = os.getenv("OPENAI_API_KEY", "")
        if not openai_key or openai_key.startswith("sk-proj-xxx"):
            return {
                "reply": "âš ï¸ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
            }
        
        # ëŒ€í™” ì´ë ¥ êµ¬ì„±
        messages_to_send = []
        
        # ì´ì „ ëŒ€í™” ì´ë ¥ ì¶”ê°€
        for msg in req.history:
            if msg.get("role") == "user":
                messages_to_send.append(HumanMessage(content=msg.get("content", "")))
            elif msg.get("role") == "assistant":
                messages_to_send.append(AIMessage(content=msg.get("content", "")))
        
        # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        messages_to_send.append(HumanMessage(content=req.message))
        
        print(f"ğŸ” [DEBUG] chat.py - ì „ì†¡í•  Messages ê°œìˆ˜: {len(messages_to_send)}ê°œ")
        
        # LangGraph ì‹¤í–‰
        result = await deep_researcher.ainvoke(
            {
                "messages": messages_to_send,
                "domain": domain
            },
            config={
                "configurable": {
                    "domain": domain
                }
            }
        )
        
        # ìµœì¢… ë¦¬í¬íŠ¸ ì¶”ì¶œ
        messages = result.get("messages", [])
        
        # AI ë©”ì‹œì§€ë§Œ ì¶”ì¶œ (ë§ˆì§€ë§‰ Nê°œ)
        ai_messages = [msg for msg in messages if isinstance(msg, AIMessage)]
        
        # ë§ˆì§€ë§‰ 2ê°œì˜ AI ë©”ì‹œì§€ í™•ì¸ (ì¸ì‚¬ë§ + ë¦¬í¬íŠ¸ ë¶„ë¦¬)
        if len(ai_messages) >= 2:
            # ë§ˆì§€ë§‰ 2ê°œ ë©”ì‹œì§€ ë°˜í™˜ (ë¦¬ìŠ¤íŠ¸ë¡œ)
            reply_messages = [ai_messages[-2].content, ai_messages[-1].content]
            print(f"âœ… [DEBUG] 2ê°œ ë©”ì‹œì§€ ê°ì§€: {len(reply_messages)}ê°œ")
        elif len(ai_messages) == 1:
            # 1ê°œë§Œ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            reply_messages = [ai_messages[-1].content]
            print(f"âœ… [DEBUG] 1ê°œ ë©”ì‹œì§€ ê°ì§€")
        else:
            # ë©”ì‹œì§€ ì—†ìœ¼ë©´ final_report ì‚¬ìš©
            final_report = result.get("final_report", "")
            reply_messages = [final_report] if final_report else ["ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."]
        
        # 3. ìºì‹œ ì €ì¥ (ë§ˆì§€ë§‰ ë©”ì‹œì§€ë§Œ)
        cache_data = {"reply": reply_messages[-1] if reply_messages else ""}
        research_cache.set(req.message, cache_data, domain)
        
        # ì¢…ë£Œ ì‹œê°„ ê³„ì‚°
        elapsed_time = time.time() - start_time
        print(f"âœ… Deep Research ì™„ë£Œ (ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ)")
        
        # ì—¬ëŸ¬ ë©”ì‹œì§€ë©´ ë°°ì—´ë¡œ, í•˜ë‚˜ë©´ ë¬¸ìì—´ë¡œ ë°˜í™˜
        if len(reply_messages) > 1:
            return {"reply": reply_messages}
        else:
            return {"reply": reply_messages[0]}
    
    except Exception as e:
        error_msg = f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(error_msg)
        return {
            "reply": f"""
ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.

**ì˜¤ë¥˜ ë‚´ìš©:**
{str(e)}

**í•´ê²° ë°©ë²•:**
1. .env íŒŒì¼ì— API í‚¤ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
2. ì¸í„°ë„· ì—°ê²° í™•ì¸
3. ì§ˆë¬¸ì„ ë” ê°„ë‹¨í•˜ê²Œ ì‘ì„±í•´ë³´ê¸°

ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.
            """
        }
"""LangGraph nodes for AI Service Advisor"""

import asyncio
import re
from datetime import datetime
from typing import Literal
from langchain.chat_models import init_chat_model
from langchain_core.messages import (
    AIMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
    get_buffer_string,
)
from langchain_core.runnables import RunnableConfig
from langgraph.types import Command

from app.agent.configuration import Configuration
from app.agent.state import (
    AgentState,
    ClarifyWithUser,
    ConductResearch,
    ResearchComplete,
    ResearchQuestion,
    ResearcherState,
    SupervisorState,
)
from app.agent.models import ToolFact, UserContext, PricingPlan, SecurityPolicy, WorkflowType
from app.agent.decision import DecisionEngine
from app.agent.fact_extractor import extract_tool_facts
from app.agent.prompts import (
    DOMAIN_GUIDES,
    clarify_with_user_instructions,
    transform_messages_into_research_topic_prompt,
    lead_researcher_prompt,
    research_system_prompt,
    compress_research_system_prompt,
    compress_research_simple_human_message,
    final_report_generation_prompt,
    get_today_str,
    get_current_year,
    get_current_month_year,
)
from app.agent.utils import (
    think_tool,
    get_api_key_for_model,
    get_notes_from_tool_calls,
)
from app.tools.search import searcher
from app.tools.vector_store import vector_store
from app.tools.query_normalizer import query_normalizer
from app.tools.cache import research_cache

# ì„¤ì • ê°€ëŠ¥í•œ ëª¨ë¸
configurable_model = init_chat_model(
    configurable_fields=("model", "max_tokens", "api_key"),
)


async def clarify_with_user(
    state: AgentState, config: RunnableConfig
) -> Command[Literal["write_research_brief", "__end__"]]:
    """ì‚¬ìš©ì ì§ˆë¬¸ ëª…í™•í™” ë° ì£¼ì œ ê²€ì¦ + ì¿¼ë¦¬ ì •ê·œí™” + ìºì‹œ ì¡°íšŒ"""
    
    # re ëª¨ë“ˆì„ í•¨ìˆ˜ ë‚´ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ importí•˜ì—¬ ìŠ¤ì½”í”„ ë¬¸ì œ í•´ê²°
    import re
    
    configurable = Configuration.from_runnable_config(config)
    messages = state["messages"]
    domain = state.get("domain", "AI ì„œë¹„ìŠ¤")
    
    # ì§ˆë¬¸ ìˆœì„œ íŒŒì•…: HumanMessage ê°œìˆ˜ë¡œ íŒë‹¨ (ë” ì •í™•í•˜ê²Œ)
    human_messages = [msg for msg in messages if isinstance(msg, HumanMessage)]
    question_number = len(human_messages)  # 1ë²ˆì§¸, 2ë²ˆì§¸, 3ë²ˆì§¸ ì§ˆë¬¸...
    is_followup = question_number > 1  # 2ë²ˆì§¸ ì§ˆë¬¸ë¶€í„° Follow-up
    
    # ë””ë²„ê¹…
    print(f"ğŸ” [DEBUG] clarify - Messages: {len(messages)}ê°œ, HumanMessage: {len(human_messages)}ê°œ, ì§ˆë¬¸ ìˆœì„œ: {question_number}ë²ˆì§¸, Follow-up: {is_followup}")
    
    # ========== ğŸ†• 1ë‹¨ê³„: ì¿¼ë¦¬ ì •ê·œí™” (ìºì‹œ í‚¤ ìƒì„±) ==========
    last_user_message = messages[-1].content if messages else ""
    
    model_config = {
        "model": configurable.research_model,
        "max_tokens": 200,  # ì •ê·œí™”ëŠ” ì§§ê²Œ
        "api_key": get_api_key_for_model(configurable.research_model, config),
    }
    
    normalized = await query_normalizer.normalize(last_user_message, config=model_config)
    cache_key = normalized["cache_key"]
    
    # ========== ğŸ†• 2ë‹¨ê³„: Redis ìµœì¢… ë‹µë³€ ìºì‹œ ì¡°íšŒ ==========
    print(f"ğŸ” [ìºì‹œ ì¡°íšŒ] ì›ë³¸ ì§ˆë¬¸: '{last_user_message[:50]}...'")
    print(f"ğŸ” [ìºì‹œ ì¡°íšŒ] ì •ê·œí™”: '{normalized['normalized_text']}' â†’ ìºì‹œí‚¤: {cache_key[:16]}...")
    
    cached_answer = research_cache.get(cache_key, domain=domain, prefix="final")
    if cached_answer:
        print(f"âœ… [ìºì‹œ HIT] ìµœì¢… ë‹µë³€ ë°˜í™˜ (ìºì‹œí‚¤: {cache_key[:16]}...)")
        
        # Follow-upì¸ ê²½ìš° ì´ì „ ì¶”ì²œ ë„êµ¬ í™•ì¸
        # ë‹¨, ê°™ì€ ì˜ë¯¸ì˜ ì§ˆë¬¸(ê°™ì€ ìºì‹œ í‚¤)ì´ë©´ ì´ì „ ì¶”ì²œ ë„êµ¬ í™•ì¸ ê±´ë„ˆë›°ê³  ìºì‹œ ì‚¬ìš©
        if is_followup:
            # ì´ì „ ë©”ì‹œì§€ì—ì„œ ì¶”ì²œëœ ë„êµ¬ ì¶”ì¶œ (ëª¨ë“  AI ë©”ì‹œì§€ì—ì„œ)
            previous_tools_in_messages = []
            all_tools = []
            for msg in reversed(messages[:-1]):  # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì œì™¸
                if isinstance(msg, AIMessage) and hasattr(msg, 'content'):
                    content = str(msg.content)
                    # ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ ë„êµ¬ëª… ì¶”ì¶œ
                    # íŒ¨í„´ 1: ğŸ“Š [ë„êµ¬ëª…]
                    tools_found = re.findall(r'ğŸ“Š\s+([^\n]+)', content)
                    if tools_found:
                        all_tools.extend([t.strip() for t in tools_found])
                    # íŒ¨í„´ 2: ## ğŸ“Š [ë„êµ¬ëª…]
                    tools_found2 = re.findall(r'##\s+ğŸ“Š\s+([^\n]+)', content)
                    if tools_found2:
                        all_tools.extend([t.strip() for t in tools_found2])
                    # íŒ¨í„´ 3: **1ìˆœìœ„: [ë„êµ¬ëª…]**, **2ìˆœìœ„: [ë„êµ¬ëª…]**
                    tools_found3 = re.findall(r'\*\*[0-9]+ìˆœìœ„:\s*([^\*]+)\*\*', content)
                    if tools_found3:
                        all_tools.extend([t.strip() for t in tools_found3])
                    # íŒ¨í„´ 4: **ìµœì¢… ì¶”ì²œ: [ë„êµ¬ëª…]**
                    tools_found4 = re.findall(r'\*\*ìµœì¢… ì¶”ì²œ:\s*([^\*]+)\*\*', content)
                    if tools_found4:
                        all_tools.extend([t.strip() for t in tools_found4])
            
            # ì¤‘ë³µ ì œê±°
            seen = set()
            for tool in all_tools:
                # ë„êµ¬ëª… ì •ì œ (ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°)
                tool_clean = re.sub(r'[\(\)\[\]ì›”\s\$0-9]+', '', tool).strip()
                if tool_clean and tool_clean not in seen and len(tool_clean) > 2:
                    seen.add(tool_clean)
                    previous_tools_in_messages.append(tool_clean)
            
            # ì´ì „ ì¶”ì²œ ë„êµ¬ê°€ ìˆìœ¼ë©´ ìºì‹œ ê²€ì¦, ì—†ìœ¼ë©´ ê°™ì€ ì˜ë¯¸ì˜ ì§ˆë¬¸ì´ë¯€ë¡œ ìºì‹œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if previous_tools_in_messages:
                # ìºì‹œëœ ë‹µë³€ì—ì„œ ë„êµ¬ ì¶”ì¶œ (ë‹¤ì–‘í•œ íŒ¨í„´)
                cached_tools = []
                cached_content = cached_answer["content"]
                # íŒ¨í„´ 1: ğŸ“Š [ë„êµ¬ëª…]
                tools_found = re.findall(r'ğŸ“Š\s+([^\n]+)', cached_content)
                cached_tools.extend([t.strip() for t in tools_found])
                # íŒ¨í„´ 2: ## ğŸ“Š [ë„êµ¬ëª…]
                tools_found2 = re.findall(r'##\s+ğŸ“Š\s+([^\n]+)', cached_content)
                cached_tools.extend([t.strip() for t in tools_found2])
                # íŒ¨í„´ 3: **1ìˆœìœ„: [ë„êµ¬ëª…]**
                tools_found3 = re.findall(r'\*\*[0-9]+ìˆœìœ„:\s*([^\*]+)\*\*', cached_content)
                cached_tools.extend([t.strip() for t in tools_found3])
                
                # ì´ì „ ì¶”ì²œ ë„êµ¬ì™€ ìºì‹œëœ ë‹µë³€ì˜ ë„êµ¬ê°€ ë‹¤ë¥´ë©´ ìºì‹œ ë¬´ì‹œ
                if cached_tools:
                    # ë„êµ¬ëª… ì •ì œ
                    previous_tools_clean = [re.sub(r'[\(\)\[\]ì›”\s\$0-9]+', '', t).strip() for t in previous_tools_in_messages]
                    cached_tools_clean = [re.sub(r'[\(\)\[\]ì›”\s\$0-9]+', '', t).strip() for t in cached_tools]
                    
                    previous_tools_set = set([t for t in previous_tools_clean if len(t) > 2])
                    cached_tools_set = set([t for t in cached_tools_clean if len(t) > 2])
                    
                    # ì´ì „ ì¶”ì²œ ë„êµ¬ê°€ ìºì‹œì— ì—†ê±°ë‚˜, ìºì‹œì— ì´ì „ì— ì¶”ì²œí•˜ì§€ ì•Šì€ ìƒˆ ë„êµ¬ê°€ ìˆìœ¼ë©´ ë¬´ì‹œ
                    if not previous_tools_set.issubset(cached_tools_set) or len(cached_tools_set - previous_tools_set) > 0:
                        print(f"âš ï¸ [ìºì‹œ ë¬´ì‹œ] ì´ì „ ì¶”ì²œ ë„êµ¬({previous_tools_in_messages})ì™€ ìºì‹œ ë„êµ¬({cached_tools})ê°€ ë‹¤ë¦„. ìºì‹œ ë¬´ì‹œí•˜ê³  ìƒˆë¡œ ìƒì„±")
                        cached_answer = None  # ìºì‹œ ë¬´ì‹œ
            else:
                # ì´ì „ ì¶”ì²œ ë„êµ¬ê°€ ì—†ìœ¼ë©´ ê°™ì€ ì˜ë¯¸ì˜ ì§ˆë¬¸ì´ë¯€ë¡œ ìºì‹œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                print(f"âœ… [ìºì‹œ ì‚¬ìš©] ì´ì „ ì¶”ì²œ ë„êµ¬ ì—†ìŒ - ê°™ì€ ì˜ë¯¸ì˜ ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨, ìºì‹œ ì‚¬ìš©")
        
        if cached_answer:
            # ìºì‹œëœ ë‹µë³€ ì²˜ë¦¬
            cached_content = cached_answer["content"]
            
            print(f"ğŸ” [ìºì‹œ ì²˜ë¦¬] ìºì‹œëœ ë‹µë³€ ê¸¸ì´: {len(cached_content)}ì, is_followup: {is_followup}")
            print(f"ğŸ” [ìºì‹œ ì²˜ë¦¬] ìºì‹œëœ ë‹µë³€ ì‹œì‘ 100ì: {cached_content[:100]}")
            
            # ë¦¬í¬íŠ¸ ë³¸ë¬¸ ì¶”ì¶œ (ìºì‹œì—ëŠ” ë¦¬í¬íŠ¸ ë³¸ë¬¸ë§Œ ì €ì¥ë˜ì–´ ìˆìŒ)
            report_body = cached_content.strip()
            
            # ğŸš¨ [GREETING] íƒœê·¸ê°€ ìˆìœ¼ë©´ ì œê±°í•˜ê³  ë¦¬í¬íŠ¸ ë³¸ë¬¸ë§Œ ì¶”ì¶œ
            # ì¸ì‚¬ ë©˜íŠ¸ëŠ” ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ì§€ ì•Šê³  í•­ìƒ ìƒˆë¡œ ìƒì„±
            if "[GREETING]" in cached_content and "[/GREETING]" in cached_content:
                match = re.search(r'\[GREETING\](.*?)\[/GREETING\]', cached_content, re.DOTALL)
                if match:
                    # ì¸ì‚¬ë§ íƒœê·¸ ì œê±°í•˜ê³  ë¦¬í¬íŠ¸ ë³¸ë¬¸ë§Œ ì¶”ì¶œ
                    report_body = cached_content.replace(match.group(0), "").strip()
                    print(f"âœ… [ìºì‹œ] [GREETING] íƒœê·¸ ì œê±° í›„ ë¦¬í¬íŠ¸ ë³¸ë¬¸ ì¶”ì¶œ: {len(report_body)}ì")
            
            # ë¦¬í¬íŠ¸ ë³¸ë¬¸ì´ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
            if not report_body or len(report_body) < 50:
                print(f"âš ï¸ [ìºì‹œ ì²˜ë¦¬] ë¦¬í¬íŠ¸ ë³¸ë¬¸ì´ ë¹„ì–´ìˆìŒ - ì›ë³¸ ìºì‹œ ë‚´ìš© ì‚¬ìš©")
                report_body = cached_content.strip()
            
            # ğŸš¨ ìºì‹œ ê²€ì¦: ë¦¬í¬íŠ¸ ë³¸ë¬¸ì´ ìœ íš¨í•œì§€ í™•ì¸
            # ë¦¬í¬íŠ¸ê°€ ë„ˆë¬´ ì§§ê±°ë‚˜(200ì ë¯¸ë§Œ) ë¹„ì–´ìˆìœ¼ë©´ ìºì‹œ ë¬´ì‹œ
            if len(report_body) < 200:
                print(f"âš ï¸ [ìºì‹œ ë¬´ì‹œ] ë¦¬í¬íŠ¸ ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ ({len(report_body)}ì). ìºì‹œ ë¬´ì‹œí•˜ê³  ìƒˆë¡œ ìƒì„±")
                # pass - ìºì‹œë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì•„ë˜ ì—°êµ¬ í”„ë¡œì„¸ìŠ¤ë¡œ ì§„í–‰
            else:
                # ğŸš¨ ì¸ì‚¬ ë©˜íŠ¸ëŠ” í•­ìƒ ìƒˆë¡œ ìƒì„± (ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ì§€ ì•ŠìŒ)
                # final_report_generationê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì¸ì‚¬ ë©˜íŠ¸ ìƒì„± (ë™ì¼í•œ ëª¨ë¸, ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼)
                print(f"âœ… [ìºì‹œ ì²˜ë¦¬] ë¦¬í¬íŠ¸ ë³¸ë¬¸ì€ ìºì‹œì—ì„œ ê°€ì ¸ì˜´ ({len(report_body)}ì), ì¸ì‚¬ ë©˜íŠ¸ëŠ” final_report_generationê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ìƒì„±")
                
                # final_report_generationê³¼ ë™ì¼í•œ ëª¨ë¸ ë° ì„¤ì • ì‚¬ìš©
                greeting_model_config = {
                    "model": configurable.final_report_model,
                    "max_tokens": configurable.final_report_model_max_tokens,
                    "api_key": get_api_key_for_model(configurable.final_report_model, config),
                }
                
                # final_report_generation í”„ë¡¬í”„íŠ¸ì˜ ì¸ì‚¬ ë©˜íŠ¸ ìƒì„± ë¶€ë¶„ê³¼ ë™ì¼í•œ ìŠ¤íƒ€ì¼
                # ì‚¬ìš©ì ë©”ì‹œì§€ ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ì œê³µ (final_report_generationê³¼ ë™ì¼)
                messages_context = get_buffer_string(messages) if messages else last_user_message
                
                greeting_prompt = f"""ë‹¹ì‹ ì€ ì½”ë”© AI ë„êµ¬ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì§ˆë¬¸ì— ë§ëŠ” ìì—°ìŠ¤ëŸ½ê³  ìƒì„¸í•œ ì¸ì‚¬ ë©˜íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì‚¬ìš©ì ë©”ì‹œì§€:
{messages_context}

**ì›ì¹™:**
- ì‚¬ìš©ìì˜ í˜„ì¬ ì§ˆë¬¸ ë‚´ìš©ê³¼ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ê·¸ì— ë§ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ë©˜íŠ¸ë¥¼ ìƒì„±
- ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œ(íŒ€ ê·œëª¨, ëª©ì , ìš”êµ¬ì‚¬í•­, ë„ë©”ì¸ ë“±)ë¥¼ ë°˜ì˜
- ì§ˆë¬¸ì— ì–¸ê¸‰ëœ êµ¬ì²´ì ì¸ ë‚´ìš©(íŒ€ ê·œëª¨, ëª©ì , ìš”êµ¬ì‚¬í•­ ë“±)ì„ ë°˜ë“œì‹œ í¬í•¨
- ìì—°ìŠ¤ëŸ½ê³  ì¹œì ˆí•œ í†¤ ìœ ì§€
- ì ì ˆí•œ ê¸¸ì´ (40-100ì ì •ë„, ë„ˆë¬´ ì§§ì§€ ì•Šê²Œ)

**ì¢‹ì€ ì˜ˆì‹œ:**
- ì§ˆë¬¸: "ì €í¬ëŠ” ë°±ì—”ë“œÂ·í”„ë¡ íŠ¸ì—”ë“œ í¬í•¨í•´ì„œ 8ëª… ê·œëª¨ì˜ ê°œë°œíŒ€ì¸ë°, ì½”ë“œ ì‘ì„±ê³¼ ë¦¬ë·°ì— AIë¥¼ ë„ì…í•´ì„œ ìƒì‚°ì„±ì„ ë†’ì´ê³  ì‹¶ìŠµë‹ˆë‹¤. ì–´ë–¤ ë„êµ¬ê°€ ì¢‹ì„ê¹Œìš”?"
  ì¸ì‚¬ ë©˜íŠ¸: "ë„¤! ë°±ì—”ë“œì™€ í”„ë¡ íŠ¸ì—”ë“œë¥¼ í¬í•¨í•œ 8ëª… ê·œëª¨ì˜ ê°œë°œíŒ€ì— ì í•©í•œ AI ë„êµ¬ë“¤ì„ ë¶„ì„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. íŒ€ì˜ ì½”ë“œ ì‘ì„± ë° ë¦¬ë·° íš¨ìœ¨ì„± í–¥ìƒì— ë„ì›€ì´ ë˜ëŠ” ë„êµ¬ë¥¼ ë¹„êµí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."

- ì§ˆë¬¸: "ì½”ë“œ ì‘ì„±ê³¼ ë¦¬ë·°ë¥¼ ìœ„í•œ AI ë„êµ¬ ì¶”ì²œí•´ì¤˜"
  ì¸ì‚¬ ë©˜íŠ¸: "ë„¤! ì½”ë“œ ì‘ì„±ê³¼ ë¦¬ë·°ë¥¼ ìœ„í•œ ìµœì ì˜ AI ë„êµ¬ë¥¼ ì¶”ì²œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."

**ë‚˜ìœ ì˜ˆì‹œ (ë„ˆë¬´ ì§§ê±°ë‚˜ ë§¥ë½ ì—†ìŒ):**
- "ì•ˆë…•í•˜ì„¸ìš”." (ë„ˆë¬´ ì§§ìŒ)
- "AI ë„êµ¬ë¡œ ìƒì‚°ì„±ì„ ë†’ì—¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤." (ë„ˆë¬´ ì§§ê³  êµ¬ì²´ì ì´ì§€ ì•ŠìŒ)
- "ë„¤! ì¡°ì‚¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤." (ë„ˆë¬´ ì¼ë°˜ì )

ì¸ì‚¬ ë©˜íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš” ([GREETING] íƒœê·¸ ì—†ì´, ë‹¤ë¥¸ ì„¤ëª… ì—†ì´):"""
                
                try:
                    greeting_model = configurable_model.with_config(greeting_model_config)
                    greeting_response = await greeting_model.ainvoke([HumanMessage(content=greeting_prompt)])
                    greeting = str(greeting_response.content).strip()
                    
                    # ë¶ˆí•„ìš”í•œ ë”°ì˜´í‘œë‚˜ íƒœê·¸ ì œê±°
                    greeting = greeting.strip('"\'`').strip()
                    
                    # "ì•ˆë…•í•˜ì„¸ìš”"ë¡œë§Œ ì‹œì‘í•˜ëŠ” ë„ˆë¬´ ì§§ì€ ì‘ë‹µ ê°ì§€
                    if greeting.startswith("ì•ˆë…•í•˜ì„¸ìš”") and len(greeting) < 15:
                        print(f"âš ï¸ [ìºì‹œ ì²˜ë¦¬] LLM ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ: '{greeting}', ì¬ì‹œë„")
                        greeting = ""  # ì¬ì‹œë„í•˜ë„ë¡ ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •
                    
                    # ì‘ë‹µì´ ë„ˆë¬´ ê¸¸ë©´ ì ì ˆíˆ ìë¥´ê¸° (100ì ì´ë‚´ë¡œ)
                    if greeting and len(greeting) > 100:
                        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸° (ë§ˆì¹¨í‘œë‚˜ ëŠë‚Œí‘œ ê¸°ì¤€)
                        sentences = re.split(r'[.!?ã€‚]', greeting)
                        if len(sentences) > 1 and sentences[0]:
                            # ì²« ë²ˆì§¸ ë¬¸ì¥ë§Œ ì‚¬ìš©í•˜ê³  ë§ˆì¹¨í‘œ ì¶”ê°€
                            greeting = sentences[0].strip() + '.'
                        else:
                            # ë¬¸ì¥ êµ¬ë¶„ì´ ì—†ìœ¼ë©´ 100ìë¡œ ìë¥´ê¸°
                            greeting = greeting[:100].strip()
                    
                    # ë¹ˆ ì‘ë‹µì´ê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ ì¬ì‹œë„ (ìµœì†Œ 30ì ì´ìƒ)
                    if not greeting or len(greeting) < 30:
                        print(f"âš ï¸ [ìºì‹œ ì²˜ë¦¬] LLM ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ ({len(greeting) if greeting else 0}ì), ì¬ì‹œë„")
                        # ë” ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„ (final_report_generation ìŠ¤íƒ€ì¼)
                        retry_prompt = f"""ë‹¹ì‹ ì€ ì½”ë”© AI ë„êµ¬ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ë©”ì‹œì§€:
{messages_context}

ìœ„ ì§ˆë¬¸ì— ë§ëŠ” ìì—°ìŠ¤ëŸ½ê³  ìƒì„¸í•œ ì¸ì‚¬ ë©˜íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš”. ì§ˆë¬¸ì˜ í•µì‹¬ ë‚´ìš©(íŒ€ ê·œëª¨, ëª©ì , ìš”êµ¬ì‚¬í•­ ë“±)ì„ êµ¬ì²´ì ìœ¼ë¡œ ë°˜ì˜í•œ 40-100ì ì •ë„ì˜ ìƒì„¸í•œ ì¸ì‚¬ ë©˜íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì˜ˆì‹œ: "ë„¤! ë°±ì—”ë“œì™€ í”„ë¡ íŠ¸ì—”ë“œë¥¼ í¬í•¨í•œ 8ëª… ê·œëª¨ì˜ ê°œë°œíŒ€ì— ì í•©í•œ AI ë„êµ¬ë“¤ì„ ë¶„ì„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. íŒ€ì˜ ì½”ë“œ ì‘ì„± ë° ë¦¬ë·° íš¨ìœ¨ì„± í–¥ìƒì— ë„ì›€ì´ ë˜ëŠ” ë„êµ¬ë¥¼ ë¹„êµí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."

ì¸ì‚¬ ë©˜íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”:"""
                        retry_response = await greeting_model.ainvoke([HumanMessage(content=retry_prompt)])
                        greeting = str(retry_response.content).strip().strip('"\'`').strip()
                        
                        # ì¬ì‹œë„ í›„ì—ë„ ë„ˆë¬´ ì§§ìœ¼ë©´ ì§ˆë¬¸ ê¸°ë°˜ìœ¼ë¡œ ë™ì  ìƒì„±
                        if not greeting or len(greeting) < 30:
                            # ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì„œ ë™ì ìœ¼ë¡œ ìƒì„±
                            keywords = []
                            if "íŒ€" in last_user_message or "ê·œëª¨" in last_user_message:
                                keywords.append("íŒ€")
                            if "ì½”ë“œ" in last_user_message or "ë¦¬ë·°" in last_user_message:
                                keywords.append("ì½”ë“œ ì‘ì„± ë° ë¦¬ë·°")
                            if "ë„êµ¬" in last_user_message or "ì¶”ì²œ" in last_user_message:
                                keywords.append("ë„êµ¬ ì¶”ì²œ")
                            
                            if keywords:
                                greeting = f"ë„¤! {'ì™€ '.join(keywords[:2])}ì— ì í•©í•œ AI ë„êµ¬ë¥¼ ë¶„ì„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
                            else:
                                greeting = f"ë„¤! {last_user_message[:30]}ì— ëŒ€í•´ ì¡°ì‚¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
                    
                    print(f"âœ… [ìºì‹œ ì²˜ë¦¬] LLMìœ¼ë¡œ ì¸ì‚¬ ë©˜íŠ¸ ìƒì„± ì™„ë£Œ: '{greeting}' (ê¸¸ì´: {len(greeting)}ì)")
                except Exception as e:
                    print(f"âš ï¸ [ìºì‹œ ì²˜ë¦¬] LLM ì¸ì‚¬ ë©˜íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}, ì§ˆë¬¸ ê¸°ë°˜ ë™ì  ìƒì„±")
                    # LLM ì‹¤íŒ¨ ì‹œ ì§ˆë¬¸ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë™ì ìœ¼ë¡œ ìƒì„± (í•˜ë“œì½”ë”© ìµœì†Œí™”)
                    question_preview = last_user_message[:50] if len(last_user_message) > 50 else last_user_message
                    greeting = f"ë„¤! {question_preview}ì— ëŒ€í•´ ì¡°ì‚¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
                    print(f"âœ… [ìºì‹œ ì²˜ë¦¬] ë™ì  ìƒì„± ì¸ì‚¬ ë©˜íŠ¸: '{greeting}'")
                print(f"âœ… [ìºì‹œ ì²˜ë¦¬] ë¦¬í¬íŠ¸ ë³¸ë¬¸ ê¸¸ì´: {len(report_body)}ì, ì‹œì‘ 100ì: {report_body[:100]}")
                
                return Command(
                    goto="__end__",
                    update={"messages": [
                        AIMessage(content=greeting),
                        AIMessage(content=report_body)
                    ]}
                )
    
    print(f"âš ï¸ [ìºì‹œ MISS] ì •ê·œí™”ëœ ì¿¼ë¦¬: '{normalized['normalized_text']}' (í‚¤ì›Œë“œ: {normalized['keywords']})")
    
    # ========== ğŸ†• 3ë‹¨ê³„: ë²¡í„° DBë¡œ ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰ ==========
    # ìºì‹œ ë¯¸ìŠ¤ ì‹œ ìœ ì‚¬í•œ ì§ˆë¬¸ì´ ìˆëŠ”ì§€ ë²¡í„° DBì—ì„œ ê²€ìƒ‰
    similar_query = vector_store.search_similar_query(
        query=last_user_message,
        domain=domain,
        limit=1,
        score_threshold=0.85  # ë†’ì€ ìœ ì‚¬ë„ë§Œ (85% ì´ìƒ)
    )
    
    if similar_query and similar_query.get("cache_key"):
        similar_cache_key = similar_query["cache_key"]
        print(f"ğŸ” [ìœ ì‚¬ ì§ˆë¬¸ ë°œê²¬] ìœ ì‚¬ë„: {similar_query['score']:.3f}, ê¸°ì¡´ ì§ˆë¬¸: '{similar_query['query'][:50]}...'")
        print(f"ğŸ” [ìœ ì‚¬ ì§ˆë¬¸] ìºì‹œ í‚¤ ì¬ì‚¬ìš©: {similar_cache_key[:16]}...")
        
        # ìœ ì‚¬ ì§ˆë¬¸ì˜ ìºì‹œ í‚¤ë¡œ Redisì—ì„œ ë‹µë³€ ê°€ì ¸ì˜¤ê¸°
        cached_answer = research_cache.get(similar_cache_key, domain=domain, prefix="final")
        if cached_answer:
            print(f"âœ… [ìœ ì‚¬ ì§ˆë¬¸ ìºì‹œ HIT] ìµœì¢… ë‹µë³€ ë°˜í™˜ (ìœ ì‚¬ ì§ˆë¬¸ì˜ ìºì‹œ í‚¤: {similar_cache_key[:16]}...)")
            
            # ë¦¬í¬íŠ¸ ë³¸ë¬¸ ì¶”ì¶œ ë° ì¸ì‚¬ ë©˜íŠ¸ ìƒì„± (ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼)
            cached_content = cached_answer["content"]
            report_body = cached_content.strip()
            
            # [GREETING] íƒœê·¸ ì œê±°
            if "[GREETING]" in cached_content and "[/GREETING]" in cached_content:
                match = re.search(r'\[GREETING\](.*?)\[/GREETING\]', cached_content, re.DOTALL)
                if match:
                    report_body = cached_content.replace(match.group(0), "").strip()
            
            if not report_body or len(report_body) < 50:
                report_body = cached_content.strip()
            
            if len(report_body) >= 200:
                # ì¸ì‚¬ ë©˜íŠ¸ ìƒì„± (ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼)
                print(f"âœ… [ìœ ì‚¬ ì§ˆë¬¸ ì²˜ë¦¬] ë¦¬í¬íŠ¸ ë³¸ë¬¸ì€ ìºì‹œì—ì„œ ê°€ì ¸ì˜´ ({len(report_body)}ì), ì¸ì‚¬ ë©˜íŠ¸ëŠ” ìƒˆë¡œ ìƒì„±")
                
                # final_report_generationê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì¸ì‚¬ ë©˜íŠ¸ ìƒì„±
                greeting_model_config = {
                    "model": configurable.final_report_model,
                    "max_tokens": configurable.final_report_model_max_tokens,
                    "api_key": get_api_key_for_model(configurable.final_report_model, config),
                }
                
                messages_context = get_buffer_string(messages) if messages else last_user_message
                
                greeting_prompt = f"""ë‹¹ì‹ ì€ ì½”ë”© AI ë„êµ¬ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì§ˆë¬¸ì— ë§ëŠ” ìì—°ìŠ¤ëŸ½ê³  ìƒì„¸í•œ ì¸ì‚¬ ë©˜íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì‚¬ìš©ì ë©”ì‹œì§€:
{messages_context}

**ì›ì¹™:**
- ì‚¬ìš©ìì˜ í˜„ì¬ ì§ˆë¬¸ ë‚´ìš©ê³¼ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ê·¸ì— ë§ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ë©˜íŠ¸ë¥¼ ìƒì„±
- ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œ(íŒ€ ê·œëª¨, ëª©ì , ìš”êµ¬ì‚¬í•­, ë„ë©”ì¸ ë“±)ë¥¼ ë°˜ì˜
- ì§ˆë¬¸ì— ì–¸ê¸‰ëœ êµ¬ì²´ì ì¸ ë‚´ìš©(íŒ€ ê·œëª¨, ëª©ì , ìš”êµ¬ì‚¬í•­ ë“±)ì„ ë°˜ë“œì‹œ í¬í•¨
- ìì—°ìŠ¤ëŸ½ê³  ì¹œì ˆí•œ í†¤ ìœ ì§€
- ì ì ˆí•œ ê¸¸ì´ (40-100ì ì •ë„, ë„ˆë¬´ ì§§ì§€ ì•Šê²Œ)

ì¸ì‚¬ ë©˜íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš” ([GREETING] íƒœê·¸ ì—†ì´, ë‹¤ë¥¸ ì„¤ëª… ì—†ì´):"""
                
                try:
                    greeting_model = configurable_model.with_config(greeting_model_config)
                    greeting_response = await greeting_model.ainvoke([HumanMessage(content=greeting_prompt)])
                    greeting = str(greeting_response.content).strip().strip('"\'`').strip()
                    
                    if not greeting or len(greeting) < 30:
                        greeting = f"ë„¤! {last_user_message[:30]}ì— ëŒ€í•´ ì¡°ì‚¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
                    
                    print(f"âœ… [ìœ ì‚¬ ì§ˆë¬¸ ì²˜ë¦¬] ì¸ì‚¬ ë©˜íŠ¸ ìƒì„± ì™„ë£Œ: '{greeting}'")
                    
                    return Command(
                        goto="__end__",
                        update={"messages": [
                            AIMessage(content=greeting),
                            AIMessage(content=report_body)
                        ]}
                    )
                except Exception as e:
                    print(f"âš ï¸ [ìœ ì‚¬ ì§ˆë¬¸ ì²˜ë¦¬] ì¸ì‚¬ ë©˜íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
                    greeting = f"ë„¤! {last_user_message[:30]}ì— ëŒ€í•´ ì¡°ì‚¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
                    return Command(
                        goto="__end__",
                        update={"messages": [
                            AIMessage(content=greeting),
                            AIMessage(content=report_body)
                        ]}
                    )
    
    # ìºì‹œ ë¯¸ìŠ¤ ë° ìœ ì‚¬ ì§ˆë¬¸ë„ ì—†ìŒ â†’ ìƒˆë¡œ ìƒì„±
    print(f"âš ï¸ [ìºì‹œ MISS + ìœ ì‚¬ ì§ˆë¬¸ ì—†ìŒ] ìƒˆë¡œ ìƒì„± ì§„í–‰")
    
    model_config = {
        "model": configurable.research_model,
        "max_tokens": configurable.research_model_max_tokens,
        "api_key": get_api_key_for_model(configurable.research_model, config),
    }
    
    clarification_model = (
        configurable_model
        .with_structured_output(ClarifyWithUser)
        .with_retry(stop_after_attempt=configurable.max_structured_output_retries)
        .with_config(model_config)
    )
    
    prompt_content = clarify_with_user_instructions.format(
        messages=get_buffer_string(messages),
        date=get_today_str(),
        domain=domain,
        is_followup="YES" if is_followup else "NO"
    )
    
    response = await clarification_model.ainvoke([HumanMessage(content=prompt_content)])
    
    # ğŸš¨ ì£¼ì œ ê´€ë ¨ì„± ì²´í¬ (í•­ìƒ ì‹¤í–‰!)
    if not response.is_on_topic:
        print(f"âš ï¸ [DEBUG] ì£¼ì œì—ì„œ ë²—ì–´ë‚œ ì§ˆë¬¸ ê°ì§€")
        return Command(
            goto="__end__",
            update={"messages": [AIMessage(content=response.off_topic_message)]}
        )
    
    # ëª…í™•í™” ë¹„í™œì„±í™” ì‹œ ë°”ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¡œ (ì£¼ì œ ê²€ì¦ í›„)
    if not configurable.allow_clarification:
        print(f"âœ… [DEBUG] ì£¼ì œ ê²€ì¦ í†µê³¼ - ë°”ë¡œ ì—°êµ¬ ì‹œì‘")
        return Command(
            goto="write_research_brief",
            update={
                "messages": [AIMessage(content=response.verification)],
                "normalized_query": normalized  # ğŸ†• ì •ê·œí™” ì •ë³´ ì €ì¥
            }
        )
    
    # ëª…í™•í™” í•„ìš” ì—¬ë¶€ ì²´í¬
    if response.need_clarification:
        return Command(
            goto="__end__",
            update={"messages": [AIMessage(content=response.question)]}
        )
    else:
        return Command(
            goto="write_research_brief",
            update={
                "messages": [AIMessage(content=response.verification)],
                "normalized_query": normalized  # ğŸ†• ì •ê·œí™” ì •ë³´ ì €ì¥
            }
        )


async def write_research_brief(
    state: AgentState, config: RunnableConfig
) -> Command[Literal["research_supervisor"]]:
    """ì—°êµ¬ ê³„íš ìˆ˜ë¦½"""
    
    # re ëª¨ë“ˆì„ í•¨ìˆ˜ ë‚´ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ importí•˜ì—¬ ìŠ¤ì½”í”„ ë¬¸ì œ í•´ê²°
    import re
    
    configurable = Configuration.from_runnable_config(config)
    domain = state.get("domain", "AI ì„œë¹„ìŠ¤")
    domain_guide = DOMAIN_GUIDES.get(domain, "")
    
    research_model_config = {
        "model": configurable.research_model,
        "max_tokens": configurable.research_model_max_tokens,
        "api_key": get_api_key_for_model(configurable.research_model, config),
    }
    
    research_model = (
        configurable_model
        .with_structured_output(ResearchQuestion)
        .with_retry(stop_after_attempt=configurable.max_structured_output_retries)
        .with_config(research_model_config)
    )
    
    # domain_guide í¬ë§·íŒ… (transform_messagesì—ì„œë„ ì‚¬ìš©)
    try:
        formatted_domain_guide_for_research = domain_guide.format(
            date=get_today_str(),
            current_year=get_current_year(),
            current_month_year=get_current_month_year()
        )
    except KeyError:
        formatted_domain_guide_for_research = domain_guide
    
    # Messages ê°€ì ¸ì˜¤ê¸° ë° Follow-up íŒë‹¨
    messages_list = state.get("messages", [])
    human_messages = [msg for msg in messages_list if isinstance(msg, HumanMessage)]
    question_number = len(human_messages)
    is_followup = question_number > 1
    
    # ì´ì „ ë„êµ¬ ì¶”ì¶œ (Follow-upì¸ ê²½ìš°) - ëª¨ë“  AI ë©”ì‹œì§€ì—ì„œ ì¶”ì¶œ
    previous_tools = ""
    if is_followup:
        all_tools = []
        for msg in reversed(messages_list[:-1]):  # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì œì™¸
            if isinstance(msg, AIMessage) and hasattr(msg, 'content'):
                content = str(msg.content)
                # ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ ë„êµ¬ëª… ì¶”ì¶œ
                # íŒ¨í„´ 1: ğŸ“Š [ë„êµ¬ëª…]
                tools_found = re.findall(r'ğŸ“Š\s+([^\n]+)', content)
                if tools_found:
                    all_tools.extend([t.strip() for t in tools_found])
                # íŒ¨í„´ 2: ## ğŸ“Š [ë„êµ¬ëª…]
                tools_found2 = re.findall(r'##\s+ğŸ“Š\s+([^\n]+)', content)
                if tools_found2:
                    all_tools.extend([t.strip() for t in tools_found2])
                # íŒ¨í„´ 3: **1ìˆœìœ„: [ë„êµ¬ëª…]**, **2ìˆœìœ„: [ë„êµ¬ëª…]**
                tools_found3 = re.findall(r'\*\*[0-9]+ìˆœìœ„:\s*([^\*]+)\*\*', content)
                if tools_found3:
                    all_tools.extend([t.strip() for t in tools_found3])
                # íŒ¨í„´ 4: **ìµœì¢… ì¶”ì²œ: [ë„êµ¬ëª…]**
                tools_found4 = re.findall(r'\*\*ìµœì¢… ì¶”ì²œ:\s*([^\*]+)\*\*', content)
                if tools_found4:
                    all_tools.extend([t.strip() for t in tools_found4])
        
        # ì¤‘ë³µ ì œê±°í•˜ê³  ìˆœì„œ ìœ ì§€
        seen = set()
        unique_tools = []
        for tool in all_tools:
            # ë„êµ¬ëª… ì •ì œ (ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°)
            tool_clean = re.sub(r'[\(\)\[\]ì›”\s\$0-9]+', '', tool).strip()
            if tool_clean and tool_clean not in seen and len(tool_clean) > 2:
                seen.add(tool_clean)
                unique_tools.append(tool_clean)
        
        previous_tools = ", ".join(unique_tools[:10])  # ìµœëŒ€ 10ê°œ
        print(f"ğŸ” [DEBUG] write_research_brief - ì´ì „ ì¶”ì²œ ë„êµ¬ ì¶”ì¶œ: {previous_tools}")
    
    prompt_content = transform_messages_into_research_topic_prompt.format(
        messages=get_buffer_string(messages_list),
        date=get_today_str(),
        current_year=get_current_year(),
        current_month_year=get_current_month_year(),
        domain=domain,
        domain_guide=formatted_domain_guide_for_research,
        is_followup="YES" if is_followup else "NO",
        previous_tools=previous_tools if previous_tools else "ì—†ìŒ",
        question_type="comparison"  # ì„ì‹œê°’, LLMì´ íŒë‹¨í•œ ê°’ìœ¼ë¡œ ëŒ€ì²´ë¨
    )
    
    response = await research_model.ainvoke([HumanMessage(content=prompt_content)])
    
    # ì§ˆë¬¸ ìœ í˜•ì€ LLMì´ ìŠ¤ìŠ¤ë¡œ íŒë‹¨ (response.question_type ì‚¬ìš©)
    question_type = response.question_type if hasattr(response, 'question_type') else "comparison"
    
    print(f"ğŸ” [DEBUG] write_research_brief - Messages: {len(messages_list)}ê°œ, ì§ˆë¬¸ ìˆœì„œ: {question_number}ë²ˆì§¸, Follow-up: {is_followup}, ì§ˆë¬¸ìœ í˜•: {question_type} (LLM íŒë‹¨), ì´ì „ ë„êµ¬: {previous_tools}")
    
    # ë””ë²„ê¹…: Research Briefì™€ ì œì•½ ì¡°ê±´ í™•ì¸
    print(f"ğŸ” [DEBUG] Research Brief: {response.research_brief[:200]}...")
    print(f"ğŸ” [DEBUG] Hard Constraints ì¶”ì¶œ: {response.hard_constraints}")
    
    # ì œì•½ ì¡°ê±´ì„ dictë¡œ ë³€í™˜í•˜ì—¬ stateì— ì €ì¥
    constraints = response.hard_constraints.model_dump() if hasattr(response, 'hard_constraints') and response.hard_constraints else {}
    
    # domain_guideë„ í¬ë§·íŒ… í•„ìš” (current_year ë“± í¬í•¨)
    try:
        formatted_domain_guide = domain_guide.format(
            date=get_today_str(),
            current_year=get_current_year(),
            current_month_year=get_current_month_year()
        )
    except KeyError:
        # í¬ë§·íŒ… ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        formatted_domain_guide = domain_guide
    
    supervisor_system_prompt = lead_researcher_prompt.format(
        date=get_today_str(),
        current_year=get_current_year(),
        current_month_year=get_current_month_year(),
        domain=domain,
        domain_guide=formatted_domain_guide,
        max_concurrent_research_units=configurable.max_concurrent_research_units,
        max_researcher_iterations=configurable.max_researcher_iterations
    )
    
    return Command(
        goto="research_supervisor",
        update={
            "research_brief": response.research_brief,
            "question_type": response.question_type,  # LLMì´ íŒë‹¨í•œ ì§ˆë¬¸ ìœ í˜• ì €ì¥
            "constraints": constraints,  # ì œì•½ ì¡°ê±´ ì €ì¥
            "supervisor_messages": {
                "type": "override",
                "value": [
                    SystemMessage(content=supervisor_system_prompt),
                    HumanMessage(content=response.research_brief)
                ]
            }
        }
    )


async def supervisor(
    state: SupervisorState, config: RunnableConfig
) -> Command[Literal["supervisor_tools"]]:
    """ì—°êµ¬ ìŠˆí¼ë°”ì´ì € (ì—°êµ¬ ê³„íš ë° ìœ„ì„)"""
    
    configurable = Configuration.from_runnable_config(config)
    
    research_model_config = {
        "model": configurable.research_model,
        "max_tokens": configurable.research_model_max_tokens,
        "api_key": get_api_key_for_model(configurable.research_model, config),
    }
    
    tools = [ConductResearch, ResearchComplete, think_tool]
    
    research_model = (
        configurable_model
        .bind_tools(tools)
        .with_retry(stop_after_attempt=configurable.max_structured_output_retries)
        .with_config(research_model_config)
    )
    
    supervisor_messages = state.get("supervisor_messages", [])
    response = await research_model.ainvoke(supervisor_messages)
    
    return Command(
        goto="supervisor_tools",
        update={
            "supervisor_messages": [response],
            "research_iterations": state.get("research_iterations", 0) + 1
        }
    )


async def supervisor_tools(
    state: SupervisorState, config: RunnableConfig
) -> Command[Literal["supervisor", "__end__"]]:
    """ìŠˆí¼ë°”ì´ì € ë„êµ¬ ì‹¤í–‰"""
    
    configurable = Configuration.from_runnable_config(config)
    supervisor_messages = state.get("supervisor_messages", [])
    research_iterations = state.get("research_iterations", 0)
    most_recent_message = supervisor_messages[-1]
    
    # ì¢…ë£Œ ì¡°ê±´
    exceeded_iterations = research_iterations > configurable.max_researcher_iterations
    no_tool_calls = not most_recent_message.tool_calls
    research_complete_called = any(
        tc["name"] == "ResearchComplete" for tc in most_recent_message.tool_calls
    )
    
    if exceeded_iterations or no_tool_calls or research_complete_called:
        # notes ì¶”ì¶œ (ëª¨ë“  ToolMessageì—ì„œ ì¶”ì¶œ)
        notes = get_notes_from_tool_calls(supervisor_messages)
        
        # ë””ë²„ê¹…: notes í™•ì¸
        print(f"ğŸ” [DEBUG] supervisor_tools ì¢…ë£Œ - notes ê°œìˆ˜: {len(notes)}")
        print(f"ğŸ” [DEBUG] notes ë‚´ìš©: {notes[:2] if notes else 'ì—†ìŒ'}")
        
        # notesê°€ ë¹„ì–´ìˆìœ¼ë©´ raw_notesì—ì„œ ì¶”ì¶œ ì‹œë„
        if not notes:
            raw_notes = state.get("raw_notes", [])
            if raw_notes:
                print(f"ğŸ” [DEBUG] raw_notesì—ì„œ notes ì¶”ì¶œ ì‹œë„: {len(raw_notes)}ê°œ")
                notes = raw_notes if isinstance(raw_notes, list) else [raw_notes]
        
        return Command(
            goto="__end__",
            update={
                "notes": notes if notes else ["ì—°êµ¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."],
                "research_brief": state.get("research_brief", "")
            }
        )
    
    # ë„êµ¬ ì‹¤í–‰
    all_tool_messages = []
    update_payload = {"supervisor_messages": []}
    
    # ëª¨ë“  tool_calls ì²˜ë¦¬
    for tc in most_recent_message.tool_calls:
        if tc["name"] == "think_tool":
            all_tool_messages.append(ToolMessage(
                content=f"ì‚¬ê³  ê¸°ë¡: {tc['args']['reflection']}",
                name="think_tool",
                tool_call_id=tc["id"]
            ))
        
        elif tc["name"] == "ConductResearch":
            # ë‚˜ì¤‘ì— ì¼ê´„ ì²˜ë¦¬
            pass
        
        elif tc["name"] == "ResearchComplete":
            all_tool_messages.append(ToolMessage(
                content="ì—°êµ¬ ì™„ë£Œ í™•ì¸",
                name="ResearchComplete",
                tool_call_id=tc["id"]
            ))
        
        else:
            # ì•Œ ìˆ˜ ì—†ëŠ” tool callì—ë„ ì‘ë‹µ (ì˜¤ë¥˜ ë°©ì§€)
            all_tool_messages.append(ToolMessage(
                content=f"ë„êµ¬ '{tc['name']}'ëŠ” ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                name=tc["name"],
                tool_call_id=tc["id"]
            ))
    
    # ConductResearch ì¼ê´„ ì²˜ë¦¬
    conduct_calls = [tc for tc in most_recent_message.tool_calls if tc["name"] == "ConductResearch"]
    
    if conduct_calls:
        # researcher_subgraph import (ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
        from app.agent.graph import researcher_subgraph
        
        allowed_calls = conduct_calls[:configurable.max_concurrent_research_units]
        skipped_calls = conduct_calls[configurable.max_concurrent_research_units:]
        
        # ë³‘ë ¬ ì—°êµ¬ ì‹¤í–‰
        tasks = [
            researcher_subgraph.ainvoke({
                "researcher_messages": [HumanMessage(content=tc["args"]["research_topic"])],
                "research_topic": tc["args"]["research_topic"],
                "domain": state.get("domain")
            }, config)
            for tc in allowed_calls
        ]
        
        results = await asyncio.gather(*tasks)
        
        for observation, tc in zip(results, allowed_calls):
            all_tool_messages.append(ToolMessage(
                content=observation.get("compressed_research", "ì—°êµ¬ ì‹¤íŒ¨"),
                name=tc["name"],
                tool_call_id=tc["id"]
            ))
        
        # ì œí•œ ì´ˆê³¼ë¡œ ê±´ë„ˆë›´ í˜¸ì¶œì—ë„ ì‘ë‹µ (ì˜¤ë¥˜ ë°©ì§€)
        for tc in skipped_calls:
            all_tool_messages.append(ToolMessage(
                content="ë³‘ë ¬ ì—°êµ¬ ì œí•œìœ¼ë¡œ ë‹¤ìŒ ë°˜ë³µì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤.",
                name=tc["name"],
                tool_call_id=tc["id"]
            ))
        
        # raw_notes ìˆ˜ì§‘
        raw_notes_list = []
        for obs in results:
            obs_raw_notes = obs.get("raw_notes", [])
            if obs_raw_notes:
                if isinstance(obs_raw_notes, list):
                    raw_notes_list.extend(obs_raw_notes)
                else:
                    raw_notes_list.append(str(obs_raw_notes))
        
        if raw_notes_list:
            update_payload["raw_notes"] = raw_notes_list
            print(f"ğŸ” [DEBUG] raw_notes ìˆ˜ì§‘: {len(raw_notes_list)}ê°œ")
    
    update_payload["supervisor_messages"] = all_tool_messages
    return Command(goto="supervisor", update=update_payload)


async def researcher(
    state: ResearcherState, config: RunnableConfig
) -> Command[Literal["researcher_tools"]]:
    """ê°œë³„ ì—°êµ¬ì› (Vector DB ì¡°íšŒ â†’ ì›¹ ê²€ìƒ‰)"""
    
    configurable = Configuration.from_runnable_config(config)
    domain = state.get("domain", "AI ì„œë¹„ìŠ¤")
    domain_guide = DOMAIN_GUIDES.get(domain, "")
    
    research_model_config = {
        "model": configurable.research_model,
        "max_tokens": configurable.research_model_max_tokens,
        "api_key": get_api_key_for_model(configurable.research_model, config),
    }
    
    # ========== ğŸ†• Vector DB ê²€ìƒ‰ ë„êµ¬ ì¶”ê°€ ==========
    async def vector_search(query: str) -> str:
        """Vector DBì—ì„œ Facts ê²€ìƒ‰ (ì›¹ ê²€ìƒ‰ ì „ ìš°ì„  ì‹œë„, threshold ì™„í™”)"""
        # thresholdë¥¼ 0.75 â†’ 0.65ë¡œ ë‚®ì¶°ì„œ ë” ë§ì€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
        facts = vector_store.search_facts(query, limit=5, score_threshold=0.65)
        
        if not facts:
            return "Vector DBì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•©ë‹ˆë‹¤."
        
        # ê²°ê³¼ê°€ 3ê°œ ì´ìƒì´ë©´ ì¶©ë¶„í•˜ë‹¤ê³  íŒë‹¨
        if len(facts) >= 3:
            formatted = f"âœ… Vector DBì—ì„œ {len(facts)}ê°œ ê´€ë ¨ ì •ë³´ ë°œê²¬ (ì¶©ë¶„í•¨):\n\n"
            for idx, fact in enumerate(facts, 1):
                age_days = (datetime.now().timestamp() - fact['created_at']) / 86400
                formatted += f"{idx}. [ì‹ ë¢°ë„ {fact['score']:.2f}, {age_days:.0f}ì¼ ì „]\n"
                formatted += f"   {fact['text'][:300]}...\n"
                formatted += f"   ì¶œì²˜: {fact['source']} ({fact.get('url', '')[:50]}...)\n\n"
            return formatted
        
        # ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ì›¹ ê²€ìƒ‰ í•„ìš”
        formatted = f"âš ï¸ Vector DBì—ì„œ {len(facts)}ê°œ ê´€ë ¨ ì •ë³´ ë°œê²¬ (ë¶€ì¡±í•¨, ì›¹ ê²€ìƒ‰ í•„ìš”):\n\n"
        for idx, fact in enumerate(facts, 1):
            age_days = (datetime.now().timestamp() - fact['created_at']) / 86400
            formatted += f"{idx}. [ì‹ ë¢°ë„ {fact['score']:.2f}, {age_days:.0f}ì¼ ì „]\n"
            formatted += f"   {fact['text'][:300]}...\n"
            formatted += f"   ì¶œì²˜: {fact['source']} ({fact.get('url', '')[:50]}...)\n\n"
        formatted += "ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”."
        return formatted
    
    # ê²€ìƒ‰ ë„êµ¬ ì •ì˜
    async def web_search(query: str) -> str:
        """ì›¹ ê²€ìƒ‰ ë„êµ¬ (Vector DBì— ì •ë³´ê°€ ì—†ì„ ë•Œ ì‚¬ìš©)"""
        result = await searcher.search(
            query=query,
            max_results=configurable.search_max_results,
            search_depth=configurable.search_depth
        )
        
        if not result["success"]:
            return f"ê²€ìƒ‰ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
        
        # ========== ğŸ†• ê²€ìƒ‰ ê²°ê³¼ë¥¼ Vector DBì— ì €ì¥ ==========
        facts_to_store = []
        for r in result["results"]:
            facts_to_store.append({
                "text": f"{r['title']}: {r['content']}",
                "source": result['source'],
                "url": r['url'],
                "metadata": {
                    "score": r.get('score', 0),
                    "query": query
                }
            })
        
        if facts_to_store:
            vector_store.add_facts(facts_to_store, ttl_days=30)
        
        # ê²°ê³¼ í¬ë§·íŒ…
        formatted = f"ê²€ìƒ‰ ê²°ê³¼ ({result['source']}):\n\n"
        for idx, r in enumerate(result["results"], 1):
            formatted += f"{idx}. {r['title']}\n"
            formatted += f"   URL: {r['url']}\n"
            formatted += f"   ë‚´ìš©: {r['content'][:200]}...\n\n"
        
        return formatted
    
    tools = [vector_search, web_search, think_tool]
    
    research_model = (
        configurable_model
        .bind_tools(tools)
        .with_retry(stop_after_attempt=configurable.max_structured_output_retries)
        .with_config(research_model_config)
    )
    
    # domain_guideë„ í¬ë§·íŒ… í•„ìš” (current_year ë“± í¬í•¨)
    try:
        formatted_domain_guide_researcher = domain_guide.format(
            date=get_today_str(),
            current_year=get_current_year(),
            current_month_year=get_current_month_year()
        )
    except KeyError:
        # í¬ë§·íŒ… ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        formatted_domain_guide_researcher = domain_guide
    
    researcher_prompt = research_system_prompt.format(
        domain=domain,
        domain_guide=formatted_domain_guide_researcher,
        date=get_today_str(),
        current_year=get_current_year(),
        current_month_year=get_current_month_year()
    )
    
    messages = [SystemMessage(content=researcher_prompt)] + state.get("researcher_messages", [])
    response = await research_model.ainvoke(messages)
    
    return Command(
        goto="researcher_tools",
        update={
            "researcher_messages": [response],
            "tool_call_iterations": state.get("tool_call_iterations", 0) + 1
        }
    )


async def researcher_tools(
    state: ResearcherState, config: RunnableConfig
) -> Command[Literal["researcher", "compress_research"]]:
    """ì—°êµ¬ì› ë„êµ¬ ì‹¤í–‰"""
    
    configurable = Configuration.from_runnable_config(config)
    researcher_messages = state.get("researcher_messages", [])
    most_recent_message = researcher_messages[-1]
    
    # ë„êµ¬ í˜¸ì¶œ ì—†ìœ¼ë©´ ì¢…ë£Œ
    if not most_recent_message.tool_calls:
        return Command(goto="compress_research")
    
    # ë„êµ¬ ì‹¤í–‰
    tool_outputs = []
    
    for tc in most_recent_message.tool_calls:
        # ========== ğŸ†• Vector DB ê²€ìƒ‰ ì²˜ë¦¬ ==========
        if tc["name"] == "vector_search":
            # thresholdë¥¼ 0.75 â†’ 0.65ë¡œ ë‚®ì¶°ì„œ ë” ë§ì€ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            facts = vector_store.search_facts(tc["args"]["query"], limit=5, score_threshold=0.65)
            
            if facts:
                # ê²°ê³¼ê°€ 3ê°œ ì´ìƒì´ë©´ ì¶©ë¶„í•˜ë‹¤ê³  íŒë‹¨
                if len(facts) >= 3:
                    formatted = f"âœ… Vector DBì—ì„œ {len(facts)}ê°œ ê´€ë ¨ ì •ë³´ ë°œê²¬ (ì¶©ë¶„í•¨):\n\n"
                    for idx, fact in enumerate(facts, 1):
                        from datetime import datetime
                        age_days = (datetime.now().timestamp() - fact['created_at']) / 86400
                        formatted += f"{idx}. [ì‹ ë¢°ë„ {fact['score']:.2f}, {age_days:.0f}ì¼ ì „]\n"
                        formatted += f"   {fact['text'][:300]}...\n"
                        formatted += f"   ì¶œì²˜: {fact['source']} ({fact.get('url', '')[:50]}...)\n\n"
                    content = formatted
                else:
                    # ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ì›¹ ê²€ìƒ‰ í•„ìš”
                    formatted = f"âš ï¸ Vector DBì—ì„œ {len(facts)}ê°œ ê´€ë ¨ ì •ë³´ ë°œê²¬ (ë¶€ì¡±í•¨, ì›¹ ê²€ìƒ‰ í•„ìš”):\n\n"
                    for idx, fact in enumerate(facts, 1):
                        from datetime import datetime
                        age_days = (datetime.now().timestamp() - fact['created_at']) / 86400
                        formatted += f"{idx}. [ì‹ ë¢°ë„ {fact['score']:.2f}, {age_days:.0f}ì¼ ì „]\n"
                        formatted += f"   {fact['text'][:300]}...\n"
                        formatted += f"   ì¶œì²˜: {fact['source']} ({fact.get('url', '')[:50]}...)\n\n"
                    formatted += "ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”."
                    content = formatted
            else:
                content = "Vector DBì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”."
            
            tool_outputs.append(ToolMessage(
                content=content,
                name="vector_search",
                tool_call_id=tc["id"]
            ))
        
        elif tc["name"] == "web_search":
            # êµì°¨ ê²€ì¦ í™œì„±í™” (Tavily + Serper Fallback)
            result = await searcher.search(
                query=tc["args"]["query"],
                max_results=configurable.search_max_results,
                enable_verification=True  # êµì°¨ ê²€ì¦ í™œì„±í™”
            )
            
            if result["success"]:
                # ========== ğŸ†• ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ Vector DBì— ì €ì¥ ==========
                facts_to_store = []
                for r in result["results"]:
                    facts_to_store.append({
                        "text": f"{r['title']}: {r['content']}",
                        "source": result['source'],
                        "url": r['url'],
                        "metadata": {
                            "score": r.get('score', 0),
                            "query": tc["args"]["query"],
                            "is_official": r.get('is_official', False)
                        }
                    })
                
                if facts_to_store:
                    vector_store.add_facts(facts_to_store, ttl_days=30)
                
                source_info = result.get("source", "unknown")
                if source_info == "verified":
                    verified_info = f"êµì°¨ ê²€ì¦ë¨ (Tavily: {result.get('tavily_count', 0)}ê°œ, DuckDuckGo: {result.get('ddg_count', 0)}ê°œ â†’ {result.get('verified_count', 0)}ê°œ ê²€ì¦)"
                else:
                    verified_info = f"({source_info})"
                
                formatted = f"ê²€ìƒ‰ ê²°ê³¼ {verified_info}:\n\n"
                
                # ê³µì‹ ì‚¬ì´íŠ¸ ê²°ê³¼ í‘œì‹œ
                official_results = [r for r in result["results"] if r.get("is_official", False)]
                if official_results:
                    formatted += "ğŸ“Œ ê³µì‹ ì‚¬ì´íŠ¸ ê²°ê³¼:\n"
                    for idx, r in enumerate(official_results, 1):
                        formatted += f"{idx}. {r['title']}\n   URL: {r['url']}\n   {r['content'][:200]}...\n\n"
                
                # ì¼ë°˜ ê²°ê³¼
                other_results = [r for r in result["results"] if not r.get("is_official", False)]
                if other_results:
                    if official_results:
                        formatted += "ê¸°íƒ€ ê²°ê³¼:\n"
                    for idx, r in enumerate(other_results, len(official_results) + 1):
                        formatted += f"{idx}. {r['title']}\n   URL: {r['url']}\n   {r['content'][:200]}...\n\n"
                
                # ê°€ê²© ì •ë³´ ì¶”ì¶œ ë° í‘œì‹œ (ê°€ê²© ê´€ë ¨ ì¿¼ë¦¬ì¸ ê²½ìš°)
                if any(kw in tc["args"]["query"].lower() for kw in ["pricing", "cost", "subscription", "plan", "ê°€ê²©"]):
                    pricing_info = searcher.extract_pricing_info(result["results"])
                    if pricing_info["pricing"]:
                        formatted += f"\nğŸ’° ì¶”ì¶œëœ ê°€ê²© ì •ë³´ (ì‹ ë¢°ë„: {pricing_info['confidence']}):\n"
                        for p in pricing_info["pricing"]:
                            formatted += f"- {p['plan']}: {p['price']} (ì¶œì²˜: {len(p['sources'])}ê°œ, ê³µì‹: {p['official_count']}ê°œ)\n"
                
                content = formatted
            else:
                content = f"ê²€ìƒ‰ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
            
            tool_outputs.append(ToolMessage(
                content=content,
                name="web_search",
                tool_call_id=tc["id"]
            ))
        
        elif tc["name"] == "think_tool":
            tool_outputs.append(ToolMessage(
                content=f"ì‚¬ê³ : {tc['args']['reflection']}",
                name="think_tool",
                tool_call_id=tc["id"]
            ))
        
        else:
            # ì•Œ ìˆ˜ ì—†ëŠ” tool callì—ë„ ì‘ë‹µ (ì˜¤ë¥˜ ë°©ì§€)
            tool_outputs.append(ToolMessage(
                content=f"ë„êµ¬ '{tc['name']}'ëŠ” ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                name=tc["name"],
                tool_call_id=tc["id"]
            ))
    
    # ì¢…ë£Œ ì¡°ê±´
    exceeded = state.get("tool_call_iterations", 0) >= configurable.max_react_tool_calls
    
    if exceeded:
        return Command(goto="compress_research", update={"researcher_messages": tool_outputs})
    
    return Command(goto="researcher", update={"researcher_messages": tool_outputs})


async def compress_research(state: ResearcherState, config: RunnableConfig):
    """ì—°êµ¬ ê²°ê³¼ ì••ì¶•"""
    
    configurable = Configuration.from_runnable_config(config)
    
    compression_model = configurable_model.with_config({
        "model": configurable.compression_model,
        "max_tokens": configurable.compression_model_max_tokens,
        "api_key": get_api_key_for_model(configurable.compression_model, config),
    })
    
    researcher_messages = state.get("researcher_messages", [])
    researcher_messages.append(HumanMessage(content=compress_research_simple_human_message))
    
    compression_prompt = compress_research_system_prompt.format(date=get_today_str())
    messages = [SystemMessage(content=compression_prompt)] + researcher_messages
    
    try:
        response = await compression_model.ainvoke(messages)
        
        raw_notes = "\n".join([
            str(msg.content) for msg in researcher_messages
            if isinstance(msg, (ToolMessage, AIMessage))
        ])
        
        return {
            "compressed_research": str(response.content),
            "raw_notes": [raw_notes]
        }
    
    except Exception as e:
        print(f"âŒ ì••ì¶• ì‹¤íŒ¨: {e}")
        return {
            "compressed_research": "ì—°êµ¬ ê²°ê³¼ ì••ì¶• ì‹¤íŒ¨",
            "raw_notes": [""]
        }


async def run_decision_engine(state: AgentState, config: RunnableConfig):
    """Decision Engine ì‹¤í–‰ (ì˜ì‚¬ê²°ì • ì§ˆë¬¸ì¸ ê²½ìš°)"""
    
    # re ëª¨ë“ˆì„ í•¨ìˆ˜ ë‚´ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ importí•˜ì—¬ ìŠ¤ì½”í”„ ë¬¸ì œ í•´ê²°
    import re
    
    # ğŸš¨ ìµœì í™”: Decision ì§ˆë¬¸ ì—¬ë¶€ë¥¼ ë¨¼ì € í™•ì¸ (ë¹ ë¥¸ ë°˜í™˜)
    question_type = state.get("question_type", "comparison")
    messages_list = state.get("messages", [])
    last_user_message = str(messages_list[-1].content).lower() if messages_list else ""
    
    is_decision_question = (
        question_type in ["decision", "comparison"] or
        any(keyword in last_user_message for keyword in [
            "ì¤‘ í•˜ë‚˜ë§Œ", "í•˜ë‚˜ë§Œ", "ì„ íƒ", "ì–´ë–¤ ê²ƒì´", "ë§ì„ê¹Œ", "ì¶”ì²œ", "ì–´ë–¤ ë„êµ¬", 
            "ì¢‹ì„ê¹Œ", "ì í•©", "ìµœì í™”", "ì–´ë–¤ê²Œ", "ë­˜", "ë¬´ì—‡ì„", "ì–´ë–¤ê²Œ ì¢‹", "ì–´ë–¤ ê²ƒì´ ì¢‹",
            "ë¹„êµ", "vs", "ëŒ€ë¹„", "ì°¨ì´", "ì–´ë–¤ê²Œ ë‚˜ì€", "ë” ì¢‹ì€", "ì–´ëŠê²Œ", "ìµœì "
        ]) or
        "ì–´ë–¤ ë„êµ¬ê°€ ì¢‹ì„ê¹Œìš”" in last_user_message or
        ("ì–´ë–¤ ë„êµ¬" in last_user_message and "ì¢‹" in last_user_message) or
        ("vs" in last_user_message or "ëŒ€ë¹„" in last_user_message) or
        ("ìµœì í™”" in last_user_message and "ë„êµ¬" in last_user_message)  # ğŸ†• "ìµœì í™”ëœ ë„êµ¬" íŒ¨í„´
    )
    
    if not is_decision_question:
        # Decision ì§ˆë¬¸ì´ ì•„ë‹ˆë©´ Decision Engine ì‹¤í–‰ ì•ˆ í•¨ (ë¹ ë¥¸ ë°˜í™˜)
        return {}
    
    # ğŸš¨ ìµœì í™”: ì œì•½ ì¡°ê±´ì´ ë¶€ì¡±í•œì§€ ë¨¼ì € í™•ì¸ (LLM í˜¸ì¶œ ì „ì—)
    constraints = state.get("constraints", {})
    team_size = constraints.get("team_size") if constraints else None
    budget_max = constraints.get("budget_max") if constraints else None
    
    # ë©”ì‹œì§€ì—ì„œ íŒ€ ê·œëª¨ì™€ ì˜ˆì‚° ì¶”ì¶œ ì‹œë„ (ë¹ ë¥¸ í™•ì¸)
    if not team_size and messages_list:
        last_user_msg = str(messages_list[-1].content)
        team_size_match = re.search(r'(\d+)\s*ëª…', last_user_msg)
        if team_size_match:
            team_size = int(team_size_match.group(1))
    
    if not budget_max and messages_list:
        last_user_msg = str(messages_list[-1].content).lower()
        budget_patterns = [
            r'ì›”\s*\$?\s*(\d+)',
            r'\$?\s*(\d+)\s*ê¹Œì§€',
            r'\$?\s*(\d+)\s*ê°€ëŠ¥',
            r'\$?\s*(\d+)\s*ì´í•˜',
            r'\$?\s*(\d+)\s*ì´ë‚´',
        ]
        for pattern in budget_patterns:
            budget_match = re.search(pattern, last_user_msg)
            if budget_match:
                budget_max = float(budget_match.group(1))
                break
    
    # ì œì•½ ì¡°ê±´ì´ ë¶€ì¡±í•˜ë©´ ë¹ ë¥´ê²Œ ë°˜í™˜ (tool_facts ì¶”ì¶œ ì•ˆ í•¨)
    has_sufficient_constraints = team_size is not None or budget_max is not None
    if not has_sufficient_constraints:
        print(f"âš¡ [Decision Engine] ì œì•½ ì¡°ê±´ ë¶€ì¡± - ë¹ ë¥¸ ë°˜í™˜ (team_size: {team_size}, budget_max: {budget_max})")
        return {}  # route_after_researchì—ì„œ clarify_missing_constraintsë¡œ ë¼ìš°íŒ…
    
    # ì œì•½ ì¡°ê±´ì´ ì¶©ë¶„í•˜ë©´ tool_facts ì¶”ì¶œ ë° Decision Engine ì‹¤í–‰
    notes = state.get("notes", [])
    findings = "\n\n".join(notes)
    tool_facts = state.get("tool_facts", [])
    
    print(f"ğŸ” [Decision Engine DEBUG] is_decision_question: {is_decision_question}, tool_facts: {len(tool_facts) if tool_facts else 0}ê°œ")
    print(f"ğŸ” [Decision Engine DEBUG] findings ê¸¸ì´: {len(findings) if findings else 0}ì")
    
    # Findingsê°€ ìˆìœ¼ë©´ tool_facts ì¶”ì¶œ ì‹œë„ (ìµœì†Œ ê¸¸ì´ 50ìë¡œ ì™„í™”)
    if not tool_facts and findings and len(findings.strip()) >= 50:
        print(f"ğŸ” [Fact Extractor] Findingsì—ì„œ ë„êµ¬ ì‚¬ì‹¤ ì¶”ì¶œ ì‹œì‘ (Findings ê¸¸ì´: {len(findings)}ì)")
        try:
            extracted_facts = await extract_tool_facts(findings, config, max_retries=3)
            if extracted_facts:
                tool_facts = [fact.model_dump() for fact in extracted_facts]
                print(f"âœ… [Fact Extractor] {len(tool_facts)}ê°œ ë„êµ¬ ì‚¬ì‹¤ ì¶”ì¶œ ì™„ë£Œ")
                state["tool_facts"] = tool_facts
            else:
                print(f"âš ï¸ [Fact Extractor] ë„êµ¬ ì‚¬ì‹¤ ì¶”ì¶œ ì‹¤íŒ¨ (Findings ê¸¸ì´: {len(findings)}ì)")
        except Exception as e:
            print(f"âš ï¸ [Fact Extractor] ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
    
    # ğŸš¨ Decision ì§ˆë¬¸ì¸ë° tool_factsê°€ ì—†ìœ¼ë©´ Findingsì—ì„œ ë‹¤ì‹œ ì¶”ì¶œ ì‹œë„ (ë” ì ê·¹ì ìœ¼ë¡œ)
    if is_decision_question and not tool_facts:
        if findings and len(findings.strip()) >= 50:
            print(f"ğŸ” [Decision Engine] tool_facts ì—†ìŒ - Findingsì—ì„œ ì¬ì¶”ì¶œ ì‹œë„ (Findings ê¸¸ì´: {len(findings)}ì)")
            try:
                # ì¬ì‹œë„ ì‹œ ë” ê¸´ max_tokensë¡œ ì‹œë„ (ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ í™œìš©)
                extracted_facts = await extract_tool_facts(findings, config, max_retries=3)
                if extracted_facts:
                    tool_facts = [fact.model_dump() for fact in extracted_facts]
                    print(f"âœ… [Decision Engine] ì¬ì¶”ì¶œ ì„±ê³µ: {len(tool_facts)}ê°œ ë„êµ¬ ì‚¬ì‹¤")
                    state["tool_facts"] = tool_facts
                else:
                    print(f"âš ï¸ [Decision Engine] tool_facts ì¶”ì¶œ ì‹¤íŒ¨ - Findingsì—ì„œ ë„êµ¬ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ (Findings ê¸¸ì´: {len(findings)}ì)")
                    print(f"ğŸ” [Decision Engine] Findings ìƒ˜í”Œ (ì²˜ìŒ 500ì): {findings[:500]}")
            except Exception as e:
                print(f"âš ï¸ [Decision Engine] tool_facts ì¶”ì¶œ ì˜¤ë¥˜: {e}")
                import traceback
                traceback.print_exc()
        else:
            print(f"âš ï¸ [Decision Engine] findingsê°€ ë¶€ì¡±í•¨ ({len(findings) if findings else 0}ì, ìµœì†Œ 50ì í•„ìš”)")
    
    if not tool_facts:
        # Decision ì§ˆë¬¸ì¸ë° tool_factsê°€ ì—†ìœ¼ë©´ Decision Engine ì‹¤í–‰ ë¶ˆê°€
        print(f"ğŸš¨ [Decision Engine] Decision ì§ˆë¬¸ì´ì§€ë§Œ tool_facts ì—†ìŒ - Decision Engine ì‹¤í–‰ ë¶ˆê°€")
        # ğŸš¨ ì¤‘ìš”: tool_factsê°€ ì—†ìœ¼ë©´ decision_resultë„ ì—†ìœ¼ë¯€ë¡œ route_after_researchì—ì„œ cannot_answerë¡œ ê°
        # í•˜ì§€ë§Œ ì‚¬ìš©ìê°€ ì¼ë°˜ ë¦¬í¬íŠ¸ë¥¼ ì›í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ë¹ˆ dict ë°˜í™˜í•˜ì—¬ route_after_researchì—ì„œ ì²˜ë¦¬í•˜ë„ë¡ í•¨
        return {}
    
    # Decision Engine ì‹¤í–‰
    try:
        constraints = state.get("constraints", {})
        tech_stack = constraints.get("must_support_language", []) if constraints else []
        
        if not tech_stack and messages_list:
            # HumanMessageë§Œ ì°¾ì•„ì„œ ì‚¬ìš©ì ë©”ì‹œì§€ í™•ì¸
            human_messages = [msg for msg in messages_list if isinstance(msg, HumanMessage)]
            if human_messages:
                last_user_msg = str(human_messages[-1].content).lower()
            else:
                last_user_msg = ""
            # í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ì¶”ì¶œ (ë‹¤ì–‘í•œ íŒ¨í„´ ì¸ì‹, ë” ìœ ì—°í•˜ê²Œ)
            # ë°±ì—”ë“œ/í”„ë¡ íŠ¸ì—”ë“œ í‚¤ì›Œë“œì—ì„œ ìŠ¤íƒ ì¶”ì¶œ (ì¶”ì¸¡ì ì´ì§€ë§Œ ìœ ìš©í•œ ì •ë³´)
            if "ë°±ì—”ë“œ" in last_user_msg or "backend" in last_user_msg:
                if "java" not in [lang.lower() for lang in tech_stack]:
                    tech_stack.append("Java")
            if "í”„ë¡ íŠ¸ì—”ë“œ" in last_user_msg or "frontend" in last_user_msg or "í”„ë¡ íŠ¸" in last_user_msg:
                if "javascript" not in [lang.lower() for lang in tech_stack]:
                    tech_stack.append("JavaScript")
                if "typescript" not in [lang.lower() for lang in tech_stack]:
                    tech_stack.append("TypeScript")
            
            # ì¼ë°˜ì ì¸ í”„ë¡œê·¸ë˜ë° ì–¸ì–´ í‚¤ì›Œë“œ ë§¤ì¹­ (ë” ë§ì€ ì–¸ì–´ ì§€ì›)
            language_keywords_map = {
                "python": "Python",
                "java": "Java",
                "javascript": "JavaScript",
                "js": "JavaScript",
                "typescript": "TypeScript",
                "ts": "TypeScript",
                "go": "Go",
                "golang": "Go",
                "rust": "Rust",
                "c++": "C++",
                "cpp": "C++",
                "c#": "C#",
                "csharp": "C#",
                "php": "PHP",
                "ruby": "Ruby",
                "swift": "Swift",
                "kotlin": "Kotlin",
                "scala": "Scala",
                "node.js": "JavaScript",
                "nodejs": "JavaScript",
                "node": "JavaScript",
                "dart": "Dart",
                "flutter": "Dart",
                "r": "R",
                "matlab": "MATLAB",
                "perl": "Perl",
                "lua": "Lua"
            }
            
            for lang_keyword, lang_name in language_keywords_map.items():
                # ë‹¨ì–´ ê²½ê³„ë¥¼ ê³ ë ¤í•œ ë§¤ì¹­ (ë” ì •í™•í•˜ê²Œ)
                pattern = r'\b' + re.escape(lang_keyword) + r'\b'
                if re.search(pattern, last_user_msg, re.IGNORECASE):
                    if lang_name not in tech_stack:
                        tech_stack.append(lang_name)
            
            # í”„ë ˆì„ì›Œí¬/ë¼ì´ë¸ŒëŸ¬ë¦¬ í‚¤ì›Œë“œì—ì„œ ì–¸ì–´ ì¶”ë¡ 
            if "react" in last_user_msg or "vue" in last_user_msg or "angular" in last_user_msg:
                if "JavaScript" not in tech_stack:
                    tech_stack.append("JavaScript")
                if "TypeScript" not in tech_stack:
                    tech_stack.append("TypeScript")
            if "spring" in last_user_msg:
                if "Java" not in tech_stack:
                    tech_stack.append("Java")
            if "django" in last_user_msg or "flask" in last_user_msg:
                if "Python" not in tech_stack:
                    tech_stack.append("Python")
        
        # workflow_focus ì¶”ì¶œ (ëª¨ë“  ì‚¬ìš©ì ë©”ì‹œì§€ì—ì„œ í™•ì¸)
        workflow_focus = []
        if messages_list:
            # ëª¨ë“  HumanMessageì—ì„œ í‚¤ì›Œë“œ í™•ì¸ (ìµœì‹  ë©”ì‹œì§€ ìš°ì„ )
            human_messages = [msg for msg in messages_list if isinstance(msg, HumanMessage)]
            if human_messages:
                # ëª¨ë“  ì‚¬ìš©ì ë©”ì‹œì§€ë¥¼ í•©ì³ì„œ í™•ì¸ (ìµœì‹  ë©”ì‹œì§€ê°€ ìš°ì„ ì´ì§€ë§Œ ì´ì „ ë§¥ë½ë„ ì°¸ê³ )
                all_user_text = " ".join([str(msg.content).lower() for msg in human_messages])
                last_user_msg = str(human_messages[-1].content).lower()
                
                # ì½”ë“œ ë¦¬ë·° ìš”êµ¬ì‚¬í•­ í™•ì¸ (ë” í¬ê´„ì ì´ê³  ìœ ì—°í•˜ê²Œ)
                review_keywords = [
                    "pr ë¦¬ë·°", "pull request ë¦¬ë·°", "pull request", "pr",
                    "ì½”ë“œ ë¦¬ë·°", "ë¦¬ë·° ì§€ì›", "ë¦¬ë·°ê¹Œì§€", "ë¦¬ë·° ê¸°ëŠ¥", 
                    "pr ë¶„ì„", "pr ìë™", "ì½”ë“œ ì‘ì„±ê³¼ ë¦¬ë·°", "ë¦¬ë·°",
                    "code review", "review", "pullrequest"
                ]
                if any(keyword in all_user_text for keyword in review_keywords):
                    if WorkflowType.CODE_REVIEW not in workflow_focus:
                        workflow_focus.append(WorkflowType.CODE_REVIEW)
                
                # ì½”ë“œ ì‘ì„± ìš”êµ¬ì‚¬í•­ í™•ì¸ (ë” í¬ê´„ì ì´ê³  ìœ ì—°í•˜ê²Œ)
                code_keywords = [
                    "ì½”ë“œ ì‘ì„±", "ì½”ë“œ ìƒì„±", "ìë™ì™„ì„±", "ì½”ë“œ ì™„ì„±", 
                    "ì½”ë“œ ì‘ì„±ê³¼ ë¦¬ë·°", "ì½”ë“œ", "ì½”ë”©", "í”„ë¡œê·¸ë˜ë°",
                    "code generation", "code completion", "autocomplete",
                    "coding", "programming", "ai assistant", "ai ë„êµ¬"
                ]
                if any(keyword in all_user_text for keyword in code_keywords):
                    # CODE_GENERATIONê³¼ CODE_COMPLETION ëª¨ë‘ ì¶”ê°€
                    if WorkflowType.CODE_GENERATION not in workflow_focus:
                        workflow_focus.append(WorkflowType.CODE_GENERATION)
                    if WorkflowType.CODE_COMPLETION not in workflow_focus:
                        workflow_focus.append(WorkflowType.CODE_COMPLETION)
                
                if "ë¦¬íŒ©í† ë§" in all_user_text:
                    if WorkflowType.REFACTORING not in workflow_focus:
                        workflow_focus.append(WorkflowType.REFACTORING)
                if "ë””ë²„ê¹…" in all_user_text:
                    if WorkflowType.DEBUGGING not in workflow_focus:
                        workflow_focus.append(WorkflowType.DEBUGGING)
            
            # ê¸°ë³¸ê°’: workflow_focusê°€ ë¹„ì–´ìˆìœ¼ë©´ CODE_COMPLETION ì¶”ê°€ (ì¼ë°˜ì ì¸ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤)
            # í•˜ì§€ë§Œ ì´ê±´ ì„ íƒì ì´ë¯€ë¡œ, ì‚¬ìš©ìê°€ ëª…í™•íˆ ì–¸ê¸‰í•˜ì§€ ì•Šì•˜ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë„ í—ˆìš©
            # (ì ìˆ˜ ê³„ì‚°ì—ì„œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì²˜ë¦¬ë¨)
            if not workflow_focus:
                # ì‚¬ìš©ìê°€ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰í•˜ì§€ ì•Šì•˜ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
                # ì ìˆ˜ ê³„ì‚°ì—ì„œ workflow_focusê°€ ë¹„ì–´ìˆìœ¼ë©´ ë†’ì€ ì ìˆ˜ ë¶€ì—¬í•˜ë„ë¡ ë˜ì–´ ìˆìŒ
                pass
        
        # UserContext ìƒì„±
        current_team_size = None
        current_budget_max = None
        current_required_integrations = []
        
        if messages_list:
            # HumanMessageë§Œ ì°¾ì•„ì„œ ì‚¬ìš©ì ë©”ì‹œì§€ í™•ì¸
            human_messages = [msg for msg in messages_list if isinstance(msg, HumanMessage)]
            if human_messages:
                all_user_text = " ".join([str(msg.content) for msg in human_messages])
                last_user_msg = all_user_text.lower()
            else:
                last_user_msg = ""
            
            # íŒ€ ê·œëª¨ ì¶”ì¶œ
            team_size_match = re.search(r'(\d+)\s*ëª…', last_user_msg)
            if team_size_match:
                current_team_size = int(team_size_match.group(1))
            
            # ì˜ˆì‚° ì¶”ì¶œ (ì›” $XXX, $XXX/ì›”, ì›” XXXë‹¬ëŸ¬ ë“±)
            budget_patterns = [
                r'ì›”\s*\$?\s*(\d+)',  # "ì›” $100", "ì›” 100"
                r'\$?\s*(\d+)\s*ê¹Œì§€',  # "$100ê¹Œì§€", "100ê¹Œì§€"
                r'\$?\s*(\d+)\s*ê°€ëŠ¥',  # "$100 ê°€ëŠ¥", "100 ê°€ëŠ¥"
                r'\$?\s*(\d+)\s*ì´í•˜',  # "$100 ì´í•˜", "100 ì´í•˜"
                r'\$?\s*(\d+)\s*ì´ë‚´',  # "$100 ì´ë‚´", "100 ì´ë‚´"
            ]
            for pattern in budget_patterns:
                budget_match = re.search(pattern, last_user_msg)
                if budget_match:
                    current_budget_max = float(budget_match.group(1))
                    break
            
            # í†µí•© ê¸°ëŠ¥ ì¶”ì¶œ (GitHub, GitLab, Slack ë“±)
            integration_keywords = {
                "github": "GitHub",
                "gitlab": "GitLab",
                "slack": "Slack",
                "jira": "Jira",
                "bitbucket": "Bitbucket",
                "azure": "Azure DevOps",
                "trello": "Trello",
                "notion": "Notion",
            }
            for keyword, integration_name in integration_keywords.items():
                if keyword in last_user_msg:
                    if integration_name not in current_required_integrations:
                        current_required_integrations.append(integration_name)
        
        # constraintsì—ì„œ ê°€ì ¸ì˜¨ ê°’ì´ ì—†ìœ¼ë©´ ë©”ì‹œì§€ì—ì„œ ì¶”ì¶œí•œ ê°’ ì‚¬ìš©
        final_team_size = current_team_size or (constraints.get("team_size") if constraints else None)
        final_budget_max = current_budget_max or (constraints.get("budget_max") if constraints else None)
        final_required_integrations = current_required_integrations or (constraints.get("required_integrations", []) if constraints else [])
        
        user_context = UserContext(
            team_size=final_team_size,
            tech_stack=tech_stack,
            budget_max=final_budget_max,
            security_required=constraints.get("security_required", False) if constraints else False,
            required_integrations=final_required_integrations,
            workflow_focus=workflow_focus,
            excluded_tools=constraints.get("excluded_tools", []) if constraints else []
        )
        
        # ğŸš¨ ìƒì„¸ ë””ë²„ê¹… ë¡œê·¸: ì…ë ¥ State ì¶œë ¥
        print("=" * 80)
        print("ğŸ” [Decision Engine INPUT]")
        print(f"  team_size: {final_team_size} (ë©”ì‹œì§€: {current_team_size}, constraints: {constraints.get('team_size') if constraints else None})")
        print(f"  tech_stack: {tech_stack}")
        print(f"  budget_max: {final_budget_max} (ë©”ì‹œì§€: {current_budget_max}, constraints: {constraints.get('budget_max') if constraints else None})")
        print(f"  security_required: {constraints.get('security_required', False) if constraints else False}")
        print(f"  required_integrations: {final_required_integrations} (ë©”ì‹œì§€: {current_required_integrations})")
        print(f"  workflow_focus: {[w.value for w in workflow_focus]}")
        print(f"  excluded_tools: {constraints.get('excluded_tools', []) if constraints else []}")
        print(f"  tool_facts ê°œìˆ˜: {len(tool_facts)}ê°œ")
        if tool_facts:
            print(f"  tool_facts ë„êµ¬ëª…: {[fact.get('name', 'Unknown') for fact in tool_facts[:5]]}")
        print("=" * 80)
        
        # Decision Engine ì‹¤í–‰
        tools = [ToolFact(**fact) for fact in tool_facts]
        engine = DecisionEngine(user_context)
        decision_result = engine.make_decision(tools)
        
        print(f"âœ… [Decision Engine] ì‹¤í–‰ ì™„ë£Œ: ì¶”ì²œ {len(decision_result.recommended_tools)}ê°œ, ì œì™¸ {len(decision_result.excluded_tools)}ê°œ")
        
        return {
            "decision_result": decision_result.model_dump(),
            "tool_facts": tool_facts  # tool_factsë¥¼ stateì— ì €ì¥í•˜ì—¬ route_after_researchì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•˜ë„ë¡
        }
    except Exception as e:
        print(f"âš ï¸ [Decision Engine] ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return {}


async def final_report_generation(state: AgentState, config: RunnableConfig):
    """ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± + Redis ìºì‹± (ì¼ë°˜ ë¦¬í¬íŠ¸, LLM ì‚¬ìš©)"""
    
    # re ëª¨ë“ˆì„ í•¨ìˆ˜ ë‚´ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ importí•˜ì—¬ ìŠ¤ì½”í”„ ë¬¸ì œ í•´ê²°
    import re
    
    configurable = Configuration.from_runnable_config(config)
    notes = state.get("notes", [])
    findings = "\n\n".join(notes)
    domain = state.get("domain", "AI ì„œë¹„ìŠ¤")
    
    # Messages ê°€ì ¸ì˜¤ê¸° ë° Follow-up íŒë‹¨
    messages_list = state.get("messages", [])
    human_messages = [msg for msg in messages_list if isinstance(msg, HumanMessage)]
    question_number = len(human_messages)
    is_followup = question_number > 1
    
    # ë””ë²„ê¹…: findings í™•ì¸
    print(f"ğŸ” [DEBUG] final_report_generation ì‹œì‘")
    print(f"ğŸ” [DEBUG] notes ê°œìˆ˜: {len(notes)}")
    print(f"ğŸ” [DEBUG] findings ê¸¸ì´: {len(findings)}ì")
    print(f"ğŸ” [DEBUG] findings ì‹œì‘ 200ì: {findings[:200]}")
    print(f"ğŸ” [DEBUG] is_followup: {is_followup}")
    
    # findingsê°€ ë¹„ì–´ìˆì„ ë•Œ ì²˜ë¦¬
    if not findings or len(findings.strip()) < 50:
        print(f"âš ï¸ [DEBUG] findingsê°€ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìŒ: {len(findings)}ì")
        
        # Follow-up ì§ˆë¬¸ì¸ ê²½ìš° ì´ì „ ëŒ€í™” ë‚´ìš© í™œìš©
        if is_followup:
            print(f"âš ï¸ [DEBUG] Follow-up ì§ˆë¬¸ì´ì§€ë§Œ findingsê°€ ë¹„ì–´ìˆìŒ - ì´ì „ ëŒ€í™” ë‚´ìš© í™œìš©")
            # ì´ì „ AI ë©”ì‹œì§€ì—ì„œ ë„êµ¬ ì •ë³´ ì¶”ì¶œ
            previous_ai_messages = [msg for msg in messages_list[:-1] if isinstance(msg, AIMessage)]
            if previous_ai_messages:
                # ë§ˆì§€ë§‰ AI ë©”ì‹œì§€ì˜ ë‚´ìš©ì„ findingsë¡œ ì‚¬ìš©
                last_ai_content = str(previous_ai_messages[-1].content) if previous_ai_messages else ""
                if len(last_ai_content) > 100:
                    findings = f"ì´ì „ ì¶”ì²œ ë‚´ìš©:\n{last_ai_content}\n\nìƒˆë¡œìš´ ì§ˆë¬¸ì— ëŒ€í•œ ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•©ë‹ˆë‹¤."
                    print(f"âœ… [DEBUG] ì´ì „ ëŒ€í™” ë‚´ìš©ì„ findingsë¡œ ì‚¬ìš©: {len(findings)}ì")
                else:
                    # ì´ì „ ëŒ€í™” ë‚´ìš©ë„ ë¶€ì¡±í•˜ë©´ research_brief ì‚¬ìš©
                    research_brief = state.get("research_brief", "")
                    if research_brief:
                        findings = f"ì—°êµ¬ ì§ˆë¬¸: {research_brief}\n\nì´ì „ ì¶”ì²œ ë„êµ¬ì— ëŒ€í•œ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤."
                    else:
                        findings = "ì´ì „ì— ì¶”ì²œí•œ ë„êµ¬ì— ëŒ€í•œ ì¶”ê°€ ì •ë³´ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤."
            else:
                # ì´ì „ ëŒ€í™”ë„ ì—†ìœ¼ë©´ research_brief ì‚¬ìš©
                research_brief = state.get("research_brief", "")
                findings = f"ì—°êµ¬ ì§ˆë¬¸: {research_brief}\n\nì¶”ê°€ ì •ë³´ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤." if research_brief else "ì¶”ê°€ ì •ë³´ë¥¼ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤."
        else:
            # ì²˜ìŒ ì§ˆë¬¸ì¸ë° findingsê°€ ë¹„ì–´ìˆìœ¼ë©´ ì—ëŸ¬
            error_greeting = "ë„¤! ì¡°ì‚¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
            error_message = "ì£„ì†¡í•©ë‹ˆë‹¤. ì—°êµ¬ ê²°ê³¼ê°€ ë¶€ì¡±í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
            return {
                "final_report": error_message,
                "messages": [
                    AIMessage(content=error_greeting),
                    AIMessage(content=error_message)
                ],
                "notes": {"type": "override", "value": []}
            }
    
    writer_model_config = {
        "model": configurable.final_report_model,
        "max_tokens": configurable.final_report_model_max_tokens,
        "api_key": get_api_key_for_model(configurable.final_report_model, config),
    }
    
    # Messages ê°€ì ¸ì˜¤ê¸° ë° Follow-up íŒë‹¨
    messages_list = state.get("messages", [])
    human_messages = [msg for msg in messages_list if isinstance(msg, HumanMessage)]
    question_number = len(human_messages)
    is_followup = question_number > 1
    
    print(f"ğŸ” [DEBUG] is_followup: {is_followup}, question_number: {question_number}")
    
    # ì´ì „ ë„êµ¬ ì¶”ì¶œ (Follow-upì¸ ê²½ìš°) - ëª¨ë“  AI ë©”ì‹œì§€ì—ì„œ ì¶”ì¶œ
    previous_tools = ""
    if is_followup:
        all_tools = []
        for msg in reversed(messages_list[:-1]):  # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì œì™¸
            if isinstance(msg, AIMessage) and hasattr(msg, 'content'):
                content = str(msg.content)
                # ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ ë„êµ¬ëª… ì¶”ì¶œ
                # íŒ¨í„´ 1: ğŸ“Š [ë„êµ¬ëª…]
                tools_found = re.findall(r'ğŸ“Š\s+([^\n]+)', content)
                if tools_found:
                    all_tools.extend([t.strip() for t in tools_found])
                # íŒ¨í„´ 2: ## ğŸ“Š [ë„êµ¬ëª…]
                tools_found2 = re.findall(r'##\s+ğŸ“Š\s+([^\n]+)', content)
                if tools_found2:
                    all_tools.extend([t.strip() for t in tools_found2])
                # íŒ¨í„´ 3: **1ìˆœìœ„: [ë„êµ¬ëª…]**, **2ìˆœìœ„: [ë„êµ¬ëª…]**
                tools_found3 = re.findall(r'\*\*[0-9]+ìˆœìœ„:\s*([^\*]+)\*\*', content)
                if tools_found3:
                    all_tools.extend([t.strip() for t in tools_found3])
                # íŒ¨í„´ 4: **ìµœì¢… ì¶”ì²œ: [ë„êµ¬ëª…]**
                tools_found4 = re.findall(r'\*\*ìµœì¢… ì¶”ì²œ:\s*([^\*]+)\*\*', content)
                if tools_found4:
                    all_tools.extend([t.strip() for t in tools_found4])
        
        # ì¤‘ë³µ ì œê±°í•˜ê³  ìˆœì„œ ìœ ì§€
        seen = set()
        unique_tools = []
        for tool in all_tools:
            # ë„êµ¬ëª… ì •ì œ (ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°)
            tool_clean = re.sub(r'[\(\)\[\]ì›”\s\$0-9]+', '', tool).strip()
            if tool_clean and tool_clean not in seen and len(tool_clean) > 2:
                seen.add(tool_clean)
                unique_tools.append(tool_clean)
        
        previous_tools = ", ".join(unique_tools[:10])  # ìµœëŒ€ 10ê°œ
        print(f"ğŸ” [DEBUG] final_report - ì´ì „ ì¶”ì²œ ë„êµ¬ ì¶”ì¶œ: {previous_tools}")
    
    # ì§ˆë¬¸ ìœ í˜•ì€ stateì—ì„œ ê°€ì ¸ì˜¤ê¸° (LLMì´ íŒë‹¨í•œ ê°’)
    question_type = state.get("question_type", "comparison")
    
    print(f"ğŸ” [DEBUG] final_report - Messages: {len(messages_list)}ê°œ, ì§ˆë¬¸ ìˆœì„œ: {question_number}ë²ˆì§¸, Follow-up: {is_followup}, ì§ˆë¬¸ìœ í˜•: {question_type} (LLM íŒë‹¨), ì´ì „ ë„êµ¬: {previous_tools}")
    
    # ì œì•½ ì¡°ê±´ ê°€ì ¸ì˜¤ê¸°
    constraints = state.get("constraints", {})
    print(f"ğŸ” [DEBUG] final_report - ì œì•½ ì¡°ê±´: {constraints}")
    
    # ì œì•½ ì¡°ê±´ì„ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…
    constraints_text = ""
    if constraints:
        constraints_text = "**ğŸš¨ í•˜ë“œ ì œì•½ ì¡°ê±´ (ë°˜ë“œì‹œ ì¤€ìˆ˜í•´ì•¼ í•¨):**\n\n"
        if constraints.get("budget_max"):
            constraints_text += f"- ìµœëŒ€ ì˜ˆì‚°: {constraints['budget_max']:,}ì›\n"
        if constraints.get("security_required"):
            constraints_text += f"- ë³´ì•ˆ/í”„ë¼ì´ë²„ì‹œ: í•„ìˆ˜ (ì™¸ë¶€ ì„œë²„ ì „ì†¡ ê¸ˆì§€)\n"
        if constraints.get("excluded_tools"):
            constraints_text += f"- **ì œì™¸í•  ë„êµ¬ (ì ˆëŒ€ ì¶”ì²œ ê¸ˆì§€)**: {', '.join(constraints['excluded_tools'])}\n"
        if constraints.get("excluded_features"):
            constraints_text += f"- **ê¸ˆì§€ëœ ê¸°ëŠ¥**: {', '.join(constraints['excluded_features'])}\n"
        if constraints.get("team_size"):
            constraints_text += f"- íŒ€ ê·œëª¨: {constraints['team_size']}ëª…\n"
        if constraints.get("must_support_ide"):
            constraints_text += f"- í•„ìˆ˜ ì§€ì› IDE: {', '.join(constraints['must_support_ide'])}\n"
        if constraints.get("must_support_language"):
            constraints_text += f"- í•„ìˆ˜ ì§€ì› ì–¸ì–´: {', '.join(constraints['must_support_language'])}\n"
        if constraints.get("other_requirements"):
            constraints_text += f"- ê¸°íƒ€ ìš”êµ¬ì‚¬í•­: {', '.join(constraints['other_requirements'])}\n"
        constraints_text += "\n**âš ï¸ ì¤‘ìš”**: ìœ„ ì œì•½ ì¡°ê±´ì„ ìœ„ë°˜í•˜ëŠ” ë„êµ¬ëŠ” ì¶”ì²œ ëª©ë¡ì—ì„œ ì™„ì „íˆ ì œì™¸í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¨ìˆœíˆ ì–¸ê¸‰í•˜ê±°ë‚˜ ì„¤ëª…ë§Œ í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ì•„ì˜ˆ ì¶”ì²œí•˜ì§€ ë§ˆì„¸ìš”.\n"
    else:
        constraints_text = "ì œì•½ ì¡°ê±´ ì—†ìŒ"
    
    # ğŸš¨ Decision Engineì€ run_decision_engine ë…¸ë“œì—ì„œ ì‹¤í–‰ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì‹¤í–‰í•˜ì§€ ì•ŠìŒ
    # Decision Engine ê²°ê³¼ê°€ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ì¼ë°˜ ë¦¬í¬íŠ¸ ìƒì„± (Discovery ì§ˆë¬¸ìš©)
    decision_info = ""
    decision_result = state.get("decision_result")
    
    if decision_result:
        # Decision Engine ê²°ê³¼ê°€ ìˆìœ¼ë©´ decision_info ìƒì„± (structured_report_generationì—ì„œ ì‚¬ìš©)
        from app.agent.models import DecisionResult
        try:
            result = DecisionResult(**decision_result)
            constraints_dict = state.get("constraints", {})
            team_size = constraints_dict.get("team_size") if constraints_dict else None
            tech_stack = constraints_dict.get("must_support_language", []) if constraints_dict else []
            
            # ë¹„ìš© ë¶„ì„
            cost_analysis = ""
            tool_facts = state.get("tool_facts", [])
            if team_size and tool_facts:
                for tool_fact_dict in tool_facts:
                    tool_name = tool_fact_dict.get("name", "")
                    if tool_name in result.recommended_tools[:3]:
                        pricing_plans = tool_fact_dict.get("pricing_plans", [])
                        team_plans = [p for p in pricing_plans if p.get("plan_type") in ["team", "business", "enterprise"]]
                        if team_plans:
                            cheapest_plan = min(team_plans, key=lambda p: p.get("price_per_user_per_month") or float('inf'))
                            if cheapest_plan.get("price_per_user_per_month"):
                                monthly_cost = cheapest_plan["price_per_user_per_month"] * team_size
                                annual_cost = monthly_cost * 12
                                cost_analysis += f"- {tool_name}: ${monthly_cost:.0f}/ì›” (${annual_cost:.0f}/ë…„, {team_size}ëª… ê¸°ì¤€)\n"
            
            # ìƒì„¸ ì ìˆ˜ ë¶„ì„ (ToolScore ê°ì²´ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì ‘ê·¼)
            detailed_scores = ""
            for score in result.tool_scores:
                # scoreëŠ” ToolScore ê°ì²´ì´ë¯€ë¡œ ì†ì„±ìœ¼ë¡œ ì§ì ‘ ì ‘ê·¼
                tool_name = score.tool_name
                total_score = score.total_score
                detailed_scores += f"\n**{tool_name}** (ë‚´ë¶€ í‰ê°€ ì ìˆ˜ ì°¸ê³ ìš©, ì‚¬ìš©ìì—ê²ŒëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ë³€í™˜):\n"
                detailed_scores += f"  - ì–¸ì–´ ì§€ì› ì ìˆ˜: {score.language_support_score:.2f} â†’ 'ì–¸ì–´ ì§€ì› ìš°ìˆ˜' ë“±ìœ¼ë¡œ ë³€í™˜\n"
                detailed_scores += f"  - í†µí•© ê¸°ëŠ¥ ì ìˆ˜: {score.integration_score:.2f} â†’ 'í•„ìš”í•œ í†µí•© ì§€ì›' ë“±ìœ¼ë¡œ ë³€í™˜\n"
                detailed_scores += f"  - ì—…ë¬´ ì í•©ì„± ì ìˆ˜: {score.workflow_fit_score:.2f} â†’ 'ìš”êµ¬ì‚¬í•­ ì í•©' ë“±ìœ¼ë¡œ ë³€í™˜\n"
                detailed_scores += f"  - ê°€ê²© ì ìˆ˜: {score.price_score:.2f} â†’ 'ë¹„ìš© íš¨ìœ¨ì ' ë“±ìœ¼ë¡œ ë³€í™˜\n"
                detailed_scores += f"  - ë³´ì•ˆ ì ìˆ˜: {score.security_score:.2f} â†’ 'ë³´ì•ˆ ìš”êµ¬ì‚¬í•­ ì¶©ì¡±' ë“±ìœ¼ë¡œ ë³€í™˜\n"
            
            # Decision Engine ê²°ê³¼ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ë‚´ë¶€ í‰ê°€ ê³¼ì • ìˆ¨ê¹€)
            recommended_tools_list = "\n".join([f"{i+1}. {tool}" for i, tool in enumerate(result.recommended_tools[:3])])
            
            decision_info = f"""
**ë‚´ë¶€ ë¶„ì„ ê²°ê³¼ (ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš” - ì‚¬ìš©ìì—ê²ŒëŠ” ë‚´ë¶€ í‰ê°€ ê³¼ì •ì„ ìˆ¨ê¸°ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì¶”ì²œë§Œ ì œì‹œí•˜ì„¸ìš”!):**

**ì¶”ì²œ ë„êµ¬ ìˆœì„œ (ìœ„ì—ì„œë¶€í„° ìš°ì„ ìˆœìœ„):**
{recommended_tools_list}

**ì œì™¸ëœ ë„êµ¬ (ì¶”ì²œí•˜ì§€ ì•Šì•„ì•¼ í•  ë„êµ¬):**
{', '.join(result.excluded_tools) if result.excluded_tools else "ì—†ìŒ"}

**ë¹„ìš© ì •ë³´ ({team_size}ëª… íŒ€ ê¸°ì¤€):**
{cost_analysis if cost_analysis else "ë¹„ìš© ì •ë³´ ì—†ìŒ"}

**ê° ë„êµ¬ë³„ í‰ê°€ ê·¼ê±° (ë‚´ë¶€ ì°¸ê³ ìš©, ì‚¬ìš©ìì—ê²ŒëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ë³€í™˜):**
{chr(10).join(f"- **{tool}**: {reason}" for tool, reason in result.reasoning.items())}

**ğŸš¨ ë§¤ìš° ì¤‘ìš”: ë‹µë³€ ì‘ì„± ê·œì¹™ (ë‚´ë¶€ í‰ê°€ ê³¼ì •ì„ ìˆ¨ê¸°ê³  ìì—°ìŠ¤ëŸ¬ìš´ ì¶”ì²œë§Œ ì œì‹œí•˜ì„¸ìš”!)**
1. **ë‚´ë¶€ í‰ê°€ ê³¼ì • ìˆ¨ê¹€**: "ì ìˆ˜", "Decision Engine", "ë¶„ì„ ê²°ê³¼" ê°™ì€ ë‚´ë¶€ ë§ˆì»¤ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”!
2. **ìì—°ìŠ¤ëŸ¬ìš´ ì¶”ì²œ í˜•ì‹**: "ì ìˆ˜ê°€ ë†’ì•„ì„œ", "ì´ì  1.0" ê°™ì€ í‘œí˜„ ëŒ€ì‹ , "íŒ€ ê·œëª¨ì™€ ì˜ˆì‚°ì— ê°€ì¥ ì í•©í•œ", "ìš”êµ¬ì‚¬í•­ì„ ê°€ì¥ ì˜ ì¶©ì¡±í•˜ëŠ”" ê°™ì€ ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”!
3. **ì¶”ì²œ ìˆœì„œ ìœ ì§€**: ìœ„ì— ë‚˜ì—´ëœ ìˆœì„œëŒ€ë¡œ ì¶”ì²œí•˜ë˜, "1ìˆœìœ„", "2ìˆœìœ„" ê°™ì€ í‘œí˜„ ëŒ€ì‹  "ê°€ì¥ ì¶”ì²œí•˜ëŠ” ë„êµ¬", "ëŒ€ì•ˆìœ¼ë¡œ ê³ ë ¤í•  ìˆ˜ ìˆëŠ” ë„êµ¬" ê°™ì€ ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”!
4. **ë¹„ìš© ì •ë³´ ìì—°ìŠ¤ëŸ½ê²Œ í¬í•¨**: ë¹„ìš© ë¶„ì„ì„ í¬í•¨í•˜ë˜, "$XXX/ì›” (ì´ì  1.0 ê¸°ì¤€)" ê°™ì€ í‘œí˜„ ëŒ€ì‹ , "$XXX/ì›”"ë§Œ í‘œì‹œí•˜ì„¸ìš”!
5. **íŒë‹¨ ì´ìœ  ìì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„**: "ì–¸ì–´ ì§€ì› ì ìˆ˜ 1.0" ëŒ€ì‹  "ì–¸ì–´ ì§€ì›ì´ ìš°ìˆ˜í•˜ë‹¤", "ì—…ë¬´ ì í•©ì„± ì ìˆ˜ 0.9" ëŒ€ì‹  "ì½”ë“œ ì‘ì„±ê³¼ ë¦¬ë·°ì— ì í•©í•˜ë‹¤" ê°™ì€ ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ì„ ì‚¬ìš©í•˜ì„¸ìš”!
6. **ì œì™¸ëœ ë„êµ¬ ì²˜ë¦¬**: ì œì™¸ëœ ë„êµ¬ëŠ” ì¶”ì²œí•˜ì§€ ì•Šë˜, "ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì œì™¸ë¨" ê°™ì€ í‘œí˜„ ëŒ€ì‹ , "ìš”êµ¬ì‚¬í•­ì„ ì¶©ì¡±í•˜ì§€ ì•Šì•„" ê°™ì€ ìì—°ìŠ¤ëŸ¬ìš´ ì´ìœ ë¥¼ ì œì‹œí•˜ì„¸ìš”!
7. **ì½”ë“œ ë¦¬ë·° ìš”êµ¬ì‚¬í•­ ë°˜ì˜**: ì‚¬ìš©ìê°€ "ì½”ë“œ ì‘ì„±ê³¼ ë¦¬ë·°"ë¥¼ ìš”ì²­í–ˆë‹¤ë©´, ì¶”ì²œ ë„êµ¬ê°€ ë¦¬ë·° ê¸°ëŠ¥ì„ ì§€ì›í•˜ëŠ”ì§€ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•˜ì„¸ìš”!
8. **ëª…í™•í•œ ê²°ë¡ **: "ë‘˜ ë‹¤ ì¢‹ìŠµë‹ˆë‹¤" ê°™ì€ ì¤‘ë¦½ì  ë‹µë³€ ëŒ€ì‹ , ì‚¬ìš©ì ìƒí™©ì— ë§ëŠ” ëª…í™•í•œ ì¶”ì²œì„ ì œì‹œí•˜ì„¸ìš”!

**âš ï¸ ì ˆëŒ€ ê¸ˆì§€:**
- "ì ìˆ˜", "ì´ì ", "Decision Engine", "ë¶„ì„ ê²°ê³¼", "í‰ê°€" ê°™ì€ ë‚´ë¶€ í‰ê°€ ìš©ì–´ ì‚¬ìš© ê¸ˆì§€!
- "1.0", "0.85" ê°™ì€ ìˆ«ì ì ìˆ˜ ì§ì ‘ ë…¸ì¶œ ê¸ˆì§€!
- "ğŸš¨ğŸš¨ğŸš¨ Decision Engine ë¶„ì„ ê²°ê³¼" ê°™ì€ ë‚´ë¶€ ë§ˆì»¤ ì‚¬ìš© ê¸ˆì§€!
- ì‚¬ìš©ìê°€ ì „ë¬¸ê°€ê°€ ì•„ë‹Œ ì¼ë°˜ ì‚¬ìš©ìì²˜ëŸ¼, ìì—°ìŠ¤ëŸ½ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”!

"""
        except Exception as e:
            print(f"âš ï¸ [Final Report] DecisionResult íŒŒì‹± ì‹¤íŒ¨: {e}")
            decision_info = ""
    
    final_prompt = final_report_generation_prompt.format(
        research_brief=state.get("research_brief", ""),
        messages=get_buffer_string(messages_list),
        findings=findings,
        date=get_today_str(),
        is_followup="YES" if is_followup else "NO",
        previous_tools=previous_tools if previous_tools else "ì—†ìŒ",
        question_type=question_type,
        constraints=constraints_text + decision_info
    )
    
    try:
        print(f"ğŸ” [DEBUG] ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘ (í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(final_prompt)}ì)")
        print(f"ğŸ” [DEBUG] í”„ë¡¬í”„íŠ¸ ì‹œì‘ 300ì: {final_prompt[:300]}")
        
        final_report = await configurable_model.with_config(writer_model_config).ainvoke([
            HumanMessage(content=final_prompt)
        ])
        
        print(f"ğŸ” [DEBUG] ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
        report_content = str(final_report.content).strip()
        print(f"ğŸ” [DEBUG] ë¦¬í¬íŠ¸ ë‚´ìš© ê¸¸ì´: {len(report_content)}ì")
        print(f"ğŸ” [DEBUG] ë¦¬í¬íŠ¸ ì‹œì‘ 200ì: {report_content[:200]}")
        
        # ë¦¬í¬íŠ¸ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ ì—ëŸ¬ ì²˜ë¦¬
        if not report_content or len(report_content) < 50:
            print(f"âš ï¸ [DEBUG] ë¦¬í¬íŠ¸ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìŒ: {len(report_content)}ì")
            print(f"âš ï¸ [DEBUG] ë¦¬í¬íŠ¸ ì „ì²´ ë‚´ìš©: {repr(report_content)}")
            error_greeting = "ë„¤! ì¡°ê±´ì— ë§ì¶° ë¶„ì„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤." if is_followup else "ë„¤! ì¡°ì‚¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
            error_message = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
            return {
                "final_report": error_message,
                "messages": [
                    AIMessage(content=error_greeting),
                    AIMessage(content=error_message)
                ],
                "notes": {"type": "override", "value": []}
            }
        
        # ========== ğŸ†• ìµœì¢… ë‹µë³€ì„ Redisì— ìºì‹± ==========
        normalized_query = state.get("normalized_query", {})
        print(f"ğŸ” [DEBUG] final_report - normalized_query: {normalized_query}")
        
        if normalized_query and normalized_query.get("cache_key"):
            cache_key = normalized_query["cache_key"]
            print(f"ğŸ’¾ [ìºì‹œ ì €ì¥] ì •ê·œí™”: '{normalized_query.get('normalized_text', '')}' â†’ ìºì‹œí‚¤: {cache_key[:16]}...")
            research_cache.set(
                cache_key,
                {"content": report_content},
                domain=domain,
                prefix="final"
            )
            print(f"âœ… [ìºì‹œ ì €ì¥] ìµœì¢… ë‹µë³€ ì €ì¥ ì™„ë£Œ (ìºì‹œí‚¤: {cache_key[:16]}..., TTL: 7ì¼)")
            
            # ========== ğŸ†• ì§ˆë¬¸-ìºì‹œ í‚¤ ë§¤í•‘ì„ ë²¡í„° DBì— ì €ì¥ (ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰ìš©) ==========
            # ì›ë³¸ ì§ˆë¬¸ ê°€ì ¸ì˜¤ê¸°
            messages_list = state.get("messages", [])
            last_user_message = messages_list[-1].content if messages_list and isinstance(messages_list[-1], HumanMessage) else ""
            
            if last_user_message:
                vector_store.add_query_mapping(
                    query=last_user_message,
                    cache_key=cache_key,
                    normalized_text=normalized_query.get("normalized_text", ""),
                    domain=domain,
                    ttl_days=7
                )
                print(f"âœ… [ë²¡í„° DB ì €ì¥] ì§ˆë¬¸-ìºì‹œ í‚¤ ë§¤í•‘ ì €ì¥ ì™„ë£Œ (ì§ˆë¬¸: '{last_user_message[:50]}...')")
        else:
            print(f"âš ï¸ [ìºì‹œ ì €ì¥ ì‹¤íŒ¨] normalized_query ì—†ìŒ: {normalized_query}")
        
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° (```ë¡œ ì‹œì‘í•˜ê³  ëë‚˜ëŠ” ê²½ìš°)
        report_content = report_content.strip()
        if report_content.startswith("```") and report_content.endswith("```"):
            # ì²« ì¤„ì˜ ``` ì œê±°
            lines = report_content.split('\n')
            if lines[0].strip().startswith("```"):
                lines = lines[1:]
            # ë§ˆì§€ë§‰ ì¤„ì˜ ``` ì œê±°
            if lines and lines[-1].strip() == "```":
                lines = lines[:-1]
            report_content = '\n'.join(lines)
        
        # [GREETING] íƒœê·¸ê°€ ìˆìœ¼ë©´ ì¸ì‚¬ë§ê³¼ ë¦¬í¬íŠ¸ ë¶„ë¦¬
        print(f"ğŸ” [DEBUG] ë¦¬í¬íŠ¸ ì‹œì‘ 100ì: {report_content[:100]}")
        
        if "[GREETING]" in report_content and "[/GREETING]" in report_content:
            # íƒœê·¸ì™€ ë‚´ìš©ì„ ì¶”ì¶œ (ì—¬ëŸ¬ ì¤„ í¬í•¨)
            match = re.search(r'\[GREETING\](.*?)\[/GREETING\]', report_content, re.DOTALL)
            if match:
                greeting = match.group(1).strip()
                # íƒœê·¸ ì „ì²´ë¥¼ ì œê±°í•˜ê³  ë‚˜ë¨¸ì§€ë¥¼ ë¦¬í¬íŠ¸ë¡œ
                report_body = report_content.replace(match.group(0), "").strip()
                
                print(f"âœ… [DEBUG] ì¸ì‚¬ë§ ì¶”ì¶œ ì„±ê³µ: {greeting[:50]}...")
                print(f"âœ… [DEBUG] ë¦¬í¬íŠ¸ ë³¸ë¬¸ ê¸¸ì´: {len(report_body)}ì")
                print(f"âœ… [DEBUG] ë¦¬í¬íŠ¸ ë³¸ë¬¸ ì‹œì‘: {report_body[:100]}")
                
                # report_bodyê°€ ë¹„ì–´ìˆìœ¼ë©´ ì›ë³¸ report_content ì‚¬ìš©
                if not report_body or len(report_body) < 50:
                    print(f"âš ï¸ [DEBUG] report_bodyê°€ ë¹„ì–´ìˆìŒ - ì›ë³¸ report_content ì‚¬ìš©")
                    report_body = report_content
                
                # ë‘ ê°œì˜ ë©”ì‹œì§€ë¡œ ë°˜í™˜
                messages_to_add = [
                    AIMessage(content=greeting),
                    AIMessage(content=report_body)
                ]
            else:
                print(f"âŒ [DEBUG] íƒœê·¸ íŒŒì‹± ì‹¤íŒ¨ - ì •ê·œì‹ ë§¤ì¹­ ì‹¤íŒ¨")
                # íƒœê·¸ íŒŒì‹± ì‹¤íŒ¨ ì‹œì—ë„ ë©˜íŠ¸ + ë¦¬í¬íŠ¸ë¡œ ë¶„ë¦¬
                last_msg = messages_list[-1].content.lower() if messages_list else ""
                greeting = "ë„¤! ì¡°ê±´ì— ë§ì¶° ë¶„ì„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤." if is_followup else "ë„¤! ì¡°ì‚¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
                messages_to_add = [
                    AIMessage(content=greeting),
                    AIMessage(content=report_content)
                ]
        else:
            print(f"âœ… [DEBUG] GREETING íƒœê·¸ ì—†ìŒ - í‚¤ì›Œë“œ ê¸°ë°˜ ë©˜íŠ¸ ìƒì„±")
            # í‚¤ì›Œë“œ ê¸°ë°˜ìœ¼ë¡œ ê°„ë‹¨í•˜ê³  ë¹ ë¥´ê²Œ ë©˜íŠ¸ ìƒì„±
            last_msg = messages_list[-1].content.lower() if messages_list else ""
            greeting = "ë„¤! ì¡°ì‚¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
            
            if "ê°€ê²©" in last_msg or "ì–¼ë§ˆ" in last_msg or "ë¹„ìš©" in last_msg:
                greeting = "ë„¤! ê°€ê²© ì •ë³´ë¥¼ ì•Œë ¤ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
            elif "ì¶”ì²œ" in last_msg or "ìˆœìœ„" in last_msg:
                greeting = "ë„¤! ì¡°ê±´ì— ë§ì¶° ì¶”ì²œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
            elif "ì„ íƒ" in last_msg or "ê³¨ë¼" in last_msg:
                greeting = "ë„¤! ìµœì ì˜ ì„ íƒì„ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
            elif "ì°¨ì´" in last_msg or "ë¹„êµ" in last_msg:
                greeting = "ë„¤! ë¹„êµ ë¶„ì„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
            elif "ì™œ" in last_msg or "ì´ìœ " in last_msg:
                greeting = "ë„¤! ì´ìœ ë¥¼ ì„¤ëª…í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
            elif is_followup:
                greeting = "ë„¤! ì¡°ê±´ì— ë§ì¶° ë¶„ì„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
            
            messages_to_add = [
                AIMessage(content=greeting),
                AIMessage(content=report_content)
            ]
            print(f"âœ… [DEBUG] ë©˜íŠ¸ ìƒì„± ì™„ë£Œ: '{greeting}'")
        
        return {
            "final_report": report_content,
            "messages": messages_to_add,
            "notes": {"type": "override", "value": []}
        }
    
    except Exception as e:
        print(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        
        # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ë©˜íŠ¸ + ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜
        error_greeting = "ë„¤! ì¡°ê±´ì— ë§ì¶° ë¶„ì„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤." if is_followup else "ë„¤! ì¡°ì‚¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
        error_message = f"ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n\nì˜¤ë¥˜: {str(e)}"
        
        return {
            "final_report": error_message,
            "messages": [
                AIMessage(content=error_greeting),
                AIMessage(content=error_message)
            ],
            "notes": {"type": "override", "value": []}
        }


async def structured_report_generation(state: AgentState, config: RunnableConfig):
    """êµ¬ì¡°í™”ëœ ë¦¬í¬íŠ¸ ìƒì„± (Decision Engine ê²°ê³¼ ê¸°ë°˜, í…œí”Œë¦¿ ì‚¬ìš©, LLM ìµœì†Œí™”)"""
    
    # re ëª¨ë“ˆì„ í•¨ìˆ˜ ë‚´ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ importí•˜ì—¬ ìŠ¤ì½”í”„ ë¬¸ì œ í•´ê²°
    import re
    
    from app.agent.models import DecisionResult
    
    decision_result_dict = state.get("decision_result")
    if not decision_result_dict:
        # Decision Engine ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ë¦¬í¬íŠ¸ ìƒì„±ìœ¼ë¡œ í´ë°±
        return await final_report_generation(state, config)
    
    try:
        decision_result = DecisionResult(**decision_result_dict)
    except Exception as e:
        print(f"âš ï¸ [Structured Report] DecisionResult íŒŒì‹± ì‹¤íŒ¨: {e}, ì¼ë°˜ ë¦¬í¬íŠ¸ ìƒì„±ìœ¼ë¡œ í´ë°±")
        return await final_report_generation(state, config)
    
    messages_list = state.get("messages", [])
    human_messages = [msg for msg in messages_list if isinstance(msg, HumanMessage)]
    question_number = len(human_messages)
    is_followup = question_number > 1
    
    # ì‚¬ìš©ì ë§¥ë½ ì •ë³´
    constraints = state.get("constraints", {})
    tech_stack = constraints.get("must_support_language", []) if constraints else []
    team_size = constraints.get("team_size") if constraints else None
    
    # ì¸ì‚¬ ë©˜íŠ¸ ìƒì„± (ê°„ë‹¨í•˜ê²Œ)
    last_user_message = str(messages_list[-1].content) if messages_list else ""
    greeting = "ë„¤! ì¡°ê±´ì— ë§ì¶° ë¶„ì„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤." if is_followup else "ë„¤! ì¡°ì‚¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
    
    # LLMì„ ì‚¬ìš©í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë¦¬í¬íŠ¸ ìƒì„± (ë‚´ë¶€ í‰ê°€ ê³¼ì • ì™„ì „ ìˆ¨ê¹€)
    # Decision Engine ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ì§€ë§Œ, LLMì´ ìì—°ìŠ¤ëŸ½ê²Œ ë³€í™˜
    findings = state.get("findings", "")
    notes = state.get("notes", [])
    research_brief = state.get("research_brief", "")
    
    # tool_factsì—ì„œ ì¶”ì²œ ë„êµ¬ ì •ë³´ ìˆ˜ì§‘
    tool_facts = state.get("tool_facts", [])
    
    # ë¹„ìš© ì •ë³´ ìˆ˜ì§‘ (ê²€ì¦ ë¡œì§ í¬í•¨)
    def get_cost_info(tool_name, team_size):
        for tool_fact_dict in tool_facts:
            if tool_fact_dict.get("name") == tool_name:
                pricing_plans = tool_fact_dict.get("pricing_plans", [])
                if not pricing_plans:
                    continue
                
                # íŒ€ í”Œëœ ìš°ì„  ê²€ìƒ‰
                team_plans = [p for p in pricing_plans if p.get("plan_type") in ["team", "business", "enterprise"]]
                if team_plans:
                    cheapest_plan = min(team_plans, key=lambda p: p.get("price_per_user_per_month") or float('inf'))
                    price_per_user = cheapest_plan.get("price_per_user_per_month")
                    if price_per_user and price_per_user > 0:
                        monthly_cost = price_per_user * team_size
                        annual_cost = monthly_cost * 12
                        # ë¹„ìš©ì´ ë„ˆë¬´ í¬ë©´ (ì˜ˆ: $1000/ì›” ì´ìƒ) ê²€ì¦ í•„ìš”
                        if monthly_cost > 10000:  # $10,000 ì´ìƒì´ë©´ ì˜ì‹¬ìŠ¤ëŸ¬ì›€
                            print(f"âš ï¸ [ê°€ê²© ê²€ì¦] {tool_name} ê³„ì‚°ëœ ë¹„ìš©ì´ ë¹„ì •ìƒì ìœ¼ë¡œ í¼: ${monthly_cost:.0f}/ì›” (ì‚¬ìš©ìë‹¹ ${price_per_user}/ì›”)")
                        return f"${monthly_cost:.0f}/ì›” (${annual_cost:.0f}/ë…„)"
                
                # íŒ€ í”Œëœì´ ì—†ìœ¼ë©´ ê°œì¸ í”Œëœ í™•ì¸ (í•˜ì§€ë§Œ íŒ€ìš©ìœ¼ë¡œëŠ” ì¶”ì²œí•˜ì§€ ì•ŠìŒ)
                individual_plans = [p for p in pricing_plans if p.get("plan_type") in ["individual", "personal", "pro"]]
                if individual_plans:
                    cheapest_individual = min(individual_plans, key=lambda p: p.get("price_per_user_per_month") or float('inf'))
                    price_per_user = cheapest_individual.get("price_per_user_per_month")
                    if price_per_user and price_per_user > 0:
                        monthly_cost = price_per_user * team_size
                        annual_cost = monthly_cost * 12
                        # ê°œì¸ í”Œëœì€ ì°¸ê³ ìš©ìœ¼ë¡œë§Œ í‘œì‹œ (íŒ€ í”Œëœë³´ë‹¤ ë¹„ìŒ€ ìˆ˜ ìˆìŒ)
                        return f"ê°œì¸ í”Œëœ ê¸°ì¤€: ${monthly_cost:.0f}/ì›” (${annual_cost:.0f}/ë…„, ê³µì‹ íŒ€ í”Œëœ í™•ì¸ ê¶Œì¥)"
        return ""
    
    # Decision Engine ê²°ê³¼ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í˜•íƒœë¡œ ì •ë¦¬ (ë‚´ë¶€ í‰ê°€ ìš©ì–´ ì™„ì „ ì œê±°)
    recommended_tools_info = []
    for i, tool_name in enumerate(decision_result.recommended_tools[:3], 1):
        reasoning_text = decision_result.reasoning.get(tool_name, "")
        # ë‚´ë¶€ í‰ê°€ ìš©ì–´ ì œê±° ë° ìì—°ìŠ¤ëŸ½ê²Œ ë³€í™˜
        reasoning_text = re.sub(r'ê¸°ìˆ  ìŠ¤íƒ\([^\)]+\)\s*(ì™„ë²½ ì§€ì›|ë¶€ë¶„ ì§€ì›)', 'ì–¸ì–´ ì§€ì›ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤', reasoning_text)
        reasoning_text = re.sub(r'ë¶€ë¶„ ì§€ì›\s*\(\d+%\)', 'ì§€ì›í•©ë‹ˆë‹¤', reasoning_text)
        reasoning_text = re.sub(r'\d+%', '', reasoning_text)
        reasoning_text = re.sub(r'ë¹„ìš© íš¨ìœ¨ì \s*\(\$\d+/ì›”,\s*\$\d+/ë…„\)', '', reasoning_text).strip()
        if not reasoning_text:
            reasoning_text = "íŒ€ì˜ ìš”êµ¬ì‚¬í•­ì— ì í•©í•œ ë„êµ¬ì…ë‹ˆë‹¤."
        
        cost_info = get_cost_info(tool_name, team_size) if team_size else ""
        recommended_tools_info.append({
            "name": tool_name,
            "reasoning": reasoning_text,
            "cost": cost_info,
            "priority": i
        })
    
    # LLMìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë¦¬í¬íŠ¸ ìƒì„±
    from app.agent.prompts import final_report_generation_prompt, get_today_str
    
    configurable = Configuration.from_runnable_config(config)
    date = get_today_str()
    
    # ì œì•½ ì¡°ê±´ í…ìŠ¤íŠ¸ ìƒì„± (ê°„ë‹¨í•˜ê²Œ)
    constraints_text_simple = ""
    if constraints:
        if team_size:
            constraints_text_simple += f"íŒ€ ê·œëª¨: {team_size}ëª…\n"
        if tech_stack:
            constraints_text_simple += f"ê¸°ìˆ  ìŠ¤íƒ: {', '.join(tech_stack)}\n"
        if constraints.get("budget_max"):
            constraints_text_simple += f"ì˜ˆì‚°: ì›” ${constraints.get('budget_max')} ì´ë‚´\n"
    
    # Decision Engine ê²°ê³¼ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ í˜•íƒœë¡œ ë³€í™˜ (ë‚´ë¶€ í‰ê°€ ìš©ì–´ ì™„ì „ ì œê±°)
    decision_summary = f"""**ì¶”ì²œ ë„êµ¬ (ìš°ì„ ìˆœìœ„ ìˆœì„œëŒ€ë¡œ):**
{chr(10).join([f"{info['priority']}. {info['name']}: {info['reasoning']}" for info in recommended_tools_info])}

{f"**ë¹„ìš© ì •ë³´ ({team_size}ëª… íŒ€ ê¸°ì¤€):**" + chr(10) + chr(10).join([f"- {info['name']}: {info['cost']}" for info in recommended_tools_info if info['cost']]) if team_size and any(info['cost'] for info in recommended_tools_info) else ""}
"""
    
    # ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± í”„ë¡¬í”„íŠ¸ (Decision Engine ê²°ê³¼ í¬í•¨í•˜ë˜ ë‚´ë¶€ í‰ê°€ ìš©ì–´ ì™„ì „ ì œê±°)
    combined_constraints = f"{constraints_text_simple}\n\n**ë‚´ë¶€ ë¶„ì„ ê²°ê³¼ (ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš” - ë‚´ë¶€ í‰ê°€ ê³¼ì •ì€ ì™„ì „íˆ ìˆ¨ê¸°ì„¸ìš”!):**\n\n{decision_summary}"
    
    report_prompt = final_report_generation_prompt.format(
        research_brief=research_brief,
        messages=get_buffer_string(messages_list[-5:]),
        findings=findings[:3000] if findings else "ì—°êµ¬ ê²°ê³¼ ì—†ìŒ",
        date=date,
        is_followup="YES" if is_followup else "NO",
        previous_tools="",
        question_type="decision",
        constraints=combined_constraints
    )
    
    # LLMìœ¼ë¡œ ë¦¬í¬íŠ¸ ìƒì„±
    writer_model_config = {
        "model": configurable.final_report_model,
        "max_tokens": configurable.final_report_model_max_tokens,
        "api_key": get_api_key_for_model(configurable.final_report_model, config),
    }
    
    try:
        print(f"ğŸ” [DEBUG] Structured Report ìƒì„± ì‹œì‘ (í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(report_prompt)}ì)")
        final_report = await configurable_model.with_config(writer_model_config).ainvoke([
            HumanMessage(content=report_prompt)
        ])
        report_body = str(final_report.content).strip()
        
        # [GREETING] íƒœê·¸ ì œê±° (final_report_generationê³¼ ë™ì¼í•œ ë¡œì§)
        if "[GREETING]" in report_body and "[/GREETING]" in report_body:
            match = re.search(r'\[GREETING\](.*?)\[/GREETING\]', report_body, re.DOTALL)
            if match:
                report_body = report_body.replace(match.group(0), "").strip()
        
        # ë¦¬í¬íŠ¸ì—ì„œ ë‚´ë¶€ í‰ê°€ ìš©ì–´ ì œê±° (ì¶”ê°€ ì •ë¦¬)
        report_body = re.sub(r'ğŸš¨ğŸš¨ğŸš¨\s*Decision Engine.*?ğŸš¨ğŸš¨ğŸš¨', '', report_body, flags=re.DOTALL)
        report_body = re.sub(r'ğŸ“ˆ\s*ìƒì„¸ ì ìˆ˜ ë¶„ì„.*', '', report_body, flags=re.DOTALL)
        report_body = re.sub(r'ì ìˆ˜[:\s]*\d+\.?\d*', '', report_body)
        report_body = re.sub(r'ì´ì [:\s]*\d+\.?\d*', '', report_body)
        report_body = re.sub(r'\bë³´í†µ\b|\bë¶€ì í•©\b|\bë¶€ë¶„ ì§€ì›\b|\bë¯¸í¡\b|\bë¯¸ì§€ì›\b|\bë¯¸ì¶©ì¡±\b', '', report_body)
        report_body = re.sub(r'\|\s*ë„êµ¬\s*\|\s*ì–¸ì–´ ì§€ì›\s*\|\s*ì—…ë¬´ ì í•©ì„±.*?\n', '', report_body, flags=re.DOTALL)  # ë¹„êµ í…Œì´ë¸” ì œê±°
        
        # ë¦¬í¬íŠ¸ ì™„ì„±ë„ ê²€ì¦
        if not report_body or len(report_body) < 500:
            print(f"âš ï¸ [Structured Report] ë¦¬í¬íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ ({len(report_body)}ì) - ì¬ìƒì„± ì‹œë„")
            raise ValueError("ë¦¬í¬íŠ¸ê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ ë¶ˆì™„ì „í•©ë‹ˆë‹¤")
        
        # ì¶”ì²œ ë„êµ¬ê°€ ëª¨ë‘ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
        recommended_count_in_report = sum(1 for tool_name in decision_result.recommended_tools[:3] if tool_name in report_body)
        if recommended_count_in_report < len(decision_result.recommended_tools[:3]):
            print(f"âš ï¸ [Structured Report] ì¼ë¶€ ì¶”ì²œ ë„êµ¬ê°€ ë¦¬í¬íŠ¸ì— ì—†ìŒ (í¬í•¨: {recommended_count_in_report}/{len(decision_result.recommended_tools[:3])}) - ì¬ìƒì„± ì‹œë„")
            raise ValueError("ì¶”ì²œ ë„êµ¬ê°€ ëª¨ë‘ í¬í•¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
    except Exception as e:
        print(f"âš ï¸ [Structured Report] LLM ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨ ë˜ëŠ” ë¶ˆì™„ì „: {e}")
        import traceback
        traceback.print_exc()
        # Fallback: ê°„ë‹¨í•˜ì§€ë§Œ ìì—°ìŠ¤ëŸ¬ìš´ ë¦¬í¬íŠ¸ ìƒì„±
        report_body = f"## ğŸ’¡ ì¶”ì²œ ë„êµ¬\n\n"
        for info in recommended_tools_info:
            if info['priority'] == 1:
                report_body += f"### ê°€ì¥ ì¶”ì²œí•˜ëŠ” ë„êµ¬: {info['name']}\n\n"
            else:
                report_body += f"### ëŒ€ì•ˆ {info['priority']-1}: {info['name']}\n\n"
            
            # reasoningì´ ìˆìœ¼ë©´ í¬í•¨, ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ì„¤ëª… ìƒì„±
            if info['reasoning'] and len(info['reasoning']) > 10:
                report_body += f"{info['reasoning']}\n\n"
            else:
                report_body += f"{info['name']}ì€(ëŠ”) íŒ€ì˜ ìš”êµ¬ì‚¬í•­ì— ì í•©í•œ ë„êµ¬ì…ë‹ˆë‹¤. "
                if 'code_completion' in decision_result.recommended_tools and info['name'] == decision_result.recommended_tools[0]:
                    report_body += "ì½”ë“œ ì‘ì„±ê³¼ ìë™ ì™„ì„± ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤."
                elif 'code_review' in decision_result.recommended_tools and info['name'] in decision_result.recommended_tools:
                    report_body += "ì½”ë“œ ë¦¬ë·°ì™€ í’ˆì§ˆ ê²€ì¦ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤."
                report_body += "\n\n"
            
            # ê°€ê²© ì •ë³´ í¬í•¨ (ì˜¬ë°”ë¥¸ ê³„ì‚°)
            if info['cost'] and team_size:
                report_body += f"**ê°€ê²©**: {info['cost']}\n\n"
        
        # ëª¨ë“  ì¶”ì²œ ë„êµ¬ê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸
        if len(report_body) < 500:
            # ì¶”ê°€ ì •ë³´ë¡œ ë¦¬í¬íŠ¸ ê¸¸ì´ í™•ë³´
            report_body += "\n## ğŸ’¡ ê²°ë¡ \n\n"
            if len(recommended_tools_info) > 1:
                report_body += f"ìœ„ {len(recommended_tools_info)}ê°œ ë„êµ¬ë¥¼ ì¡°í•©í•˜ì—¬ ì‚¬ìš©í•˜ë©´ ì½”ë“œ ì‘ì„±ê³¼ ë¦¬ë·° ì‘ì—…ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
            else:
                report_body += f"{recommended_tools_info[0]['name']}ì„(ë¥¼) ì‚¬ìš©í•˜ë©´ íŒ€ì˜ ê°œë°œ ìƒì‚°ì„±ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
    
    # ë””ë²„ê¹…: ë¦¬í¬íŠ¸ ìƒì„± ê²°ê³¼ í™•ì¸
    print(f"ğŸ” [Structured Report DEBUG] ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ:")
    print(f"  - ì¶”ì²œ ë„êµ¬ ê°œìˆ˜: {len(decision_result.recommended_tools)}")
    print(f"  - ì œì™¸ ë„êµ¬ ê°œìˆ˜: {len(decision_result.excluded_tools)}")
    print(f"  - ë¦¬í¬íŠ¸ ê¸¸ì´: {len(report_body)}ì")
    print(f"  - ë¦¬í¬íŠ¸ ì‹œì‘ 200ì: {report_body[:200]}")
    
    # ìºì‹œ ì €ì¥ (ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼)
    normalized_query = state.get("normalized_query", {})
    domain = state.get("domain", "AI ì„œë¹„ìŠ¤")
    if normalized_query and normalized_query.get("cache_key"):
        cache_key = normalized_query["cache_key"]
        research_cache.set(
            cache_key,
            {"content": report_body},
            domain=domain,
            prefix="final"
        )
        print(f"âœ… [ìºì‹œ ì €ì¥] êµ¬ì¡°í™”ëœ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ")
    
    return {
        "final_report": report_body,
        "messages": [
            AIMessage(content=greeting),
            AIMessage(content=report_body)
        ],
        "notes": {"type": "override", "value": []}
    }


async def clarify_missing_constraints(state: AgentState, config: RunnableConfig):
    """ì œì•½ ì¡°ê±´ì´ ë¶€ì¡±í•  ë•Œ ì‚¬ìš©ìì—ê²Œ í•„ìš”í•œ ì •ë³´ë¥¼ ì§ˆë¬¸"""
    
    # re ëª¨ë“ˆì„ í•¨ìˆ˜ ë‚´ì—ì„œ ëª…ì‹œì ìœ¼ë¡œ importí•˜ì—¬ ìŠ¤ì½”í”„ ë¬¸ì œ í•´ê²°
    import re
    
    messages_list = state.get("messages", [])
    human_messages = [msg for msg in messages_list if isinstance(msg, HumanMessage)]
    question_number = len(human_messages)
    is_followup = question_number > 1
    
    constraints = state.get("constraints", {})
    
    # ë¶€ì¡±í•œ ì œì•½ ì¡°ê±´ í™•ì¸
    missing_constraints = []
    
    # íŒ€ ê·œëª¨ í™•ì¸
    team_size = constraints.get("team_size") if constraints else None
    if not team_size:
        # ë©”ì‹œì§€ì—ì„œ íŒ€ ê·œëª¨ ì¶”ì¶œ ì‹œë„
        if messages_list:
            last_user_msg = str(messages_list[-1].content)
            team_size_match = re.search(r'(\d+)\s*ëª…', last_user_msg)
            if not team_size_match:
                missing_constraints.append("íŒ€ ê·œëª¨")
    
    # ì˜ˆì‚° í™•ì¸
    budget_max = constraints.get("budget_max") if constraints else None
    if not budget_max:
        missing_constraints.append("ì˜ˆì‚° ë²”ìœ„")
    
    # ë³´ì•ˆ ìš”êµ¬ì‚¬í•­ í™•ì¸
    security_required = constraints.get("security_required", False) if constraints else False
    # ë³´ì•ˆì€ ì„ íƒì‚¬í•­ì´ë¯€ë¡œ í•„ìˆ˜ë¡œ ë¬»ì§€ ì•ŠìŒ
    
    # ì§ˆë¬¸ ë©”ì‹œì§€ ìƒì„±
    if missing_constraints:
        question_parts = []
        if "íŒ€ ê·œëª¨" in missing_constraints:
            question_parts.append("â€¢ ëª‡ ëª…ì´ ì‚¬ìš©í•˜ì‹œë‚˜ìš”? (ê°œì¸ ì‚¬ìš©ì / íŒ€ ê·œëª¨)")
        if "ì˜ˆì‚° ë²”ìœ„" in missing_constraints:
            question_parts.append("â€¢ ì›” ì˜ˆì‚° ë²”ìœ„ëŠ” ì–´ëŠ ì •ë„ì¸ê°€ìš”? (ë¬´ë£Œë§Œ / ~$20 / ~$50 / ë¬´ì œí•œ)")
        
        question_text = f"""ì •í™•í•œ ì¶”ì²œì„ ìœ„í•´ ë‹¤ìŒ ì •ë³´ê°€ í•„ìš”í•©ë‹ˆë‹¤:

{chr(10).join(question_parts)}

ì¶”ê°€ë¡œ ë‹¤ìŒ ì •ë³´ë„ ìˆìœ¼ë©´ ë” ì •í™•í•œ ì¶”ì²œì´ ê°€ëŠ¥í•©ë‹ˆë‹¤:
â€¢ ì½”ë“œ ì™¸ë¶€ ì „ì†¡ì´ í—ˆìš©ë˜ë‚˜ìš”? (ë³´ì•ˆ ìš”êµ¬ì‚¬í•­)
â€¢ í•„ìˆ˜ë¡œ í•„ìš”í•œ í†µí•© ê¸°ëŠ¥ì´ ìˆë‚˜ìš”? (ì˜ˆ: GitHub, GitLab, Slack ë“±)
â€¢ ì£¼ë¡œ ì–´ë–¤ ì—…ë¬´ì— ì‚¬ìš©í•˜ì‹œë‚˜ìš”? (ì½”ë“œ ì‘ì„±, ì½”ë“œ ë¦¬ë·°, ë¦¬íŒ©í† ë§ ë“±)"""
    else:
        # ì œì•½ ì¡°ê±´ì€ ìˆì§€ë§Œ Decision Engine ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° (tool_facts ë¶€ì¡± ë“±)
        question_text = "ë„êµ¬ ì •ë³´ê°€ ë¶€ì¡±í•˜ì—¬ ì •í™•í•œ ë¹„êµê°€ ì–´ë µìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì‹œë©´ ì •í™•í•œ ì¶”ì²œì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    
    greeting = "ë„¤! ì¡°ê±´ì— ë§ì¶° ë¶„ì„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤." if is_followup else "ë„¤! ì¡°ì‚¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
    
    return {
        "final_report": f"{greeting}\n\n{question_text}",
        "messages": [
            AIMessage(content=greeting),
            AIMessage(content=question_text)
        ],
        "notes": {"type": "override", "value": []}
    }


async def cannot_answer(state: AgentState, config: RunnableConfig):
    """Decision Engine ê²°ê³¼ ì—†ì„ ë•Œ ë‹µë³€ ë¶ˆê°€ ë©”ì‹œì§€ (ì œì•½ ì¡°ê±´ì€ ì¶©ë¶„í•˜ì§€ë§Œ tool_facts ë¶€ì¡± ë“±)"""
    
    messages_list = state.get("messages", [])
    human_messages = [msg for msg in messages_list if isinstance(msg, HumanMessage)]
    question_number = len(human_messages)
    is_followup = question_number > 1
    
    greeting = "ë„¤! ì¡°ê±´ì— ë§ì¶° ë¶„ì„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤." if is_followup else "ë„¤! ì¡°ì‚¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
    error_message = "Decision Engine ë¶„ì„ ê²°ê³¼ê°€ ì—†ì–´ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë„êµ¬ ì •ë³´ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì§ˆë¬¸ì´ ëª…í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    
    return {
        "final_report": error_message,
        "messages": [
            AIMessage(content=greeting),
            AIMessage(content=error_message)
        ],
        "notes": {"type": "override", "value": []}
    }


def route_after_research(state: AgentState) -> Literal["structured_report_generation", "final_report_generation", "clarify_missing_constraints", "cannot_answer"]:
    """ì—°êµ¬ ì™„ë£Œ í›„ ë¼ìš°íŒ…: Decision Engine ê²°ê³¼ ìœ ë¬´ì™€ ì œì•½ ì¡°ê±´ ì¶©ë¶„ ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°"""
    
    # Decision Engineì´ ì‹¤í–‰ë˜ì–´ì•¼ í•˜ëŠ” ì§ˆë¬¸ì¸ì§€ í™•ì¸
    question_type = state.get("question_type", "comparison")
    messages_list = state.get("messages", [])
    last_user_message = str(messages_list[-1].content).lower() if messages_list else ""
    
    # ğŸš¨ ë””ë²„ê¹…: ì§ˆë¬¸ ë‚´ìš©ê³¼ íƒ€ì… í™•ì¸
    print(f"ğŸ” [Routing DEBUG] question_type: {question_type}")
    print(f"ğŸ” [Routing DEBUG] last_user_message: {last_user_message[:100]}")
    
    is_decision_question = (
        question_type in ["decision", "comparison"] or
        any(keyword in last_user_message for keyword in [
            "ì¤‘ í•˜ë‚˜ë§Œ", "í•˜ë‚˜ë§Œ", "ì„ íƒ", "ì–´ë–¤ ê²ƒì´", "ë§ì„ê¹Œ", "ì¶”ì²œ", "ì–´ë–¤ ë„êµ¬", 
            "ì¢‹ì„ê¹Œ", "ì í•©", "ìµœì í™”", "ì–´ë–¤ê²Œ", "ë­˜", "ë¬´ì—‡ì„", "ì–´ë–¤ê²Œ ì¢‹", "ì–´ë–¤ ê²ƒì´ ì¢‹",
            "ë¹„êµ", "vs", "ëŒ€ë¹„", "ì°¨ì´", "ì–´ë–¤ê²Œ ë‚˜ì€", "ë” ì¢‹ì€", "ì–´ëŠê²Œ", "ìµœì "
        ]) or
        "ì–´ë–¤ ë„êµ¬ê°€ ì¢‹ì„ê¹Œìš”" in last_user_message or
        ("ì–´ë–¤ ë„êµ¬" in last_user_message and "ì¢‹" in last_user_message) or
        ("vs" in last_user_message or "ëŒ€ë¹„" in last_user_message) or
        ("ìµœì í™”" in last_user_message and "ë„êµ¬" in last_user_message)  # ğŸ†• "ìµœì í™”ëœ ë„êµ¬" íŒ¨í„´
    )
    
    # ğŸš¨ ë””ë²„ê¹…: Decision ì§ˆë¬¸ íŒì • ê²°ê³¼
    print(f"ğŸ” [Routing DEBUG] is_decision_question: {is_decision_question}")
    
    # Decision Engine ê²°ê³¼ í™•ì¸
    decision_result = state.get("decision_result")
    tool_facts = state.get("tool_facts", [])
    
    # ì œì•½ ì¡°ê±´ ì¶©ë¶„ ì—¬ë¶€ í™•ì¸
    constraints = state.get("constraints", {})
    team_size = constraints.get("team_size") if constraints else None
    budget_max = constraints.get("budget_max") if constraints else None
    
    # ë©”ì‹œì§€ì—ì„œ íŒ€ ê·œëª¨ ì¶”ì¶œ ì‹œë„
    if not team_size and messages_list:
        last_user_msg = str(messages_list[-1].content)
        import re
        team_size_match = re.search(r'(\d+)\s*ëª…', last_user_msg)
        if team_size_match:
            team_size = int(team_size_match.group(1))
    
    has_sufficient_constraints = team_size is not None or budget_max is not None
    
    # ğŸš¨ ë””ë²„ê¹…: Decision Engine ê²°ê³¼ í™•ì¸
    print(f"ğŸ” [Routing DEBUG] decision_result ì¡´ì¬: {decision_result is not None}")
    print(f"ğŸ” [Routing DEBUG] decision_result íƒ€ì…: {type(decision_result)}")
    if decision_result:
        if isinstance(decision_result, dict):
            print(f"ğŸ” [Routing DEBUG] decision_result.keys(): {list(decision_result.keys())}")
            print(f"ğŸ” [Routing DEBUG] recommended_tools: {decision_result.get('recommended_tools', [])}")
        else:
            print(f"ğŸ” [Routing DEBUG] decision_result.recommended_tools: {getattr(decision_result, 'recommended_tools', [])}")
    print(f"ğŸ” [Routing DEBUG] tool_facts ê°œìˆ˜: {len(tool_facts) if tool_facts else 0}")
    print(f"ğŸ” [Routing DEBUG] ì œì•½ ì¡°ê±´ ì¶©ë¶„ ì—¬ë¶€: {has_sufficient_constraints} (team_size: {team_size}, budget_max: {budget_max})")
    
    if is_decision_question:
        # ğŸš¨ Decision ì§ˆë¬¸ì¸ ê²½ìš°
        # Decision Engine ê²°ê³¼ê°€ ìˆê³  ì¶”ì²œ ë„êµ¬ê°€ ìˆìœ¼ë©´ êµ¬ì¡°í™”ëœ ë¦¬í¬íŠ¸ ìƒì„±
        if decision_result:
            # decision_resultê°€ dictì¸ ê²½ìš° model_dump()ëœ ê²°ê³¼ì´ë¯€ë¡œ recommended_tools í™•ì¸
            if isinstance(decision_result, dict):
                recommended_tools_list = decision_result.get("recommended_tools", [])
            elif hasattr(decision_result, "recommended_tools"):
                recommended_tools_list = decision_result.recommended_tools
            else:
                recommended_tools_list = []
            
            recommended_count = len(recommended_tools_list) if recommended_tools_list else 0
            print(f"ğŸ” [Routing DEBUG] recommended_count: {recommended_count}")
            
            if recommended_count > 0:
                print(f"âœ… [Routing] Decision ì§ˆë¬¸ + Decision Engine ê²°ê³¼ ìˆìŒ (ì¶”ì²œ {recommended_count}ê°œ) â†’ structured_report_generation")
                return "structured_report_generation"
            else:
                # Decision Engine ê²°ê³¼ëŠ” ìˆì§€ë§Œ ì¶”ì²œ ë„êµ¬ê°€ ì—†ëŠ” ê²½ìš°: í•„í„°ë§ì´ ë„ˆë¬´ ì—„ê²©í–ˆì„ ìˆ˜ ìˆìŒ
                print(f"âš ï¸ [Routing DEBUG] Decision Engine ê²°ê³¼ëŠ” ìˆì§€ë§Œ ì¶”ì²œ ë„êµ¬ê°€ ì—†ìŒ (recommended_tools ë¹ˆ ë¦¬ìŠ¤íŠ¸)")
                print(f"âš ï¸ [Routing] í•„í„°ë§ì´ ë„ˆë¬´ ì—„ê²©í–ˆê±°ë‚˜ tool_facts ì •ë³´ ë¶€ì¡± â†’ final_report_generation (fallback)")
                return "final_report_generation"
        elif not has_sufficient_constraints:
            # ì œì•½ ì¡°ê±´ì´ ë¶€ì¡±í•˜ë©´ ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸
            print(f"ğŸ” [Routing] Decision ì§ˆë¬¸ì´ì§€ë§Œ ì œì•½ ì¡°ê±´ ë¶€ì¡± â†’ clarify_missing_constraints")
            return "clarify_missing_constraints"
        else:
            # ì œì•½ ì¡°ê±´ì€ ì¶©ë¶„í•˜ì§€ë§Œ Decision Engine ê²°ê³¼ê°€ ì—†ìŒ (tool_facts ë¶€ì¡± ë“±)
            # ğŸ†• Fallback: ì¼ë°˜ ë¦¬í¬íŠ¸ ìƒì„±ìœ¼ë¡œ ëŒ€ì²´ (ì‚¬ìš©ìì—ê²Œ ìµœì†Œí•œì˜ ë‹µë³€ ì œê³µ)
            print(f"âš ï¸ [Routing] Decision ì§ˆë¬¸ + ì œì•½ ì¡°ê±´ ì¶©ë¶„ + Decision Engine ê²°ê³¼ ì—†ìŒ â†’ final_report_generation (fallback)")
            print(f"âš ï¸ [Routing DEBUG] decision_result: {decision_result}, tool_facts: {len(tool_facts) if tool_facts else 0}ê°œ")
            print(f"âš ï¸ [Routing] tool_factsê°€ ì—†ì–´ Decision Engineì„ ì‹¤í–‰í•  ìˆ˜ ì—†ì§€ë§Œ, ì¼ë°˜ ë¦¬í¬íŠ¸ë¡œ ë‹µë³€ ì œê³µ")
            return "final_report_generation"
    else:
        # Discovery ì§ˆë¬¸ì¸ ê²½ìš°: ì¼ë°˜ ë¦¬í¬íŠ¸ ìƒì„± (Decision Engine ë¶ˆí•„ìš”)
        print(f"âœ… [Routing] Discovery ì§ˆë¬¸ â†’ final_report_generation")
        return "final_report_generation"




"""LangGraph nodes for AI Service Advisor"""

import asyncio
from datetime import datetime
from typing import Literal
from langchain.chat_models import init_chat_model
from langchain_core.messages import (
    AIMessage,
    HumanMessage,
    SystemMessage,
    ToolMessage,
    get_buffer_string,
)
from langchain_core.runnables import RunnableConfig
from langgraph.types import Command

from app.agent.configuration import Configuration
from app.agent.state import (
    AgentState,
    ClarifyWithUser,
    ConductResearch,
    ResearchComplete,
    ResearchQuestion,
    ResearcherState,
    SupervisorState,
)
from app.agent.prompts import (
    DOMAIN_GUIDES,
    clarify_with_user_instructions,
    transform_messages_into_research_topic_prompt,
    lead_researcher_prompt,
    research_system_prompt,
    compress_research_system_prompt,
    compress_research_simple_human_message,
    final_report_generation_prompt,
    get_today_str,
    get_current_year,
    get_current_month_year,
)
from app.agent.utils import (
    think_tool,
    get_api_key_for_model,
    get_notes_from_tool_calls,
)
from app.tools.search import searcher
from app.tools.vector_store import vector_store
from app.tools.query_normalizer import query_normalizer
from app.tools.cache import research_cache

# ì„¤ì • ê°€ëŠ¥í•œ ëª¨ë¸
configurable_model = init_chat_model(
    configurable_fields=("model", "max_tokens", "api_key"),
)


async def clarify_with_user(
    state: AgentState, config: RunnableConfig
) -> Command[Literal["write_research_brief", "__end__"]]:
    """ì‚¬ìš©ì ì§ˆë¬¸ ëª…í™•í™” ë° ì£¼ì œ ê²€ì¦ + ì¿¼ë¦¬ ì •ê·œí™” + ìºì‹œ ì¡°íšŒ"""
    
    configurable = Configuration.from_runnable_config(config)
    messages = state["messages"]
    domain = state.get("domain", "AI ì„œë¹„ìŠ¤")
    
    # ê°„ë‹¨í•œ íŒë‹¨: Messages ê°œìˆ˜ë§Œìœ¼ë¡œ
    is_followup = len(messages) >= 3  # ì§ˆë¬¸1 + ë‹µë³€1 + ì§ˆë¬¸2
    
    # ë””ë²„ê¹…
    print(f"ğŸ” [DEBUG] clarify - Messages: {len(messages)}ê°œ, Follow-up: {is_followup}")
    
    # ========== ğŸ†• 1ë‹¨ê³„: ì¿¼ë¦¬ ì •ê·œí™” (ìºì‹œ í‚¤ ìƒì„±) ==========
    last_user_message = messages[-1].content if messages else ""
    
    model_config = {
        "model": configurable.research_model,
        "max_tokens": 200,  # ì •ê·œí™”ëŠ” ì§§ê²Œ
        "api_key": get_api_key_for_model(configurable.research_model, config),
    }
    
    normalized = await query_normalizer.normalize(last_user_message, config=model_config)
    cache_key = normalized["cache_key"]
    
    # ========== ğŸ†• 2ë‹¨ê³„: Redis ìµœì¢… ë‹µë³€ ìºì‹œ ì¡°íšŒ ==========
    print(f"ğŸ” [ìºì‹œ ì¡°íšŒ] ì›ë³¸ ì§ˆë¬¸: '{last_user_message[:50]}...'")
    print(f"ğŸ” [ìºì‹œ ì¡°íšŒ] ì •ê·œí™”: '{normalized['normalized_text']}' â†’ ìºì‹œí‚¤: {cache_key[:16]}...")
    
    cached_answer = research_cache.get(cache_key, domain=domain, prefix="final")
    if cached_answer:
        print(f"âœ… [ìºì‹œ HIT] ìµœì¢… ë‹µë³€ ë°˜í™˜ (ìºì‹œí‚¤: {cache_key[:16]}...)")
        return Command(
            goto="__end__",
            update={"messages": [AIMessage(content=cached_answer["content"])]}
        )
    
    print(f"âš ï¸ [ìºì‹œ MISS] ì •ê·œí™”ëœ ì¿¼ë¦¬: '{normalized['normalized_text']}' (í‚¤ì›Œë“œ: {normalized['keywords']})")
    
    model_config = {
        "model": configurable.research_model,
        "max_tokens": configurable.research_model_max_tokens,
        "api_key": get_api_key_for_model(configurable.research_model, config),
    }
    
    clarification_model = (
        configurable_model
        .with_structured_output(ClarifyWithUser)
        .with_retry(stop_after_attempt=configurable.max_structured_output_retries)
        .with_config(model_config)
    )
    
    prompt_content = clarify_with_user_instructions.format(
        messages=get_buffer_string(messages),
        date=get_today_str(),
        domain=domain,
        is_followup="YES" if is_followup else "NO"
    )
    
    response = await clarification_model.ainvoke([HumanMessage(content=prompt_content)])
    
    # ğŸš¨ ì£¼ì œ ê´€ë ¨ì„± ì²´í¬ (í•­ìƒ ì‹¤í–‰!)
    if not response.is_on_topic:
        print(f"âš ï¸ [DEBUG] ì£¼ì œì—ì„œ ë²—ì–´ë‚œ ì§ˆë¬¸ ê°ì§€")
        return Command(
            goto="__end__",
            update={"messages": [AIMessage(content=response.off_topic_message)]}
        )
    
    # ëª…í™•í™” ë¹„í™œì„±í™” ì‹œ ë°”ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¡œ (ì£¼ì œ ê²€ì¦ í›„)
    if not configurable.allow_clarification:
        print(f"âœ… [DEBUG] ì£¼ì œ ê²€ì¦ í†µê³¼ - ë°”ë¡œ ì—°êµ¬ ì‹œì‘")
        return Command(
            goto="write_research_brief",
            update={
                "messages": [AIMessage(content=response.verification)],
                "normalized_query": normalized  # ğŸ†• ì •ê·œí™” ì •ë³´ ì €ì¥
            }
        )
    
    # ëª…í™•í™” í•„ìš” ì—¬ë¶€ ì²´í¬
    if response.need_clarification:
        return Command(
            goto="__end__",
            update={"messages": [AIMessage(content=response.question)]}
        )
    else:
        return Command(
            goto="write_research_brief",
            update={
                "messages": [AIMessage(content=response.verification)],
                "normalized_query": normalized  # ğŸ†• ì •ê·œí™” ì •ë³´ ì €ì¥
            }
        )


async def write_research_brief(
    state: AgentState, config: RunnableConfig
) -> Command[Literal["research_supervisor"]]:
    """ì—°êµ¬ ê³„íš ìˆ˜ë¦½"""
    
    configurable = Configuration.from_runnable_config(config)
    domain = state.get("domain", "AI ì„œë¹„ìŠ¤")
    domain_guide = DOMAIN_GUIDES.get(domain, "")
    
    research_model_config = {
        "model": configurable.research_model,
        "max_tokens": configurable.research_model_max_tokens,
        "api_key": get_api_key_for_model(configurable.research_model, config),
    }
    
    research_model = (
        configurable_model
        .with_structured_output(ResearchQuestion)
        .with_retry(stop_after_attempt=configurable.max_structured_output_retries)
        .with_config(research_model_config)
    )
    
    # domain_guide í¬ë§·íŒ… (transform_messagesì—ì„œë„ ì‚¬ìš©)
    try:
        formatted_domain_guide_for_research = domain_guide.format(
            date=get_today_str(),
            current_year=get_current_year(),
            current_month_year=get_current_month_year()
        )
    except KeyError:
        formatted_domain_guide_for_research = domain_guide
    
    # Messages ê°€ì ¸ì˜¤ê¸° ë° Follow-up íŒë‹¨
    messages_list = state.get("messages", [])
    is_followup = len(messages_list) >= 3
    
    # ì´ì „ ë„êµ¬ ì¶”ì¶œ (Follow-upì¸ ê²½ìš°)
    previous_tools = ""
    if is_followup:
        import re
        for msg in reversed(messages_list[:-1]):
            if hasattr(msg, 'content'):
                tools_found = re.findall(r'ğŸ“Š\s+([^\n]+)', str(msg.content))
                if tools_found:
                    previous_tools = ", ".join(tools_found[:10])
                    break
    
    # ì§ˆë¬¸ ìœ í˜• íŒë‹¨
    last_user_msg = messages_list[-1].content.lower() if messages_list else ""
    question_type = "comparison"  # ê¸°ë³¸ê°’
    
    if any(kw in last_user_msg for kw in ["í•˜ë‚˜ë§Œ", "ìµœì¢…", "ê²°ì •", "ì„ íƒ", "ê³¨ë¼"]):
        question_type = "decision"
    elif any(kw in last_user_msg for kw in ["ì™œ", "ì´ìœ ", "ì°¨ì´", "í¬ê¸°", "ì„¤ëª…", "ë­ê°€ ë‹¬ë¼"]):
        question_type = "explanation"
    elif any(kw in last_user_msg for kw in ["ê°€ê²©", "ì–¼ë§ˆ", "ë¹„ìš©", "ì–´ë–¤ ê¸°ëŠ¥", "ì„ í˜¸ë„", "ì¸ê¸°ë„", "ë§ì´ ì‚¬ìš©", "ë” ë§ì´", "ì‚¬ëŒë“¤ì´", "í‰ê°€"]):
        question_type = "information"
    
    print(f"ğŸ” [DEBUG] write_research_brief - Messages: {len(messages_list)}ê°œ, Follow-up: {is_followup}, ì§ˆë¬¸ìœ í˜•: {question_type}, ì´ì „ ë„êµ¬: {previous_tools}")
    
    prompt_content = transform_messages_into_research_topic_prompt.format(
        messages=get_buffer_string(messages_list),
        date=get_today_str(),
        current_year=get_current_year(),
        current_month_year=get_current_month_year(),
        domain=domain,
        domain_guide=formatted_domain_guide_for_research,
        is_followup="YES" if is_followup else "NO",
        previous_tools=previous_tools if previous_tools else "ì—†ìŒ",
        question_type=question_type
    )
    
    response = await research_model.ainvoke([HumanMessage(content=prompt_content)])
    
    # ë””ë²„ê¹…: Research Brief í™•ì¸
    print(f"ğŸ” [DEBUG] Research Brief: {response.research_brief[:200]}...")
    
    # domain_guideë„ í¬ë§·íŒ… í•„ìš” (current_year ë“± í¬í•¨)
    try:
        formatted_domain_guide = domain_guide.format(
            date=get_today_str(),
            current_year=get_current_year(),
            current_month_year=get_current_month_year()
        )
    except KeyError:
        # í¬ë§·íŒ… ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        formatted_domain_guide = domain_guide
    
    supervisor_system_prompt = lead_researcher_prompt.format(
        date=get_today_str(),
        current_year=get_current_year(),
        current_month_year=get_current_month_year(),
        domain=domain,
        domain_guide=formatted_domain_guide,
        max_concurrent_research_units=configurable.max_concurrent_research_units,
        max_researcher_iterations=configurable.max_researcher_iterations
    )
    
    return Command(
        goto="research_supervisor",
        update={
            "research_brief": response.research_brief,
            "supervisor_messages": {
                "type": "override",
                "value": [
                    SystemMessage(content=supervisor_system_prompt),
                    HumanMessage(content=response.research_brief)
                ]
            }
        }
    )


async def supervisor(
    state: SupervisorState, config: RunnableConfig
) -> Command[Literal["supervisor_tools"]]:
    """ì—°êµ¬ ìŠˆí¼ë°”ì´ì € (ì—°êµ¬ ê³„íš ë° ìœ„ì„)"""
    
    configurable = Configuration.from_runnable_config(config)
    
    research_model_config = {
        "model": configurable.research_model,
        "max_tokens": configurable.research_model_max_tokens,
        "api_key": get_api_key_for_model(configurable.research_model, config),
    }
    
    tools = [ConductResearch, ResearchComplete, think_tool]
    
    research_model = (
        configurable_model
        .bind_tools(tools)
        .with_retry(stop_after_attempt=configurable.max_structured_output_retries)
        .with_config(research_model_config)
    )
    
    supervisor_messages = state.get("supervisor_messages", [])
    response = await research_model.ainvoke(supervisor_messages)
    
    return Command(
        goto="supervisor_tools",
        update={
            "supervisor_messages": [response],
            "research_iterations": state.get("research_iterations", 0) + 1
        }
    )


async def supervisor_tools(
    state: SupervisorState, config: RunnableConfig
) -> Command[Literal["supervisor", "__end__"]]:
    """ìŠˆí¼ë°”ì´ì € ë„êµ¬ ì‹¤í–‰"""
    
    configurable = Configuration.from_runnable_config(config)
    supervisor_messages = state.get("supervisor_messages", [])
    research_iterations = state.get("research_iterations", 0)
    most_recent_message = supervisor_messages[-1]
    
    # ì¢…ë£Œ ì¡°ê±´
    exceeded_iterations = research_iterations > configurable.max_researcher_iterations
    no_tool_calls = not most_recent_message.tool_calls
    research_complete_called = any(
        tc["name"] == "ResearchComplete" for tc in most_recent_message.tool_calls
    )
    
    if exceeded_iterations or no_tool_calls or research_complete_called:
        return Command(
            goto="__end__",
            update={
                "notes": get_notes_from_tool_calls(supervisor_messages),
                "research_brief": state.get("research_brief", "")
            }
        )
    
    # ë„êµ¬ ì‹¤í–‰
    all_tool_messages = []
    update_payload = {"supervisor_messages": []}
    
    # ëª¨ë“  tool_calls ì²˜ë¦¬
    for tc in most_recent_message.tool_calls:
        if tc["name"] == "think_tool":
            all_tool_messages.append(ToolMessage(
                content=f"ì‚¬ê³  ê¸°ë¡: {tc['args']['reflection']}",
                name="think_tool",
                tool_call_id=tc["id"]
            ))
        
        elif tc["name"] == "ConductResearch":
            # ë‚˜ì¤‘ì— ì¼ê´„ ì²˜ë¦¬
            pass
        
        elif tc["name"] == "ResearchComplete":
            all_tool_messages.append(ToolMessage(
                content="ì—°êµ¬ ì™„ë£Œ í™•ì¸",
                name="ResearchComplete",
                tool_call_id=tc["id"]
            ))
        
        else:
            # ì•Œ ìˆ˜ ì—†ëŠ” tool callì—ë„ ì‘ë‹µ (ì˜¤ë¥˜ ë°©ì§€)
            all_tool_messages.append(ToolMessage(
                content=f"ë„êµ¬ '{tc['name']}'ëŠ” ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                name=tc["name"],
                tool_call_id=tc["id"]
            ))
    
    # ConductResearch ì¼ê´„ ì²˜ë¦¬
    conduct_calls = [tc for tc in most_recent_message.tool_calls if tc["name"] == "ConductResearch"]
    
    if conduct_calls:
        # researcher_subgraph import (ìˆœí™˜ ì°¸ì¡° ë°©ì§€)
        from app.agent.graph import researcher_subgraph
        
        allowed_calls = conduct_calls[:configurable.max_concurrent_research_units]
        skipped_calls = conduct_calls[configurable.max_concurrent_research_units:]
        
        # ë³‘ë ¬ ì—°êµ¬ ì‹¤í–‰
        tasks = [
            researcher_subgraph.ainvoke({
                "researcher_messages": [HumanMessage(content=tc["args"]["research_topic"])],
                "research_topic": tc["args"]["research_topic"],
                "domain": state.get("domain")
            }, config)
            for tc in allowed_calls
        ]
        
        results = await asyncio.gather(*tasks)
        
        for observation, tc in zip(results, allowed_calls):
            all_tool_messages.append(ToolMessage(
                content=observation.get("compressed_research", "ì—°êµ¬ ì‹¤íŒ¨"),
                name=tc["name"],
                tool_call_id=tc["id"]
            ))
        
        # ì œí•œ ì´ˆê³¼ë¡œ ê±´ë„ˆë›´ í˜¸ì¶œì—ë„ ì‘ë‹µ (ì˜¤ë¥˜ ë°©ì§€)
        for tc in skipped_calls:
            all_tool_messages.append(ToolMessage(
                content="ë³‘ë ¬ ì—°êµ¬ ì œí•œìœ¼ë¡œ ë‹¤ìŒ ë°˜ë³µì—ì„œ ì²˜ë¦¬ë©ë‹ˆë‹¤.",
                name=tc["name"],
                tool_call_id=tc["id"]
            ))
        
        # raw_notes ìˆ˜ì§‘
        raw_notes = "\n".join([
            "\n".join(obs.get("raw_notes", [])) for obs in results
        ])
        if raw_notes:
            update_payload["raw_notes"] = [raw_notes]
    
    update_payload["supervisor_messages"] = all_tool_messages
    return Command(goto="supervisor", update=update_payload)


async def researcher(
    state: ResearcherState, config: RunnableConfig
) -> Command[Literal["researcher_tools"]]:
    """ê°œë³„ ì—°êµ¬ì› (Vector DB ì¡°íšŒ â†’ ì›¹ ê²€ìƒ‰)"""
    
    configurable = Configuration.from_runnable_config(config)
    domain = state.get("domain", "AI ì„œë¹„ìŠ¤")
    domain_guide = DOMAIN_GUIDES.get(domain, "")
    
    research_model_config = {
        "model": configurable.research_model,
        "max_tokens": configurable.research_model_max_tokens,
        "api_key": get_api_key_for_model(configurable.research_model, config),
    }
    
    # ========== ğŸ†• Vector DB ê²€ìƒ‰ ë„êµ¬ ì¶”ê°€ ==========
    async def vector_search(query: str) -> str:
        """Vector DBì—ì„œ Facts ê²€ìƒ‰ (ì›¹ ê²€ìƒ‰ ì „ ìš°ì„  ì‹œë„)"""
        facts = vector_store.search_facts(query, limit=5, score_threshold=0.75)
        
        if not facts:
            return "Vector DBì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ì´ í•„ìš”í•©ë‹ˆë‹¤."
        
        # ê²°ê³¼ í¬ë§·íŒ…
        formatted = f"âœ… Vector DBì—ì„œ {len(facts)}ê°œ ê´€ë ¨ ì •ë³´ ë°œê²¬:\n\n"
        for idx, fact in enumerate(facts, 1):
            age_days = (datetime.now().timestamp() - fact['created_at']) / 86400
            formatted += f"{idx}. [ì‹ ë¢°ë„ {fact['score']:.2f}, {age_days:.0f}ì¼ ì „]\n"
            formatted += f"   {fact['text'][:300]}...\n"
            formatted += f"   ì¶œì²˜: {fact['source']} ({fact['url'][:50]}...)\n\n"
        
        return formatted
    
    # ê²€ìƒ‰ ë„êµ¬ ì •ì˜
    async def web_search(query: str) -> str:
        """ì›¹ ê²€ìƒ‰ ë„êµ¬ (Vector DBì— ì •ë³´ê°€ ì—†ì„ ë•Œ ì‚¬ìš©)"""
        result = await searcher.search(
            query=query,
            max_results=configurable.search_max_results,
            search_depth=configurable.search_depth
        )
        
        if not result["success"]:
            return f"ê²€ìƒ‰ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
        
        # ========== ğŸ†• ê²€ìƒ‰ ê²°ê³¼ë¥¼ Vector DBì— ì €ì¥ ==========
        facts_to_store = []
        for r in result["results"]:
            facts_to_store.append({
                "text": f"{r['title']}: {r['content']}",
                "source": result['source'],
                "url": r['url'],
                "metadata": {
                    "score": r.get('score', 0),
                    "query": query
                }
            })
        
        if facts_to_store:
            vector_store.add_facts(facts_to_store, ttl_days=30)
        
        # ê²°ê³¼ í¬ë§·íŒ…
        formatted = f"ê²€ìƒ‰ ê²°ê³¼ ({result['source']}):\n\n"
        for idx, r in enumerate(result["results"], 1):
            formatted += f"{idx}. {r['title']}\n"
            formatted += f"   URL: {r['url']}\n"
            formatted += f"   ë‚´ìš©: {r['content'][:200]}...\n\n"
        
        return formatted
    
    tools = [vector_search, web_search, think_tool]
    
    research_model = (
        configurable_model
        .bind_tools(tools)
        .with_retry(stop_after_attempt=configurable.max_structured_output_retries)
        .with_config(research_model_config)
    )
    
    # domain_guideë„ í¬ë§·íŒ… í•„ìš” (current_year ë“± í¬í•¨)
    try:
        formatted_domain_guide_researcher = domain_guide.format(
            date=get_today_str(),
            current_year=get_current_year(),
            current_month_year=get_current_month_year()
        )
    except KeyError:
        # í¬ë§·íŒ… ë³€ìˆ˜ê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        formatted_domain_guide_researcher = domain_guide
    
    researcher_prompt = research_system_prompt.format(
        domain=domain,
        domain_guide=formatted_domain_guide_researcher,
        date=get_today_str(),
        current_year=get_current_year(),
        current_month_year=get_current_month_year()
    )
    
    messages = [SystemMessage(content=researcher_prompt)] + state.get("researcher_messages", [])
    response = await research_model.ainvoke(messages)
    
    return Command(
        goto="researcher_tools",
        update={
            "researcher_messages": [response],
            "tool_call_iterations": state.get("tool_call_iterations", 0) + 1
        }
    )


async def researcher_tools(
    state: ResearcherState, config: RunnableConfig
) -> Command[Literal["researcher", "compress_research"]]:
    """ì—°êµ¬ì› ë„êµ¬ ì‹¤í–‰"""
    
    configurable = Configuration.from_runnable_config(config)
    researcher_messages = state.get("researcher_messages", [])
    most_recent_message = researcher_messages[-1]
    
    # ë„êµ¬ í˜¸ì¶œ ì—†ìœ¼ë©´ ì¢…ë£Œ
    if not most_recent_message.tool_calls:
        return Command(goto="compress_research")
    
    # ë„êµ¬ ì‹¤í–‰
    tool_outputs = []
    
    for tc in most_recent_message.tool_calls:
        # ========== ğŸ†• Vector DB ê²€ìƒ‰ ì²˜ë¦¬ ==========
        if tc["name"] == "vector_search":
            facts = vector_store.search_facts(tc["args"]["query"], limit=5, score_threshold=0.75)
            
            if facts:
                formatted = f"âœ… Vector DBì—ì„œ {len(facts)}ê°œ ê´€ë ¨ ì •ë³´ ë°œê²¬:\n\n"
                for idx, fact in enumerate(facts, 1):
                    from datetime import datetime
                    age_days = (datetime.now().timestamp() - fact['created_at']) / 86400
                    formatted += f"{idx}. [ì‹ ë¢°ë„ {fact['score']:.2f}, {age_days:.0f}ì¼ ì „]\n"
                    formatted += f"   {fact['text'][:300]}...\n"
                    formatted += f"   ì¶œì²˜: {fact['source']} ({fact.get('url', '')[:50]}...)\n\n"
                content = formatted
            else:
                content = "Vector DBì— ê´€ë ¨ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤. ì›¹ ê²€ìƒ‰ì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”."
            
            tool_outputs.append(ToolMessage(
                content=content,
                name="vector_search",
                tool_call_id=tc["id"]
            ))
        
        elif tc["name"] == "web_search":
            # êµì°¨ ê²€ì¦ í™œì„±í™” (Tavily + Serper Fallback)
            result = await searcher.search(
                query=tc["args"]["query"],
                max_results=configurable.search_max_results,
                enable_verification=True  # êµì°¨ ê²€ì¦ í™œì„±í™”
            )
            
            if result["success"]:
                # ========== ğŸ†• ì›¹ ê²€ìƒ‰ ê²°ê³¼ë¥¼ Vector DBì— ì €ì¥ ==========
                facts_to_store = []
                for r in result["results"]:
                    facts_to_store.append({
                        "text": f"{r['title']}: {r['content']}",
                        "source": result['source'],
                        "url": r['url'],
                        "metadata": {
                            "score": r.get('score', 0),
                            "query": tc["args"]["query"],
                            "is_official": r.get('is_official', False)
                        }
                    })
                
                if facts_to_store:
                    vector_store.add_facts(facts_to_store, ttl_days=30)
                
                source_info = result.get("source", "unknown")
                if source_info == "verified":
                    verified_info = f"êµì°¨ ê²€ì¦ë¨ (Tavily: {result.get('tavily_count', 0)}ê°œ, DuckDuckGo: {result.get('ddg_count', 0)}ê°œ â†’ {result.get('verified_count', 0)}ê°œ ê²€ì¦)"
                else:
                    verified_info = f"({source_info})"
                
                formatted = f"ê²€ìƒ‰ ê²°ê³¼ {verified_info}:\n\n"
                
                # ê³µì‹ ì‚¬ì´íŠ¸ ê²°ê³¼ í‘œì‹œ
                official_results = [r for r in result["results"] if r.get("is_official", False)]
                if official_results:
                    formatted += "ğŸ“Œ ê³µì‹ ì‚¬ì´íŠ¸ ê²°ê³¼:\n"
                    for idx, r in enumerate(official_results, 1):
                        formatted += f"{idx}. {r['title']}\n   URL: {r['url']}\n   {r['content'][:200]}...\n\n"
                
                # ì¼ë°˜ ê²°ê³¼
                other_results = [r for r in result["results"] if not r.get("is_official", False)]
                if other_results:
                    if official_results:
                        formatted += "ê¸°íƒ€ ê²°ê³¼:\n"
                    for idx, r in enumerate(other_results, len(official_results) + 1):
                        formatted += f"{idx}. {r['title']}\n   URL: {r['url']}\n   {r['content'][:200]}...\n\n"
                
                # ê°€ê²© ì •ë³´ ì¶”ì¶œ ë° í‘œì‹œ (ê°€ê²© ê´€ë ¨ ì¿¼ë¦¬ì¸ ê²½ìš°)
                if any(kw in tc["args"]["query"].lower() for kw in ["pricing", "cost", "subscription", "plan", "ê°€ê²©"]):
                    pricing_info = searcher.extract_pricing_info(result["results"])
                    if pricing_info["pricing"]:
                        formatted += f"\nğŸ’° ì¶”ì¶œëœ ê°€ê²© ì •ë³´ (ì‹ ë¢°ë„: {pricing_info['confidence']}):\n"
                        for p in pricing_info["pricing"]:
                            formatted += f"- {p['plan']}: {p['price']} (ì¶œì²˜: {len(p['sources'])}ê°œ, ê³µì‹: {p['official_count']}ê°œ)\n"
                
                content = formatted
            else:
                content = f"ê²€ìƒ‰ ì‹¤íŒ¨: {result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}"
            
            tool_outputs.append(ToolMessage(
                content=content,
                name="web_search",
                tool_call_id=tc["id"]
            ))
        
        elif tc["name"] == "think_tool":
            tool_outputs.append(ToolMessage(
                content=f"ì‚¬ê³ : {tc['args']['reflection']}",
                name="think_tool",
                tool_call_id=tc["id"]
            ))
        
        else:
            # ì•Œ ìˆ˜ ì—†ëŠ” tool callì—ë„ ì‘ë‹µ (ì˜¤ë¥˜ ë°©ì§€)
            tool_outputs.append(ToolMessage(
                content=f"ë„êµ¬ '{tc['name']}'ëŠ” ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                name=tc["name"],
                tool_call_id=tc["id"]
            ))
    
    # ì¢…ë£Œ ì¡°ê±´
    exceeded = state.get("tool_call_iterations", 0) >= configurable.max_react_tool_calls
    
    if exceeded:
        return Command(goto="compress_research", update={"researcher_messages": tool_outputs})
    
    return Command(goto="researcher", update={"researcher_messages": tool_outputs})


async def compress_research(state: ResearcherState, config: RunnableConfig):
    """ì—°êµ¬ ê²°ê³¼ ì••ì¶•"""
    
    configurable = Configuration.from_runnable_config(config)
    
    compression_model = configurable_model.with_config({
        "model": configurable.compression_model,
        "max_tokens": configurable.compression_model_max_tokens,
        "api_key": get_api_key_for_model(configurable.compression_model, config),
    })
    
    researcher_messages = state.get("researcher_messages", [])
    researcher_messages.append(HumanMessage(content=compress_research_simple_human_message))
    
    compression_prompt = compress_research_system_prompt.format(date=get_today_str())
    messages = [SystemMessage(content=compression_prompt)] + researcher_messages
    
    try:
        response = await compression_model.ainvoke(messages)
        
        raw_notes = "\n".join([
            str(msg.content) for msg in researcher_messages
            if isinstance(msg, (ToolMessage, AIMessage))
        ])
        
        return {
            "compressed_research": str(response.content),
            "raw_notes": [raw_notes]
        }
    
    except Exception as e:
        print(f"âŒ ì••ì¶• ì‹¤íŒ¨: {e}")
        return {
            "compressed_research": "ì—°êµ¬ ê²°ê³¼ ì••ì¶• ì‹¤íŒ¨",
            "raw_notes": [""]
        }


async def final_report_generation(state: AgentState, config: RunnableConfig):
    """ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± + Redis ìºì‹±"""
    
    configurable = Configuration.from_runnable_config(config)
    notes = state.get("notes", [])
    findings = "\n\n".join(notes)
    domain = state.get("domain", "AI ì„œë¹„ìŠ¤")
    
    writer_model_config = {
        "model": configurable.final_report_model,
        "max_tokens": configurable.final_report_model_max_tokens,
        "api_key": get_api_key_for_model(configurable.final_report_model, config),
    }
    
    # Messages ê°€ì ¸ì˜¤ê¸° ë° Follow-up íŒë‹¨
    messages_list = state.get("messages", [])
    is_followup = len(messages_list) >= 3
    
    # ì´ì „ ë„êµ¬ ì¶”ì¶œ (Follow-upì¸ ê²½ìš°)
    previous_tools = ""
    if is_followup:
        import re
        for msg in reversed(messages_list[:-1]):
            if hasattr(msg, 'content'):
                tools_found = re.findall(r'ğŸ“Š\s+([^\n]+)', str(msg.content))
                if tools_found:
                    previous_tools = ", ".join(tools_found[:10])
                    break
    
    # ì§ˆë¬¸ ìœ í˜• íŒë‹¨
    last_user_msg = messages_list[-1].content.lower() if messages_list else ""
    question_type = "comparison"  # ê¸°ë³¸ê°’
    
    if any(kw in last_user_msg for kw in ["í•˜ë‚˜ë§Œ", "ìµœì¢…", "ê²°ì •", "ì„ íƒ", "ê³¨ë¼"]):
        question_type = "decision"
    elif any(kw in last_user_msg for kw in ["ì™œ", "ì´ìœ ", "ì°¨ì´", "í¬ê¸°", "ì„¤ëª…", "ë­ê°€ ë‹¬ë¼"]):
        question_type = "explanation"
    elif any(kw in last_user_msg for kw in ["ê°€ê²©", "ì–¼ë§ˆ", "ë¹„ìš©", "ì–´ë–¤ ê¸°ëŠ¥", "ì„ í˜¸ë„", "ì¸ê¸°ë„", "ë§ì´ ì‚¬ìš©", "ë” ë§ì´", "ì‚¬ëŒë“¤ì´", "í‰ê°€"]):
        question_type = "information"
    
    print(f"ğŸ” [DEBUG] final_report - Messages: {len(messages_list)}ê°œ, Follow-up: {is_followup}, ì§ˆë¬¸ìœ í˜•: {question_type}, ì´ì „ ë„êµ¬: {previous_tools}")
    
    final_prompt = final_report_generation_prompt.format(
        research_brief=state.get("research_brief", ""),
        messages=get_buffer_string(messages_list),
        findings=findings,
        date=get_today_str(),
        is_followup="YES" if is_followup else "NO",
        previous_tools=previous_tools if previous_tools else "ì—†ìŒ",
        question_type=question_type
    )
    
    try:
        final_report = await configurable_model.with_config(writer_model_config).ainvoke([
            HumanMessage(content=final_prompt)
        ])
        
        report_content = str(final_report.content)
        
        # ========== ğŸ†• ìµœì¢… ë‹µë³€ì„ Redisì— ìºì‹± ==========
        normalized_query = state.get("normalized_query", {})
        print(f"ğŸ” [DEBUG] final_report - normalized_query: {normalized_query}")
        
        if normalized_query and normalized_query.get("cache_key"):
            cache_key = normalized_query["cache_key"]
            print(f"ğŸ’¾ [ìºì‹œ ì €ì¥] ì •ê·œí™”: '{normalized_query.get('normalized_text', '')}' â†’ ìºì‹œí‚¤: {cache_key[:16]}...")
            research_cache.set(
                cache_key,
                {"content": report_content},
                domain=domain,
                prefix="final"
            )
            print(f"âœ… [ìºì‹œ ì €ì¥] ìµœì¢… ë‹µë³€ ì €ì¥ ì™„ë£Œ (ìºì‹œí‚¤: {cache_key[:16]}..., TTL: 7ì¼)")
        else:
            print(f"âš ï¸ [ìºì‹œ ì €ì¥ ì‹¤íŒ¨] normalized_query ì—†ìŒ: {normalized_query}")
        
        # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° (```ë¡œ ì‹œì‘í•˜ê³  ëë‚˜ëŠ” ê²½ìš°)
        report_content = report_content.strip()
        if report_content.startswith("```") and report_content.endswith("```"):
            # ì²« ì¤„ì˜ ``` ì œê±°
            lines = report_content.split('\n')
            if lines[0].strip().startswith("```"):
                lines = lines[1:]
            # ë§ˆì§€ë§‰ ì¤„ì˜ ``` ì œê±°
            if lines and lines[-1].strip() == "```":
                lines = lines[:-1]
            report_content = '\n'.join(lines)
        
        # [GREETING] íƒœê·¸ê°€ ìˆìœ¼ë©´ ì¸ì‚¬ë§ê³¼ ë¦¬í¬íŠ¸ ë¶„ë¦¬
        print(f"ğŸ” [DEBUG] ë¦¬í¬íŠ¸ ì‹œì‘ 100ì: {report_content[:100]}")
        
        if "[GREETING]" in report_content and "[/GREETING]" in report_content:
            import re
            # íƒœê·¸ì™€ ë‚´ìš©ì„ ì¶”ì¶œ (ì—¬ëŸ¬ ì¤„ í¬í•¨)
            match = re.search(r'\[GREETING\](.*?)\[/GREETING\]', report_content, re.DOTALL)
            if match:
                greeting = match.group(1).strip()
                # íƒœê·¸ ì „ì²´ë¥¼ ì œê±°í•˜ê³  ë‚˜ë¨¸ì§€ë¥¼ ë¦¬í¬íŠ¸ë¡œ
                report_body = report_content.replace(match.group(0), "").strip()
                
                print(f"âœ… [DEBUG] ì¸ì‚¬ë§ ì¶”ì¶œ ì„±ê³µ: {greeting[:50]}...")
                print(f"âœ… [DEBUG] ë¦¬í¬íŠ¸ ë³¸ë¬¸ ì‹œì‘: {report_body[:100]}")
                
                # ë‘ ê°œì˜ ë©”ì‹œì§€ë¡œ ë°˜í™˜
                messages_to_add = [
                    AIMessage(content=greeting),
                    AIMessage(content=report_body)
                ]
            else:
                print(f"âŒ [DEBUG] íƒœê·¸ íŒŒì‹± ì‹¤íŒ¨ - ì •ê·œì‹ ë§¤ì¹­ ì‹¤íŒ¨")
                # íƒœê·¸ íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë©”ì‹œì§€ë¡œ
                messages_to_add = [final_report]
        else:
            print(f"âœ… [DEBUG] GREETING íƒœê·¸ ì—†ìŒ - ì¼ë°˜ ë¦¬í¬íŠ¸")
            # ì¸ì‚¬ë§ ì—†ëŠ” ê²½ìš° í•˜ë‚˜ì˜ ë©”ì‹œì§€ë¡œ
            messages_to_add = [final_report]
        
        return {
            "final_report": report_content,
            "messages": messages_to_add,
            "notes": {"type": "override", "value": []}
        }
    
    except Exception as e:
        print(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return {
            "final_report": f"ë¦¬í¬íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
            "messages": [AIMessage(content="ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨")],
            "notes": {"type": "override", "value": []}
        }




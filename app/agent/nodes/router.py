"""ë¼ìš°íŒ… ê´€ë ¨ ë…¸ë“œ - clarify_with_user, route_after_research"""

import re
from typing import Literal

from app.agent.nodes._common import (
    Command,
    END,
    RunnableConfig,
    AgentState,
    ClarifyWithUser,
    Configuration,
    AIMessage,
    HumanMessage,
    get_buffer_string,
    configurable_model,
    clarify_with_user_instructions,
    get_today_str,
    get_api_key_for_model,
    query_normalizer,
    research_cache,
    vector_store,
)


async def clarify_with_user(
    state: AgentState, config: RunnableConfig
) -> Command[Literal["write_research_brief", END]]:
    """ì‚¬ìš©ì ì§ˆë¬¸ ëª…í™•í™” ë° ì£¼ì œ ê²€ì¦ + ì¿¼ë¦¬ ì •ê·œí™” + ìºì‹œ ì¡°íšŒ"""
    
    configurable = Configuration.from_runnable_config(config)
    messages = state["messages"]
    domain = state.get("domain", "AI ì„œë¹„ìŠ¤")
    
    # ì§ˆë¬¸ ìˆœì„œ íŒŒì•…: HumanMessage ê°œìˆ˜ë¡œ íŒë‹¨ (ë” ì •í™•í•˜ê²Œ)
    human_messages = [msg for msg in messages if isinstance(msg, HumanMessage)]
    question_number = len(human_messages)  # 1ë²ˆì§¸, 2ë²ˆì§¸, 3ë²ˆì§¸ ì§ˆë¬¸...
    is_followup = question_number > 1  # 2ë²ˆì§¸ ì§ˆë¬¸ë¶€í„° Follow-up
    
    # ë””ë²„ê¹…
    print(f"ğŸ” [DEBUG] clarify - Messages: {len(messages)}ê°œ, HumanMessage: {len(human_messages)}ê°œ, ì§ˆë¬¸ ìˆœì„œ: {question_number}ë²ˆì§¸, Follow-up: {is_followup}")
    
    last_user_message = messages[-1].content if messages else ""
    
    # ========== ğŸš¨ LLM ê¸°ë°˜ ì£¼ì œ ê²€ì¦ ë° ì¸ì‚¬ ê°ì§€ (ê²€ìƒ‰/ìºì‹œ ì „ì— ë¨¼ì € ìˆ˜í–‰) ==========
    # ì£¼ì œ ê²€ì¦ì„ LLMì´ íŒë‹¨í•˜ë„ë¡ í•˜ì—¬ ë¶ˆí•„ìš”í•œ ì¿¼ë¦¬ ì •ê·œí™”/ìºì‹œ ì¡°íšŒ ë°©ì§€
    # í‚¤ì›Œë“œ ì„ ê²€ì¦ ì œê±°: LLMì´ ëª¨ë“  ì§ˆë¬¸ì˜ ì£¼ì œ ê´€ë ¨ì„±ì„ íŒë‹¨
    model_config_clarify = {
        "model": configurable.research_model,
        "max_tokens": configurable.research_model_max_tokens,
        "api_key": get_api_key_for_model(configurable.research_model, config),
    }
    
    clarification_model = (
        configurable_model
        .with_structured_output(ClarifyWithUser)
        .with_retry(stop_after_attempt=configurable.max_structured_output_retries)
        .with_config(model_config_clarify)
    )
    
    prompt_content = clarify_with_user_instructions.format(
        messages=get_buffer_string(messages),
        date=get_today_str(),
        domain=domain,
        is_followup="YES" if is_followup else "NO"
    )
    
    response = await clarification_model.ainvoke([HumanMessage(content=prompt_content)])
    
    # ğŸ†• ì¸ì‚¬ ë©”ì‹œì§€ ì²´í¬ (ê°€ì¥ ë¨¼ì €!)
    if response.is_greeting:
        print(f"ğŸ‘‹ [ì¸ì‚¬ ì‘ë‹µ] LLMì´ ì¸ì‚¬ ë©”ì‹œì§€ ê°ì§€ - ì¹œì ˆí•˜ê²Œ ì‘ë‹µ")
        # LLMì´ greeting_messageë¥¼ ìƒì„±í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ì—ì„œ ëª…ì‹œí–ˆìœ¼ë¯€ë¡œ, ì—†ìœ¼ë©´ ê²½ê³ í•˜ê³  fallback ì‚¬ìš©
        greeting_msg = response.greeting_message
        if not greeting_msg or greeting_msg.strip() == "":
            print(f"âš ï¸ [ì¸ì‚¬ ì‘ë‹µ] LLMì´ greeting_messageë¥¼ ìƒì„±í•˜ì§€ ì•ŠìŒ - fallback ì‚¬ìš©")
            greeting_msg = (
                "ì•ˆë…•í•˜ì„¸ìš”! ë°˜ê°‘ìŠµë‹ˆë‹¤ ğŸ˜Š\n\n"
                "ì €ëŠ” ì½”ë”© AI ë„êµ¬ ì¶”ì²œì„ ì „ë¬¸ìœ¼ë¡œ í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
                "íŒ€ì— ì í•©í•œ ì½”ë”© AI ë„êµ¬(ì½”ë“œ ì‘ì„±, ë¦¬ë·°, ìë™ ì™„ì„± ë“±)ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“ ì§€ ë¬¼ì–´ë³´ì„¸ìš”!"
            )
        return Command(
            goto=END,
            update={"messages": [AIMessage(content=greeting_msg)]}
        )
    
    # ğŸš¨ ì£¼ì œ ê´€ë ¨ì„± ì²´í¬ (ê²€ìƒ‰/ìºì‹œ ì „ ì°¨ë‹¨)
    if not response.is_on_topic:
        print(f"âš ï¸ [ì£¼ì œ ê²€ì¦] ì£¼ì œì—ì„œ ë²—ì–´ë‚œ ì§ˆë¬¸ ê°ì§€ - ìºì‹œ/ê²€ìƒ‰/ë²¡í„°DB ì €ì¥ ì°¨ë‹¨")
        off_topic_msg = response.off_topic_message if response.off_topic_message else "ì£„ì†¡í•©ë‹ˆë‹¤. ì €ëŠ” ì½”ë”© AI ë„êµ¬ ì¶”ì²œì„ ì „ë¬¸ìœ¼ë¡œ í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”!"
        return Command(
            goto=END,
            update={"messages": [AIMessage(content=off_topic_msg)]}
        )
    
    # ì£¼ì œ ê²€ì¦ í†µê³¼ â†’ ì´ì œ ì¿¼ë¦¬ ì •ê·œí™” ë° ìºì‹œ ì¡°íšŒ ì§„í–‰
    print(f"âœ… [ì£¼ì œ ê²€ì¦] ì£¼ì œ ê²€ì¦ í†µê³¼ - ì •ìƒ í”„ë¡œì„¸ìŠ¤ ì§„í–‰")
    
    # ========== ğŸ†• 1ë‹¨ê³„: ì¿¼ë¦¬ ì •ê·œí™” (ìºì‹œ í‚¤ ìƒì„±) ==========
    model_config = {
        "model": configurable.research_model,
        "max_tokens": 200,  # ì •ê·œí™”ëŠ” ì§§ê²Œ
        "api_key": get_api_key_for_model(configurable.research_model, config),
    }
    
    normalized = await query_normalizer.normalize(last_user_message, config=model_config)
    cache_key = normalized["cache_key"]
    
    # ========== ğŸ†• 2ë‹¨ê³„: Redis ìµœì¢… ë‹µë³€ ìºì‹œ ì¡°íšŒ ==========
    print(f"ğŸ” [ìºì‹œ ì¡°íšŒ] ì›ë³¸ ì§ˆë¬¸: '{last_user_message[:50]}...'")
    print(f"ğŸ” [ìºì‹œ ì¡°íšŒ] ì •ê·œí™”: '{normalized['normalized_text']}' â†’ ìºì‹œí‚¤: {cache_key[:16]}...")
    
    cached_answer = research_cache.get(cache_key, domain=domain, prefix="final")
    if cached_answer:
        print(f"âœ… [ìºì‹œ HIT] ìµœì¢… ë‹µë³€ ë°˜í™˜ (ìºì‹œí‚¤: {cache_key[:16]}...)")
        
        # Follow-upì¸ ê²½ìš° ì´ì „ ì¶”ì²œ ë„êµ¬ í™•ì¸
        # ë‹¨, ê°™ì€ ì˜ë¯¸ì˜ ì§ˆë¬¸(ê°™ì€ ìºì‹œ í‚¤)ì´ë©´ ì´ì „ ì¶”ì²œ ë„êµ¬ í™•ì¸ ê±´ë„ˆë›°ê³  ìºì‹œ ì‚¬ìš©
        if is_followup:
            # ì´ì „ ë©”ì‹œì§€ì—ì„œ ì¶”ì²œëœ ë„êµ¬ ì¶”ì¶œ (ëª¨ë“  AI ë©”ì‹œì§€ì—ì„œ)
            previous_tools_in_messages = []
            all_tools = []
            for msg in reversed(messages[:-1]):  # ë§ˆì§€ë§‰ ì‚¬ìš©ì ë©”ì‹œì§€ ì œì™¸
                if isinstance(msg, AIMessage) and hasattr(msg, 'content'):
                    content = str(msg.content)
                    # ë‹¤ì–‘í•œ íŒ¨í„´ìœ¼ë¡œ ë„êµ¬ëª… ì¶”ì¶œ
                    # íŒ¨í„´ 1: ğŸ“Š [ë„êµ¬ëª…]
                    tools_found = re.findall(r'ğŸ“Š\s+([^\n]+)', content)
                    if tools_found:
                        all_tools.extend([t.strip() for t in tools_found])
                    # íŒ¨í„´ 2: ## ğŸ“Š [ë„êµ¬ëª…]
                    tools_found2 = re.findall(r'##\s+ğŸ“Š\s+([^\n]+)', content)
                    if tools_found2:
                        all_tools.extend([t.strip() for t in tools_found2])
                    # íŒ¨í„´ 3: **1ìˆœìœ„: [ë„êµ¬ëª…]**, **2ìˆœìœ„: [ë„êµ¬ëª…]**
                    tools_found3 = re.findall(r'\*\*[0-9]+ìˆœìœ„:\s*([^\*]+)\*\*', content)
                    if tools_found3:
                        all_tools.extend([t.strip() for t in tools_found3])
                    # íŒ¨í„´ 4: **ìµœì¢… ì¶”ì²œ: [ë„êµ¬ëª…]**
                    tools_found4 = re.findall(r'\*\*ìµœì¢… ì¶”ì²œ:\s*([^\*]+)\*\*', content)
                    if tools_found4:
                        all_tools.extend([t.strip() for t in tools_found4])
            
            # ì¤‘ë³µ ì œê±°
            seen = set()
            for tool in all_tools:
                # ë„êµ¬ëª… ì •ì œ (ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°)
                tool_clean = re.sub(r'[\(\)\[\]ì›”\s\$0-9]+', '', tool).strip()
                if tool_clean and tool_clean not in seen and len(tool_clean) > 2:
                    seen.add(tool_clean)
                    previous_tools_in_messages.append(tool_clean)
            
            # ì´ì „ ì¶”ì²œ ë„êµ¬ê°€ ìˆìœ¼ë©´ ìºì‹œ ê²€ì¦, ì—†ìœ¼ë©´ ê°™ì€ ì˜ë¯¸ì˜ ì§ˆë¬¸ì´ë¯€ë¡œ ìºì‹œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if previous_tools_in_messages:
                # ìºì‹œëœ ë‹µë³€ì—ì„œ ë„êµ¬ ì¶”ì¶œ (ë‹¤ì–‘í•œ íŒ¨í„´)
                cached_tools = []
                cached_content = cached_answer["content"]
                # íŒ¨í„´ 1: ğŸ“Š [ë„êµ¬ëª…]
                tools_found = re.findall(r'ğŸ“Š\s+([^\n]+)', cached_content)
                cached_tools.extend([t.strip() for t in tools_found])
                # íŒ¨í„´ 2: ## ğŸ“Š [ë„êµ¬ëª…]
                tools_found2 = re.findall(r'##\s+ğŸ“Š\s+([^\n]+)', cached_content)
                cached_tools.extend([t.strip() for t in tools_found2])
                # íŒ¨í„´ 3: **1ìˆœìœ„: [ë„êµ¬ëª…]**
                tools_found3 = re.findall(r'\*\*[0-9]+ìˆœìœ„:\s*([^\*]+)\*\*', cached_content)
                cached_tools.extend([t.strip() for t in tools_found3])
                
                # ì´ì „ ì¶”ì²œ ë„êµ¬ì™€ ìºì‹œëœ ë‹µë³€ì˜ ë„êµ¬ê°€ ë‹¤ë¥´ë©´ ìºì‹œ ë¬´ì‹œ
                if cached_tools:
                    # ë„êµ¬ëª… ì •ì œ
                    previous_tools_clean = [re.sub(r'[\(\)\[\]ì›”\s\$0-9]+', '', t).strip() for t in previous_tools_in_messages]
                    cached_tools_clean = [re.sub(r'[\(\)\[\]ì›”\s\$0-9]+', '', t).strip() for t in cached_tools]
                    
                    previous_tools_set = set([t for t in previous_tools_clean if len(t) > 2])
                    cached_tools_set = set([t for t in cached_tools_clean if len(t) > 2])
                    
                    # ì´ì „ ì¶”ì²œ ë„êµ¬ê°€ ìºì‹œì— ì—†ê±°ë‚˜, ìºì‹œì— ì´ì „ì— ì¶”ì²œí•˜ì§€ ì•Šì€ ìƒˆ ë„êµ¬ê°€ ìˆìœ¼ë©´ ë¬´ì‹œ
                    if not previous_tools_set.issubset(cached_tools_set) or len(cached_tools_set - previous_tools_set) > 0:
                        print(f"âš ï¸ [ìºì‹œ ë¬´ì‹œ] ì´ì „ ì¶”ì²œ ë„êµ¬({previous_tools_in_messages})ì™€ ìºì‹œ ë„êµ¬({cached_tools})ê°€ ë‹¤ë¦„. ìºì‹œ ë¬´ì‹œí•˜ê³  ìƒˆë¡œ ìƒì„±")
                        cached_answer = None  # ìºì‹œ ë¬´ì‹œ
            else:
                # ì´ì „ ì¶”ì²œ ë„êµ¬ê°€ ì—†ìœ¼ë©´ ê°™ì€ ì˜ë¯¸ì˜ ì§ˆë¬¸ì´ë¯€ë¡œ ìºì‹œ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                print(f"âœ… [ìºì‹œ ì‚¬ìš©] ì´ì „ ì¶”ì²œ ë„êµ¬ ì—†ìŒ - ê°™ì€ ì˜ë¯¸ì˜ ì§ˆë¬¸ìœ¼ë¡œ íŒë‹¨, ìºì‹œ ì‚¬ìš©")
        
        if cached_answer:
            # ìºì‹œëœ ë‹µë³€ ì²˜ë¦¬
            cached_content = cached_answer["content"]
            
            print(f"ğŸ” [ìºì‹œ ì²˜ë¦¬] ìºì‹œëœ ë‹µë³€ ê¸¸ì´: {len(cached_content)}ì, is_followup: {is_followup}")
            print(f"ğŸ” [ìºì‹œ ì²˜ë¦¬] ìºì‹œëœ ë‹µë³€ ì‹œì‘ 100ì: {cached_content[:100]}")
            
            # ë¦¬í¬íŠ¸ ë³¸ë¬¸ ì¶”ì¶œ (ìºì‹œì—ëŠ” ë¦¬í¬íŠ¸ ë³¸ë¬¸ë§Œ ì €ì¥ë˜ì–´ ìˆìŒ)
            report_body = cached_content.strip()
            
            # ğŸš¨ [GREETING] íƒœê·¸ê°€ ìˆìœ¼ë©´ ì œê±°í•˜ê³  ë¦¬í¬íŠ¸ ë³¸ë¬¸ë§Œ ì¶”ì¶œ
            # ì¸ì‚¬ ë©˜íŠ¸ëŠ” ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ì§€ ì•Šê³  í•­ìƒ ìƒˆë¡œ ìƒì„±
            if "[GREETING]" in cached_content and "[/GREETING]" in cached_content:
                match = re.search(r'\[GREETING\](.*?)\[/GREETING\]', cached_content, re.DOTALL)
                if match:
                    # ì¸ì‚¬ë§ íƒœê·¸ ì œê±°í•˜ê³  ë¦¬í¬íŠ¸ ë³¸ë¬¸ë§Œ ì¶”ì¶œ
                    report_body = cached_content.replace(match.group(0), "").strip()
                    print(f"âœ… [ìºì‹œ] [GREETING] íƒœê·¸ ì œê±° í›„ ë¦¬í¬íŠ¸ ë³¸ë¬¸ ì¶”ì¶œ: {len(report_body)}ì")
            
            # ë¦¬í¬íŠ¸ ë³¸ë¬¸ì´ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
            if not report_body or len(report_body) < 50:
                print(f"âš ï¸ [ìºì‹œ ì²˜ë¦¬] ë¦¬í¬íŠ¸ ë³¸ë¬¸ì´ ë¹„ì–´ìˆìŒ - ì›ë³¸ ìºì‹œ ë‚´ìš© ì‚¬ìš©")
                report_body = cached_content.strip()
            
            # ğŸš¨ ìºì‹œ ê²€ì¦: ë¦¬í¬íŠ¸ ë³¸ë¬¸ì´ ìœ íš¨í•œì§€ í™•ì¸
            # ë¦¬í¬íŠ¸ê°€ ë„ˆë¬´ ì§§ê±°ë‚˜(200ì ë¯¸ë§Œ) ë¹„ì–´ìˆìœ¼ë©´ ìºì‹œ ë¬´ì‹œ
            if len(report_body) < 200:
                print(f"âš ï¸ [ìºì‹œ ë¬´ì‹œ] ë¦¬í¬íŠ¸ ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ ({len(report_body)}ì). ìºì‹œ ë¬´ì‹œí•˜ê³  ìƒˆë¡œ ìƒì„±")
                # pass - ìºì‹œë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ì•„ë˜ ì—°êµ¬ í”„ë¡œì„¸ìŠ¤ë¡œ ì§„í–‰
            else:
                # ğŸš¨ ì¸ì‚¬ ë©˜íŠ¸ëŠ” í•­ìƒ ìƒˆë¡œ ìƒì„± (ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ì§€ ì•ŠìŒ)
                # final_report_generationê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì¸ì‚¬ ë©˜íŠ¸ ìƒì„± (ë™ì¼í•œ ëª¨ë¸, ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ ìŠ¤íƒ€ì¼)
                print(f"âœ… [ìºì‹œ ì²˜ë¦¬] ë¦¬í¬íŠ¸ ë³¸ë¬¸ì€ ìºì‹œì—ì„œ ê°€ì ¸ì˜´ ({len(report_body)}ì), ì¸ì‚¬ ë©˜íŠ¸ëŠ” final_report_generationê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ìƒì„±")
                
                # final_report_generationê³¼ ë™ì¼í•œ ëª¨ë¸ ë° ì„¤ì • ì‚¬ìš©
                # ëª¨ë¸ë³„ max_tokens ì œí•œ í™•ì¸ ë° ì ìš©
                model_name_greeting = configurable.final_report_model.lower()
                if "gpt-4o-mini" in model_name_greeting:
                    greeting_max_tokens = min(configurable.final_report_model_max_tokens, 16384)  # gpt-4o-mini ìµœëŒ€ 16384
                elif "gpt-4o" in model_name_greeting and "mini" not in model_name_greeting:
                    greeting_max_tokens = min(configurable.final_report_model_max_tokens, 16384)  # gpt-4o ìµœëŒ€ 16384
                elif "gpt-4" in model_name_greeting:
                    greeting_max_tokens = min(configurable.final_report_model_max_tokens, 4096)  # gpt-4 ìµœëŒ€ 4096
                else:
                    greeting_max_tokens = min(configurable.final_report_model_max_tokens, 16384)  # ê¸°ë³¸ê°’
                
                greeting_model_config = {
                    "model": configurable.final_report_model,
                    "max_tokens": greeting_max_tokens,
                    "api_key": get_api_key_for_model(configurable.final_report_model, config),
                }
                
                # final_report_generation í”„ë¡¬í”„íŠ¸ì˜ ì¸ì‚¬ ë©˜íŠ¸ ìƒì„± ë¶€ë¶„ê³¼ ë™ì¼í•œ ìŠ¤íƒ€ì¼
                # ì‚¬ìš©ì ë©”ì‹œì§€ ì „ì²´ ì»¨í…ìŠ¤íŠ¸ ì œê³µ (final_report_generationê³¼ ë™ì¼)
                messages_context = get_buffer_string(messages) if messages else last_user_message
                
                greeting_prompt = f"""ë‹¹ì‹ ì€ ì½”ë”© AI ë„êµ¬ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì§ˆë¬¸ì— ë§ëŠ” ìì—°ìŠ¤ëŸ½ê³  ìƒì„¸í•œ ì¸ì‚¬ ë©˜íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì‚¬ìš©ì ë©”ì‹œì§€:
{messages_context}

**ì›ì¹™:**
- ì‚¬ìš©ìì˜ í˜„ì¬ ì§ˆë¬¸ ë‚´ìš©ê³¼ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ê·¸ì— ë§ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ë©˜íŠ¸ë¥¼ ìƒì„±
- ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œ(íŒ€ ê·œëª¨, ëª©ì , ìš”êµ¬ì‚¬í•­, ë„ë©”ì¸ ë“±)ë¥¼ ë°˜ì˜
- ì§ˆë¬¸ì— ì–¸ê¸‰ëœ êµ¬ì²´ì ì¸ ë‚´ìš©(íŒ€ ê·œëª¨, ëª©ì , ìš”êµ¬ì‚¬í•­ ë“±)ì„ ë°˜ë“œì‹œ í¬í•¨
- ìì—°ìŠ¤ëŸ½ê³  ì¹œì ˆí•œ í†¤ ìœ ì§€
- ì ì ˆí•œ ê¸¸ì´ (40-100ì ì •ë„, ë„ˆë¬´ ì§§ì§€ ì•Šê²Œ)

**ì¢‹ì€ ì˜ˆì‹œ:**
- ì§ˆë¬¸: "ì €í¬ëŠ” ë°±ì—”ë“œÂ·í”„ë¡ íŠ¸ì—”ë“œ í¬í•¨í•´ì„œ 8ëª… ê·œëª¨ì˜ ê°œë°œíŒ€ì¸ë°, ì½”ë“œ ì‘ì„±ê³¼ ë¦¬ë·°ì— AIë¥¼ ë„ì…í•´ì„œ ìƒì‚°ì„±ì„ ë†’ì´ê³  ì‹¶ìŠµë‹ˆë‹¤. ì–´ë–¤ ë„êµ¬ê°€ ì¢‹ì„ê¹Œìš”?"
  ì¸ì‚¬ ë©˜íŠ¸: "ë„¤! ë°±ì—”ë“œì™€ í”„ë¡ íŠ¸ì—”ë“œë¥¼ í¬í•¨í•œ 8ëª… ê·œëª¨ì˜ ê°œë°œíŒ€ì— ì í•©í•œ AI ë„êµ¬ë“¤ì„ ë¶„ì„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. íŒ€ì˜ ì½”ë“œ ì‘ì„± ë° ë¦¬ë·° íš¨ìœ¨ì„± í–¥ìƒì— ë„ì›€ì´ ë˜ëŠ” ë„êµ¬ë¥¼ ë¹„êµí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."

- ì§ˆë¬¸: "ì½”ë“œ ì‘ì„±ê³¼ ë¦¬ë·°ë¥¼ ìœ„í•œ AI ë„êµ¬ ì¶”ì²œí•´ì¤˜"
  ì¸ì‚¬ ë©˜íŠ¸: "ë„¤! ì½”ë“œ ì‘ì„±ê³¼ ë¦¬ë·°ë¥¼ ìœ„í•œ ìµœì ì˜ AI ë„êµ¬ë¥¼ ì¶”ì²œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."

**ë‚˜ìœ ì˜ˆì‹œ (ë„ˆë¬´ ì§§ê±°ë‚˜ ë§¥ë½ ì—†ìŒ):**
- "ì•ˆë…•í•˜ì„¸ìš”." (ë„ˆë¬´ ì§§ìŒ)
- "AI ë„êµ¬ë¡œ ìƒì‚°ì„±ì„ ë†’ì—¬ë“œë¦¬ê² ìŠµë‹ˆë‹¤." (ë„ˆë¬´ ì§§ê³  êµ¬ì²´ì ì´ì§€ ì•ŠìŒ)
- "ë„¤! ì¡°ì‚¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤." (ë„ˆë¬´ ì¼ë°˜ì )

ì¸ì‚¬ ë©˜íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš” ([GREETING] íƒœê·¸ ì—†ì´, ë‹¤ë¥¸ ì„¤ëª… ì—†ì´):"""
                
                try:
                    greeting_model = configurable_model.with_config(greeting_model_config)
                    greeting_response = await greeting_model.ainvoke([HumanMessage(content=greeting_prompt)])
                    greeting = str(greeting_response.content).strip()
                    
                    # ë¶ˆí•„ìš”í•œ ë”°ì˜´í‘œë‚˜ íƒœê·¸ ì œê±°
                    greeting = greeting.strip('"\'`').strip()
                    
                    # "ì•ˆë…•í•˜ì„¸ìš”"ë¡œë§Œ ì‹œì‘í•˜ëŠ” ë„ˆë¬´ ì§§ì€ ì‘ë‹µ ê°ì§€
                    if greeting.startswith("ì•ˆë…•í•˜ì„¸ìš”") and len(greeting) < 15:
                        print(f"âš ï¸ [ìºì‹œ ì²˜ë¦¬] LLM ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ: '{greeting}', ì¬ì‹œë„")
                        greeting = ""  # ì¬ì‹œë„í•˜ë„ë¡ ë¹ˆ ë¬¸ìì—´ë¡œ ì„¤ì •
                    
                    # ì‘ë‹µì´ ë„ˆë¬´ ê¸¸ë©´ ì ì ˆíˆ ìë¥´ê¸° (100ì ì´ë‚´ë¡œ)
                    if greeting and len(greeting) > 100:
                        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ìë¥´ê¸° (ë§ˆì¹¨í‘œë‚˜ ëŠë‚Œí‘œ ê¸°ì¤€)
                        sentences = re.split(r'[.!?ã€‚]', greeting)
                        if len(sentences) > 1 and sentences[0]:
                            # ì²« ë²ˆì§¸ ë¬¸ì¥ë§Œ ì‚¬ìš©í•˜ê³  ë§ˆì¹¨í‘œ ì¶”ê°€
                            greeting = sentences[0].strip() + '.'
                        else:
                            # ë¬¸ì¥ êµ¬ë¶„ì´ ì—†ìœ¼ë©´ 100ìë¡œ ìë¥´ê¸°
                            greeting = greeting[:100].strip()
                    
                    # ë¹ˆ ì‘ë‹µì´ê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ ì¬ì‹œë„ (ìµœì†Œ 30ì ì´ìƒ)
                    if not greeting or len(greeting) < 30:
                        print(f"âš ï¸ [ìºì‹œ ì²˜ë¦¬] LLM ì‘ë‹µì´ ë„ˆë¬´ ì§§ìŒ ({len(greeting) if greeting else 0}ì), ì¬ì‹œë„")
                        # ë” ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„ (final_report_generation ìŠ¤íƒ€ì¼)
                        retry_prompt = f"""ë‹¹ì‹ ì€ ì½”ë”© AI ë„êµ¬ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ì ë©”ì‹œì§€:
{messages_context}

ìœ„ ì§ˆë¬¸ì— ë§ëŠ” ìì—°ìŠ¤ëŸ½ê³  ìƒì„¸í•œ ì¸ì‚¬ ë©˜íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš”. ì§ˆë¬¸ì˜ í•µì‹¬ ë‚´ìš©(íŒ€ ê·œëª¨, ëª©ì , ìš”êµ¬ì‚¬í•­ ë“±)ì„ êµ¬ì²´ì ìœ¼ë¡œ ë°˜ì˜í•œ 40-100ì ì •ë„ì˜ ìƒì„¸í•œ ì¸ì‚¬ ë©˜íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì˜ˆì‹œ: "ë„¤! ë°±ì—”ë“œì™€ í”„ë¡ íŠ¸ì—”ë“œë¥¼ í¬í•¨í•œ 8ëª… ê·œëª¨ì˜ ê°œë°œíŒ€ì— ì í•©í•œ AI ë„êµ¬ë“¤ì„ ë¶„ì„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. íŒ€ì˜ ì½”ë“œ ì‘ì„± ë° ë¦¬ë·° íš¨ìœ¨ì„± í–¥ìƒì— ë„ì›€ì´ ë˜ëŠ” ë„êµ¬ë¥¼ ë¹„êµí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."

ì¸ì‚¬ ë©˜íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš”:"""
                        retry_response = await greeting_model.ainvoke([HumanMessage(content=retry_prompt)])
                        greeting = str(retry_response.content).strip().strip('"\'`').strip()
                        
                        # ì¬ì‹œë„ í›„ì—ë„ ë„ˆë¬´ ì§§ìœ¼ë©´ ì§ˆë¬¸ ê¸°ë°˜ìœ¼ë¡œ ë™ì  ìƒì„±
                        if not greeting or len(greeting) < 30:
                            # ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì„œ ë™ì ìœ¼ë¡œ ìƒì„±
                            keywords = []
                            if "íŒ€" in last_user_message or "ê·œëª¨" in last_user_message:
                                keywords.append("íŒ€")
                            if "ì½”ë“œ" in last_user_message or "ë¦¬ë·°" in last_user_message:
                                keywords.append("ì½”ë“œ ì‘ì„± ë° ë¦¬ë·°")
                            if "ë„êµ¬" in last_user_message or "ì¶”ì²œ" in last_user_message:
                                keywords.append("ë„êµ¬ ì¶”ì²œ")
                            
                            if keywords:
                                greeting = f"ë„¤! {'ì™€ '.join(keywords[:2])}ì— ì í•©í•œ AI ë„êµ¬ë¥¼ ë¶„ì„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
                            else:
                                greeting = f"ë„¤! {last_user_message[:30]}ì— ëŒ€í•´ ì¡°ì‚¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
                    
                    print(f"âœ… [ìºì‹œ ì²˜ë¦¬] LLMìœ¼ë¡œ ì¸ì‚¬ ë©˜íŠ¸ ìƒì„± ì™„ë£Œ: '{greeting}' (ê¸¸ì´: {len(greeting)}ì)")
                except Exception as e:
                    print(f"âš ï¸ [ìºì‹œ ì²˜ë¦¬] LLM ì¸ì‚¬ ë©˜íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}, ì§ˆë¬¸ ê¸°ë°˜ ë™ì  ìƒì„±")
                    # LLM ì‹¤íŒ¨ ì‹œ ì§ˆë¬¸ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë™ì ìœ¼ë¡œ ìƒì„± (í•˜ë“œì½”ë”© ìµœì†Œí™”)
                    question_preview = last_user_message[:50] if len(last_user_message) > 50 else last_user_message
                    greeting = f"ë„¤! {question_preview}ì— ëŒ€í•´ ì¡°ì‚¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
                    print(f"âœ… [ìºì‹œ ì²˜ë¦¬] ë™ì  ìƒì„± ì¸ì‚¬ ë©˜íŠ¸: '{greeting}'")
                print(f"âœ… [ìºì‹œ ì²˜ë¦¬] ë¦¬í¬íŠ¸ ë³¸ë¬¸ ê¸¸ì´: {len(report_body)}ì, ì‹œì‘ 100ì: {report_body[:100]}")
                
                return Command(
                    goto=END,
                    update={"messages": [
                        AIMessage(content=greeting),
                        AIMessage(content=report_body)
                    ]}
                )
    
    print(f"âš ï¸ [ìºì‹œ MISS] ì •ê·œí™”ëœ ì¿¼ë¦¬: '{normalized['normalized_text']}' (í‚¤ì›Œë“œ: {normalized['keywords']})")
    
    # ========== ğŸ†• 3ë‹¨ê³„: ë²¡í„° DBë¡œ ìœ ì‚¬ ì§ˆë¬¸ ê²€ìƒ‰ ==========
    # ìºì‹œ ë¯¸ìŠ¤ ì‹œ ìœ ì‚¬í•œ ì§ˆë¬¸ì´ ìˆëŠ”ì§€ ë²¡í„° DBì—ì„œ ê²€ìƒ‰
    similar_query = vector_store.search_similar_query(
        query=last_user_message,
        domain=domain,
        limit=1,
        score_threshold=0.85  # ë†’ì€ ìœ ì‚¬ë„ë§Œ (85% ì´ìƒ)
    )
    
    if similar_query and similar_query.get("cache_key"):
        similar_cache_key = similar_query["cache_key"]
        print(f"ğŸ” [ìœ ì‚¬ ì§ˆë¬¸ ë°œê²¬] ìœ ì‚¬ë„: {similar_query['score']:.3f}, ê¸°ì¡´ ì§ˆë¬¸: '{similar_query['query'][:50]}...'")
        print(f"ğŸ” [ìœ ì‚¬ ì§ˆë¬¸] ìºì‹œ í‚¤ ì¬ì‚¬ìš©: {similar_cache_key[:16]}...")
        
        # ìœ ì‚¬ ì§ˆë¬¸ì˜ ìºì‹œ í‚¤ë¡œ Redisì—ì„œ ë‹µë³€ ê°€ì ¸ì˜¤ê¸°
        cached_answer = research_cache.get(similar_cache_key, domain=domain, prefix="final")
        if cached_answer:
            print(f"âœ… [ìœ ì‚¬ ì§ˆë¬¸ ìºì‹œ HIT] ìµœì¢… ë‹µë³€ ë°˜í™˜ (ìœ ì‚¬ ì§ˆë¬¸ì˜ ìºì‹œ í‚¤: {similar_cache_key[:16]}...)")
            
            # ë¦¬í¬íŠ¸ ë³¸ë¬¸ ì¶”ì¶œ ë° ì¸ì‚¬ ë©˜íŠ¸ ìƒì„± (ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼)
            cached_content = cached_answer["content"]
            report_body = cached_content.strip()
            
            # [GREETING] íƒœê·¸ ì œê±°
            if "[GREETING]" in cached_content and "[/GREETING]" in cached_content:
                match = re.search(r'\[GREETING\](.*?)\[/GREETING\]', cached_content, re.DOTALL)
                if match:
                    report_body = cached_content.replace(match.group(0), "").strip()
            
            if not report_body or len(report_body) < 50:
                report_body = cached_content.strip()
            
            if len(report_body) >= 200:
                # ì¸ì‚¬ ë©˜íŠ¸ ìƒì„± (ê¸°ì¡´ ë¡œì§ê³¼ ë™ì¼)
                print(f"âœ… [ìœ ì‚¬ ì§ˆë¬¸ ì²˜ë¦¬] ë¦¬í¬íŠ¸ ë³¸ë¬¸ì€ ìºì‹œì—ì„œ ê°€ì ¸ì˜´ ({len(report_body)}ì), ì¸ì‚¬ ë©˜íŠ¸ëŠ” ìƒˆë¡œ ìƒì„±")
                
                # final_report_generationê³¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì¸ì‚¬ ë©˜íŠ¸ ìƒì„±
                # ëª¨ë¸ë³„ max_tokens ì œí•œ í™•ì¸ ë° ì ìš©
                model_name_greeting2 = configurable.final_report_model.lower()
                if "gpt-4o-mini" in model_name_greeting2:
                    greeting_max_tokens2 = min(configurable.final_report_model_max_tokens, 16384)  # gpt-4o-mini ìµœëŒ€ 16384
                elif "gpt-4o" in model_name_greeting2 and "mini" not in model_name_greeting2:
                    greeting_max_tokens2 = min(configurable.final_report_model_max_tokens, 16384)  # gpt-4o ìµœëŒ€ 16384
                elif "gpt-4" in model_name_greeting2:
                    greeting_max_tokens2 = min(configurable.final_report_model_max_tokens, 4096)  # gpt-4 ìµœëŒ€ 4096
                else:
                    greeting_max_tokens2 = min(configurable.final_report_model_max_tokens, 16384)  # ê¸°ë³¸ê°’
                
                greeting_model_config = {
                    "model": configurable.final_report_model,
                    "max_tokens": greeting_max_tokens2,
                    "api_key": get_api_key_for_model(configurable.final_report_model, config),
                }
                
                messages_context = get_buffer_string(messages) if messages else last_user_message
                
                greeting_prompt = f"""ë‹¹ì‹ ì€ ì½”ë”© AI ë„êµ¬ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì§ˆë¬¸ì— ë§ëŠ” ìì—°ìŠ¤ëŸ½ê³  ìƒì„¸í•œ ì¸ì‚¬ ë©˜íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš”.

ì‚¬ìš©ì ë©”ì‹œì§€:
{messages_context}

**ì›ì¹™:**
- ì‚¬ìš©ìì˜ í˜„ì¬ ì§ˆë¬¸ ë‚´ìš©ê³¼ ì˜ë„ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ê·¸ì— ë§ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ë©˜íŠ¸ë¥¼ ìƒì„±
- ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œ(íŒ€ ê·œëª¨, ëª©ì , ìš”êµ¬ì‚¬í•­, ë„ë©”ì¸ ë“±)ë¥¼ ë°˜ì˜
- ì§ˆë¬¸ì— ì–¸ê¸‰ëœ êµ¬ì²´ì ì¸ ë‚´ìš©(íŒ€ ê·œëª¨, ëª©ì , ìš”êµ¬ì‚¬í•­ ë“±)ì„ ë°˜ë“œì‹œ í¬í•¨
- ìì—°ìŠ¤ëŸ½ê³  ì¹œì ˆí•œ í†¤ ìœ ì§€
- ì ì ˆí•œ ê¸¸ì´ (40-100ì ì •ë„, ë„ˆë¬´ ì§§ì§€ ì•Šê²Œ)

ì¸ì‚¬ ë©˜íŠ¸ë§Œ ì¶œë ¥í•˜ì„¸ìš” ([GREETING] íƒœê·¸ ì—†ì´, ë‹¤ë¥¸ ì„¤ëª… ì—†ì´):"""
                
                try:
                    greeting_model = configurable_model.with_config(greeting_model_config)
                    greeting_response = await greeting_model.ainvoke([HumanMessage(content=greeting_prompt)])
                    greeting = str(greeting_response.content).strip().strip('"\'`').strip()
                    
                    if not greeting or len(greeting) < 30:
                        greeting = f"ë„¤! {last_user_message[:30]}ì— ëŒ€í•´ ì¡°ì‚¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
                    
                    print(f"âœ… [ìœ ì‚¬ ì§ˆë¬¸ ì²˜ë¦¬] ì¸ì‚¬ ë©˜íŠ¸ ìƒì„± ì™„ë£Œ: '{greeting}'")
                    
                    return Command(
                        goto=END,
                        update={"messages": [
                            AIMessage(content=greeting),
                            AIMessage(content=report_body)
                        ]}
                    )
                except Exception as e:
                    print(f"âš ï¸ [ìœ ì‚¬ ì§ˆë¬¸ ì²˜ë¦¬] ì¸ì‚¬ ë©˜íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
                    greeting = f"ë„¤! {last_user_message[:30]}ì— ëŒ€í•´ ì¡°ì‚¬í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
                    return Command(
                        goto=END,
                        update={"messages": [
                            AIMessage(content=greeting),
                            AIMessage(content=report_body)
                        ]}
                    )
    
    # ìºì‹œ ë¯¸ìŠ¤ ë° ìœ ì‚¬ ì§ˆë¬¸ë„ ì—†ìŒ â†’ ìƒˆë¡œ ìƒì„±
    # ì£¼ì œ ê²€ì¦ì€ ì´ë¯¸ ìœ„(ë¼ì¸ 138-147)ì—ì„œ ì™„ë£Œë˜ì—ˆìœ¼ë¯€ë¡œ responseë¥¼ ì¬ì‚¬ìš©
    print(f"âš ï¸ [ìºì‹œ MISS + ìœ ì‚¬ ì§ˆë¬¸ ì—†ìŒ] ìƒˆë¡œ ìƒì„± ì§„í–‰ (ì£¼ì œ ê²€ì¦ ì™„ë£Œ)")
    
    # ëª…í™•í™” ë¹„í™œì„±í™” ì‹œ ë°”ë¡œ ë‹¤ìŒ ë‹¨ê³„ë¡œ (ì£¼ì œ ê²€ì¦ì€ ì´ë¯¸ ì™„ë£Œë¨)
    if not configurable.allow_clarification:
        print(f"âœ… [DEBUG] ì£¼ì œ ê²€ì¦ í†µê³¼ - ë°”ë¡œ ì—°êµ¬ ì‹œì‘")
        return Command(
            goto="write_research_brief",
            update={
                "messages": [AIMessage(content=response.verification)],
                "normalized_query": normalized  # ğŸ†• ì •ê·œí™” ì •ë³´ ì €ì¥
            }
        )
    
    # ëª…í™•í™” í•„ìš” ì—¬ë¶€ ì²´í¬
    if response.need_clarification:
        return Command(
            goto=END,
            update={"messages": [AIMessage(content=response.question)]}
        )
    else:
        return Command(
            goto="write_research_brief",
            update={
                "messages": [AIMessage(content=response.verification)],
                "normalized_query": normalized  # ğŸ†• ì •ê·œí™” ì •ë³´ ì €ì¥
            }
        )


def route_after_research(state: AgentState) -> Literal["structured_report_generation", "final_report_generation", "clarify_missing_constraints", "cannot_answer"]:
    """ì—°êµ¬ ì™„ë£Œ í›„ ë¼ìš°íŒ…: Decision Engine ê²°ê³¼ ìœ ë¬´ì™€ ì œì•½ ì¡°ê±´ ì¶©ë¶„ ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°"""
    
    import re
    
    # Decision Engineì´ ì‹¤í–‰ë˜ì–´ì•¼ í•˜ëŠ” ì§ˆë¬¸ì¸ì§€ í™•ì¸
    question_type = state.get("question_type", "comparison")
    messages_list = state.get("messages", [])
    
    # ğŸš¨ HumanMessageë§Œ ì¶”ì¶œ (AI ì‘ë‹µ ë©”ì‹œì§€ ì œì™¸)
    human_messages = [msg for msg in messages_list if isinstance(msg, HumanMessage)]
    all_user_messages_text = " ".join([str(msg.content).lower() for msg in human_messages])
    last_user_message = str(human_messages[-1].content).lower() if human_messages else ""
    
    # ğŸš¨ ë””ë²„ê¹…: ì§ˆë¬¸ ë‚´ìš©ê³¼ íƒ€ì… í™•ì¸
    print(f"ğŸ” [Routing DEBUG] question_type: {question_type}")
    print(f"ğŸ” [Routing DEBUG] HumanMessage ê°œìˆ˜: {len(human_messages)}")
    print(f"ğŸ” [Routing DEBUG] last_user_message: {last_user_message[:100] if last_user_message else 'None'}")
    
    is_decision_question = (
        question_type in ["decision", "comparison"] or
        any(keyword in last_user_message for keyword in [
            "ì¤‘ í•˜ë‚˜ë§Œ", "í•˜ë‚˜ë§Œ", "ì„ íƒ", "ì–´ë–¤ ê²ƒì´", "ë§ì„ê¹Œ", "ì¶”ì²œ", "ì–´ë–¤ ë„êµ¬", 
            "ì¢‹ì„ê¹Œ", "ì í•©", "ìµœì í™”", "ì–´ë–¤ê²Œ", "ë­˜", "ë¬´ì—‡ì„", "ì–´ë–¤ê²Œ ì¢‹", "ì–´ë–¤ ê²ƒì´ ì¢‹",
            "ë¹„êµ", "vs", "ëŒ€ë¹„", "ì°¨ì´", "ì–´ë–¤ê²Œ ë‚˜ì€", "ë” ì¢‹ì€", "ì–´ëŠê²Œ", "ìµœì "
        ]) or
        "ì–´ë–¤ ë„êµ¬ê°€ ì¢‹ì„ê¹Œìš”" in last_user_message or
        ("ì–´ë–¤ ë„êµ¬" in last_user_message and "ì¢‹" in last_user_message) or
        ("vs" in last_user_message or "ëŒ€ë¹„" in last_user_message) or
        ("ìµœì í™”" in last_user_message and "ë„êµ¬" in last_user_message)
    )
    
    # ğŸš¨ ë””ë²„ê¹…: Decision ì§ˆë¬¸ íŒì • ê²°ê³¼
    print(f"ğŸ” [Routing DEBUG] is_decision_question: {is_decision_question}")
    
    # Decision Engine ê²°ê³¼ í™•ì¸
    decision_result = state.get("decision_result")
    tool_facts = state.get("tool_facts", [])
    
    # ì œì•½ ì¡°ê±´ ì¶©ë¶„ ì—¬ë¶€ í™•ì¸
    constraints = state.get("constraints", {})
    team_size = constraints.get("team_size") if constraints else None
    budget_max = constraints.get("budget_max") if constraints else None
    
    # ğŸš¨ ì „ì²´ ì‚¬ìš©ì ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì—ì„œ ì •ë³´ ì¶”ì¶œ
    has_user_type = False
    if not team_size and all_user_messages_text:
        # "ê°œì¸", "ê°œì¸ ê°œë°œì", "ê°œì¸ ì‚¬ìš©ì" ë“±ì„ ì¸ì‹í•˜ì—¬ team_size = 1ë¡œ ì„¤ì •
        if any(keyword in all_user_messages_text for keyword in ["ê°œì¸", "ê°œì¸ ê°œë°œì", "ê°œì¸ ì‚¬ìš©ì", "ê°œì¸ìš©", "ê°œì¸ìœ¼ë¡œ"]):
            team_size = 1
            has_user_type = True
        elif any(keyword in all_user_messages_text for keyword in ["íŒ€", "íŒ€ìš©", "ìš°ë¦¬ íŒ€", "íŒ€ ê·œëª¨"]):
            has_user_type = True
            # "Xëª…" íŒ¨í„´ ì°¾ê¸°
            team_size_match = re.search(r'(\d+)\s*ëª…', all_user_messages_text)
            if team_size_match:
                team_size = int(team_size_match.group(1))
        else:
            # "Xëª…" íŒ¨í„´ ì°¾ê¸°
            team_size_match = re.search(r'(\d+)\s*ëª…', all_user_messages_text)
            if team_size_match:
                team_size = int(team_size_match.group(1))
    
    # ê°œë°œ ì–¸ì–´/ë¶„ì•¼ í™•ì¸ (ì „ì²´ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì—ì„œ)
    has_development_area = False
    if all_user_messages_text:
        # í”„ë¡œê·¸ë˜ë° ì–¸ì–´ í™•ì¸ (í•˜ë“œì½”ë”© - ì–¸ì–´ëŠ” ì •í•´ì ¸ ìˆìœ¼ë¯€ë¡œ OK)
        languages = ["python", "javascript", "java", "typescript", "c++", "c#", "go", "rust", "php", "ruby", "swift", "kotlin", "dart", "r", "scala", "clojure", "perl", "lua", "matlab"]
        # ê°œë°œ ë¶„ì•¼ í™•ì¸
        domains = ["ì›¹ ê°œë°œ", "ë°±ì—”ë“œ", "í”„ë¡ íŠ¸ì—”ë“œ", "í’€ìŠ¤íƒ", "ëª¨ë°”ì¼", "ê²Œì„", "ë°ì´í„°", "ai", "ml", "ë¨¸ì‹ ëŸ¬ë‹", "ì•± ê°œë°œ"]
        # í”„ë ˆì„ì›Œí¬/ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸ (ì£¼ìš” í”„ë ˆì„ì›Œí¬ë§Œ)
        frameworks = ["react", "vue", "angular", "django", "flask", "spring", "node.js", "express", "fastapi", "laravel", "rails"]
        
        # "~ìœ¼ë¡œ ê°œë°œ", "~ë¡œ ê°œë°œ", "~ ê°œë°œ", "~ì„ ì‚¬ìš©" ê°™ì€ í‘œí˜„ë„ í¬í•¨
        if any(lang in all_user_messages_text for lang in languages) or \
           any(domain in all_user_messages_text for domain in domains) or \
           any(fw in all_user_messages_text for fw in frameworks) or \
           re.search(r'ìœ¼ë¡œ\s*ê°œë°œ|ë¡œ\s*ê°œë°œ|ê°œë°œ', all_user_messages_text):
            has_development_area = True
    
    # ğŸš¨ ë§¤ìš° ì¤‘ìš”: ê¸°ë³¸ì ìœ¼ë¡œ ì •ë³´ê°€ ì¶©ë¶„í•˜ë‹¤ê³  ê°€ì •!
    # ì •ë§ ëª¨í˜¸í•œ ê²½ìš°ë§Œ ëª…í™•í™” ìš”êµ¬
    # ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ì¶©ë¶„í•œ ì •ë³´:
    # 1. ê°œë°œ ì–¸ì–´/ë¶„ì•¼ê°€ ìˆìŒ
    # 2. ì‚¬ìš© í˜•íƒœ(ê°œì¸/íŒ€)ê°€ ìˆìŒ
    # 3. ì œì•½ ì¡°ê±´(ì˜ˆì‚°/íŒ€ ê·œëª¨)ì´ ìˆìŒ
    # 4. ì¼ë°˜ì ì¸ ì¶”ì²œ ìš”ì²­ (ì½”ë”© AI ë„êµ¬ ì¶”ì²œ ë“±)
    
    # ì •ë§ ëª¨í˜¸í•œ ê²½ìš° ì²´í¬ (ëª…í™•í™” í•„ìš”)
    is_too_vague = False
    if all_user_messages_text:
        # ë„ˆë¬´ ëª¨í˜¸í•œ í‘œí˜„ë“¤
        vague_patterns = [
            r'ë‚˜\s*ê°œë°œ\s*í• ê±´ë°',  # "ë‚˜ ê°œë°œ í• ê±´ë°"
            r'ê°œë°œ\s*í• ê±´ë°',  # "ê°œë°œ í• ê±´ë°"
            r'ê°œë°œ\s*í•˜ë ¤ê³ \s*í•˜ëŠ”ë°',  # "ê°œë°œ í•˜ë ¤ê³  í•˜ëŠ”ë°"
            r'ê°œë°œ\s*í•˜ë ¤ëŠ”ë°',  # "ê°œë°œ í•˜ë ¤ëŠ”ë°"
        ]
        # ëª¨í˜¸í•œ íŒ¨í„´ì´ ìˆê³ , ë‹¤ë¥¸ êµ¬ì²´ì ì¸ ì •ë³´ê°€ ì—†ìœ¼ë©´ ëª¨í˜¸í•¨
        has_vague_pattern = any(re.search(pattern, all_user_messages_text) for pattern in vague_patterns)
        if has_vague_pattern and not has_development_area and not has_user_type and not team_size and not budget_max:
            is_too_vague = True
    
    # ì •ë³´ ì¶©ë¶„ ì—¬ë¶€ íŒë‹¨: ëª¨í˜¸í•˜ì§€ ì•Šê³ , ì–´ëŠ ì •ë„ ì •ë³´ê°€ ìˆìœ¼ë©´ ì¶©ë¶„
    has_sufficient_info = not is_too_vague and (
        has_development_area or  # ê°œë°œ ì–¸ì–´/ë¶„ì•¼ê°€ ìˆìœ¼ë©´ ì¶©ë¶„
        has_user_type or  # ì‚¬ìš© í˜•íƒœê°€ ìˆìœ¼ë©´ ì¶©ë¶„
        team_size is not None or  # íŒ€ ê·œëª¨ê°€ ìˆìœ¼ë©´ ì¶©ë¶„
        budget_max is not None or  # ì˜ˆì‚°ì´ ìˆìœ¼ë©´ ì¶©ë¶„
        "ì½”ë”©" in all_user_messages_text or  # "ì½”ë”©" í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¶©ë¶„
        "ai" in all_user_messages_text or  # "AI" í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¶©ë¶„
        "ë„êµ¬" in all_user_messages_text  # "ë„êµ¬" í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ì¶©ë¶„ (ì¼ë°˜ ì¶”ì²œ ê°€ëŠ¥)
    )
    has_sufficient_constraints = team_size is not None or budget_max is not None
    
    # ğŸš¨ ë””ë²„ê¹…: Decision Engine ê²°ê³¼ í™•ì¸
    print(f"ğŸ” [Routing DEBUG] decision_result ì¡´ì¬: {decision_result is not None}")
    print(f"ğŸ” [Routing DEBUG] decision_result íƒ€ì…: {type(decision_result)}")
    if decision_result:
        if isinstance(decision_result, dict):
            print(f"ğŸ” [Routing DEBUG] decision_result.keys(): {list(decision_result.keys())}")
            print(f"ğŸ” [Routing DEBUG] recommended_tools: {decision_result.get('recommended_tools', [])}")
        else:
            print(f"ğŸ” [Routing DEBUG] decision_result.recommended_tools: {getattr(decision_result, 'recommended_tools', [])}")
    print(f"ğŸ” [Routing DEBUG] tool_facts ê°œìˆ˜: {len(tool_facts) if tool_facts else 0}")
    print(f"ğŸ” [Routing DEBUG] ì •ë³´ ì¶©ë¶„ ì—¬ë¶€: {has_sufficient_info} (user_type: {has_user_type}, dev_area: {has_development_area}, team_size: {team_size}, budget_max: {budget_max})")
    
    if is_decision_question:
        # ğŸš¨ Decision ì§ˆë¬¸ì¸ ê²½ìš°
        # Decision Engine ê²°ê³¼ê°€ ìˆê³  ì¶”ì²œ ë„êµ¬ê°€ ìˆìœ¼ë©´ êµ¬ì¡°í™”ëœ ë¦¬í¬íŠ¸ ìƒì„±
        if decision_result:
            # decision_resultê°€ dictì¸ ê²½ìš° model_dump()ëœ ê²°ê³¼ì´ë¯€ë¡œ recommended_tools í™•ì¸
            if isinstance(decision_result, dict):
                recommended_tools_list = decision_result.get("recommended_tools", [])
            elif hasattr(decision_result, "recommended_tools"):
                recommended_tools_list = decision_result.recommended_tools
            else:
                recommended_tools_list = []
            
            recommended_count = len(recommended_tools_list) if recommended_tools_list else 0
            print(f"ğŸ” [Routing DEBUG] recommended_count: {recommended_count}")
            
            if recommended_count > 0:
                print(f"âœ… [Routing] Decision ì§ˆë¬¸ + Decision Engine ê²°ê³¼ ìˆìŒ (ì¶”ì²œ {recommended_count}ê°œ) â†’ structured_report_generation")
                return "structured_report_generation"
            else:
                # Decision Engine ê²°ê³¼ëŠ” ìˆì§€ë§Œ ì¶”ì²œ ë„êµ¬ê°€ ì—†ëŠ” ê²½ìš°: í•„í„°ë§ì´ ë„ˆë¬´ ì—„ê²©í–ˆì„ ìˆ˜ ìˆìŒ
                print(f"âš ï¸ [Routing DEBUG] Decision Engine ê²°ê³¼ëŠ” ìˆì§€ë§Œ ì¶”ì²œ ë„êµ¬ê°€ ì—†ìŒ (recommended_tools ë¹ˆ ë¦¬ìŠ¤íŠ¸)")
                print(f"âš ï¸ [Routing] í•„í„°ë§ì´ ë„ˆë¬´ ì—„ê²©í–ˆê±°ë‚˜ tool_facts ì •ë³´ ë¶€ì¡± â†’ final_report_generation (fallback)")
                return "final_report_generation"
        elif not has_sufficient_info:
            # ğŸš¨ ê°œë°œ ì–¸ì–´/ë¶„ì•¼ë„ ì—†ê³  ì œì•½ ì¡°ê±´ë„ ì—†ìœ¼ë©´ ëª…í™•í™” í•„ìš”
            print(f"ğŸ” [Routing] Decision ì§ˆë¬¸ì´ì§€ë§Œ ì •ë³´ ë¶€ì¡± (user_type: {has_user_type}, dev_area: {has_development_area}, team_size: {team_size}, budget: {budget_max}) â†’ clarify_missing_constraints")
            return "clarify_missing_constraints"
        else:
            # ğŸš¨ ì œì•½ ì¡°ê±´ì€ ì—†ì§€ë§Œ ê°œë°œ ì–¸ì–´/ë¶„ì•¼ê°€ ìˆìœ¼ë©´ ì¶©ë¶„í•œ ì •ë³´!
            # Decision Engine ê²°ê³¼ê°€ ì—†ì–´ë„ ì¼ë°˜ ë¦¬í¬íŠ¸ë¡œ ì¶”ì²œ ì œê³µ
            print(f"âœ… [Routing] Decision ì§ˆë¬¸ + ê°œë°œ ì–¸ì–´/ë¶„ì•¼ ì •ë³´ ìˆìŒ (ì œì•½ ì¡°ê±´ ì—†ì§€ë§Œ ì¶©ë¶„) â†’ final_report_generation")
            return "final_report_generation"
    else:
        # Discovery ì§ˆë¬¸ì¸ ê²½ìš°: ì¼ë°˜ ë¦¬í¬íŠ¸ ìƒì„± (Decision Engine ë¶ˆí•„ìš”)
        print(f"âœ… [Routing] Discovery ì§ˆë¬¸ â†’ final_report_generation")
        return "final_report_generation"

